{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e07f79",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "- Decoding is the process of generating text from a language model, one token at a time.\n",
    "- After training, when you want your LLM to generate text, you need a strategy to decide which token to pick next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67c58e",
   "metadata": {},
   "source": [
    "#### The Core Problem\n",
    "At each step, your model outputs a probability distribution over your entire vocabulary (let's say 50,000+ tokens). You need to decide:\n",
    "- Which token do I pick?\n",
    "- Do I always pick the most likely one?\n",
    "- How do I balance quality vs diversity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b0160",
   "metadata": {},
   "source": [
    "#### Why This Matters\n",
    "The decoding strategy dramatically effects:\n",
    "- Quality: Coherence and relevance of generated text\n",
    "- Diversity: Whether you get repetitive or varied outputs\n",
    "- Speed: Some strategies are faster than others\n",
    "- Use case fit: Different strategies work better for different tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37255637",
   "metadata": {},
   "source": [
    "### Greedy Decoding\n",
    "- Greedy decoding is the simplest approach: at each step, always pick the token with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e14df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, input_ids, eos_token_id, max_length=50):\n",
    "    generated = input_ids.clone()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated)\n",
    "            next_logits = logits[:, -1, :]\n",
    "\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "\n",
    "        next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "\n",
    "        generated = torch.concat([generated, next_token], dim=1)\n",
    "\n",
    "        if (next_token == eos_token_id).all():\n",
    "            break\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d97f74",
   "metadata": {},
   "source": [
    "#### Advantages\n",
    "- **Deterministic**: Same input always produces same output\n",
    "- **Fast**: No sampling overhead, just argmax\n",
    "- **Simple**: Easy to implement and debug\n",
    "\n",
    "#### Disadvantages\n",
    "- **Repetitive**: Often gets stuck in loops (\"very very very very...\")\n",
    "- **No diversity**: Can't generate multiple different completions\n",
    "- **Not always best**: The highest probability at each step doesn't guarantee the best overall sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf3cf1",
   "metadata": {},
   "source": [
    "#### Beam Search\n",
    "- Beam Search tries to fix greedy's shortsightedness by exploring multiple possible sequences in parallel. Instead of committing to one token at each step, we keep track of the top-k most promising sequences.\n",
    "\n",
    "##### The Core Idea\n",
    "- Beam width (k): Number of candidate sequences to track simultaneously\n",
    "- At each step: expand all beams, keep only the k best overall sequences\n",
    "- Score sequences by their cumulative log probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1dcd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, input_ids, vocab_size, eos_token_id, beam_width, max_length):\n",
    "    beams = input_ids.repeat(beam_width, 1)\n",
    "    beam_scores = torch.zeros(beam_width, dtype=torch.float32)\n",
    "    beam_scores[1:] = float('-inf')\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            logits = model(beams)\n",
    "            next_logits = logits[:,:,-1,:]\n",
    "\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "\n",
    "        next_scores = beam_scores.unsqueeze(1) + probs\n",
    "\n",
    "        next_scores = next_scores.view(-1)\n",
    "\n",
    "        topk_values, topk_indices = torch.topk(next_scores, k=beam_width)\n",
    "\n",
    "        beam_idx = topk_values // vocab_size\n",
    "        token_idx = topk_indices % vocab_size\n",
    "\n",
    "        beams = torch.concat([\n",
    "            beams[beam_idx],\n",
    "            token_idx.unsqueeze(1)\n",
    "        ], dim=1)\n",
    "\n",
    "        if (next_logits == eos_token_id).all():\n",
    "            break\n",
    "\n",
    "    best_beam_idx = torch.argmax(beam_scores)\n",
    "    return beams[best_beam_idx].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d86cc6",
   "metadata": {},
   "source": [
    "#### Temperature Sampling\n",
    "- Temperature sampling introduces randomness into text generation by controlling how \"sharp\" or \"flat\" the probability distribution is before sampling.\n",
    "- Key idea: Modify the probability distribution, then use torch.multinomial to sample from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cdf332",
   "metadata": {},
   "source": [
    "#### Temperature Values\n",
    "\n",
    "- T = 1.0: Original distribution (no change)\n",
    "- T < 1.0 (e.g., 0.5): Makes distribution sharper (more confident, less random)\n",
    "- T > 1.0 (e.g., 2.0): Makes distribution flatter (less confident, more random)\n",
    "- T → 0: Approaches greedy (always pick max)\n",
    "- T → ∞: Uniform distribution (all tokens equally likely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_sampling(model, input_ids, temperature, max_length, eos_token_id):\n",
    "    generated = input_ids.clone()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated)\n",
    "            next_logits = logits[:,-1,:] / temperature\n",
    "\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "        if (next_token == eos_token_id).all():\n",
    "            break\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e1cb503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When T < 1: logits get BIGGER → exp() amplifies differences more\n",
    "# Example: exp(4/0.5) = exp(8) = 2981 vs exp(1/0.5) = exp(2) = 7.4\n",
    "# Ratio: 2981/7.4 = 402 (huge gap!)\n",
    "#\n",
    "# When T > 1: logits get SMALLER → exp() compresses differences\n",
    "# Example: exp(4/2) = exp(2) = 7.4 vs exp(1/2) = exp(0.5) = 1.6\n",
    "# Ratio: 7.4/1.6 = 4.6 (smaller gap!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf10468",
   "metadata": {},
   "source": [
    "##### Advantages ✅\n",
    "\n",
    "- Diversity: Different outputs each time\n",
    "- Controllable randomness: Tune with single parameter\n",
    "- Simple: Just one line of code\n",
    "- Works well: Good default for creative tasks\n",
    "\n",
    "#### Disadvantages ❌\n",
    "\n",
    "- Can be incoherent at high T (too random)\n",
    "- Can still repeat at low T (approaching greedy)\n",
    "- No quality control: Might sample low-probability nonsense\n",
    "- Hard to tune: Optimal T varies by task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1200b285",
   "metadata": {},
   "source": [
    "##### When to Use Different Temperatures\n",
    "\n",
    "- T = 0.1-0.5: Factual tasks (translation, Q&A, code generation): More deterministic, focused on high-probability tokens\n",
    "\n",
    "- T = 0.7-0.8: Default for chat/general use: Good balance of quality and diversity\n",
    "\n",
    "- T = 1.0-1.5: Creative writing, brainstorming: More variety and unexpected outputs\n",
    "\n",
    "- T > 1.5: Experimental/artistic generation: Very random, often incoherent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7ab83",
   "metadata": {},
   "source": [
    "### Top-k Sampling\n",
    "\n",
    "- Top-k sampling addresses a key problem with temperature sampling: even with low temperature, you might still sample terrible low-probability tokens.\n",
    "- Solution: Only consider the top-k most likely tokens, set all others to zero probability, then sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb786c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sampling(model, input_ids, k, temperature, eos_token_id, max_length):\n",
    "    generated = input_ids.clone()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated)\n",
    "            next_logits = logits[:,-1,:] / temperature\n",
    "\n",
    "        topk_values, topk_indices = torch.topk(next_logits, k=k, dim=-1)\n",
    "\n",
    "        filtered_logits = torch.full_like(next_logits, fill_value=float('-inf'))\n",
    "        filtered_logits.scatter_(dim=-1, index=topk_indices, src=topk_values)\n",
    "\n",
    "        probs = torch.softmax(filtered_logits, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        generated = torch.concat([generated, next_token], dim=-1)\n",
    "\n",
    "        if (next_token == eos_token_id).all():\n",
    "            break\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01920258",
   "metadata": {},
   "source": [
    "##### Advantages ✅\n",
    "\n",
    "- Prevents low-quality tokens: Can't sample garbage\n",
    "- Adaptive: Works with any vocabulary size\n",
    "- Combines with temperature: Apply both for fine control\n",
    "- Simple parameter: Just choose k\n",
    "\n",
    "##### Disadvantages ❌\n",
    "\n",
    "- Fixed k: Always keeps k tokens, even if top 2 are clearly best\n",
    "- Context-independent: k=50 might be too many for \"The capital of France is ___\" but too few for \"The weather today is ___\"\n",
    "- Can cut off good options: If k=5 but token #6 is still reasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d1444",
   "metadata": {},
   "source": [
    "#### Top-p (Nucleus) Sampling\n",
    "- Top-p sampling (also called Nucleus sampling) solves top-k's rigidity by using a dynamic cutoff based on cumulative probability instead of a fixed number of tokens.\n",
    "- Key idea: Keep adding tokens (sorted by probability) until their cumulative probability reaches p. Then sample only from those tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b0188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(model, input_ids, p, temperature, eos_token_id, max_length):\n",
    "    generated = input_ids.clone()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated)\n",
    "            next_logits = logits[:,-1,:] / temperature\n",
    "\n",
    "        sorted_logits, sorted_indices = torch.sort(next_logits, descending=True, dim=-1)\n",
    "\n",
    "        sorted_probs = torch.softmax(sorted_logits, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        nucleus_mask = cumulative_probs > p\n",
    "        nucleus_mask[..., 0] = False\n",
    "        sorted_logits[nucleus_mask] = float('-inf')\n",
    "        \n",
    "        filtered_logits = torch.full_like(next_logits, fill_value=float('-inf'))\n",
    "        filtered_logits.scatter_(dim=-1, index=sorted_indices, src=sorted_logits)\n",
    "\n",
    "        probs = torch.softmax(filtered_logits, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        generated = torch.concat([generated, next_token], dim=-1)\n",
    "\n",
    "        if eos_token_id is not None and (next_token == eos_token_id).all():\n",
    "            break\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22415b0a",
   "metadata": {},
   "source": [
    "#### Advantages ✅\n",
    "\n",
    "- Adaptive: Adjusts to model confidence\n",
    "- Flexible: Works well across different contexts\n",
    "- Quality control: Prevents sampling junk while allowing diversity\n",
    "- Intuitive parameter: p=0.9 means \"keep 90% probability mass\"\n",
    "\n",
    "#### Disadvantages ❌\n",
    "\n",
    "- Slightly more complex than top-k\n",
    "- Still can include low-quality tokens if many tokens have similar probabilities\n",
    "- Computational overhead: Sorting is O(n log n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9131be",
   "metadata": {},
   "source": [
    "### Constrative Search\n",
    "\n",
    "- The Problem It Solves: All the previous methods have a fundamental trade-off:\n",
    "    - Greedy/Beam Search: High quality but repetitive (\"very very very...\")\n",
    "    - Sampling (temp/top-k/top-p): Diverse but can be incoherent\n",
    "\n",
    "- Contrastive search tries to get the best of both worlds: high quality AND diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc9103f",
   "metadata": {},
   "source": [
    "#### The Core Idea\n",
    "Instead of just picking high-probability tokens, contrastive search balances two objectives:\n",
    "\n",
    "1. Model confidence (like greedy): Pick tokens the model thinks are likely\n",
    "2. Degeneration penalty: Avoid tokens that are too similar to what we've already generated\n",
    "\n",
    "The insight: Repetition happens because similar contexts lead to similar next tokens. If we explicitly penalize similarity to past tokens, we can break the repetition cycle while maintaining coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e05853",
   "metadata": {},
   "source": [
    "```\n",
    "score(token) = (1 - α) × model_confidence(token) \n",
    "               - α × max_similarity_to_context(token)\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- α (alpha): Hyperparameter controlling the trade-off (typically 0.6)\n",
    "- model_confidence: Probability from the model (like in greedy)\n",
    "- max_similarity_to_context: How similar this token is to recently generated tokens\n",
    "\n",
    "```\n",
    "For candidate token w:\n",
    "1. Get its hidden representation: h(w)\n",
    "2. Compare to hidden states of previously generated tokens: h(context)\n",
    "3. Compute cosine similarity: cos_sim(h(w), h(context))\n",
    "4. Take the MAXIMUM similarity (penalize being similar to ANY past token)\n",
    "```\n",
    "\n",
    "Pick the token with the highest score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b37cb",
   "metadata": {},
   "source": [
    "##### Advantages ✅\n",
    "\n",
    "- Deterministic: Same input → same output (unlike sampling)\n",
    "- No repetition: Explicitly penalizes it\n",
    "- Maintains coherence: Still respects model probabilities\n",
    "- No temperature tuning: α is more intuitive than temperature\n",
    "- Human-like text: Often produces more natural outputs than beam search\n",
    "\n",
    "##### Disadvantages ❌\n",
    "\n",
    "- Computationally expensive: Need to compute similarities for top-k candidates\n",
    "- Requires hidden states: Need access to model internals (not just logits)\n",
    "- Memory intensive: Must store hidden states of all generated tokens\n",
    "- Less diverse than sampling: Still deterministic, no randomness\n",
    "- Hyperparameter sensitive: α needs tuning per task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999bc126",
   "metadata": {},
   "source": [
    "##### Context Window Consideration\n",
    "An important practical detail: Which past tokens do we compare to?\n",
    "Typically:\n",
    "\n",
    "- Compare to tokens in a sliding window (e.g., last 20-50 tokens)\n",
    "- Not the entire history (too expensive and less relevant)\n",
    "- Recent context matters most for avoiding repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6a1d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrastive_search(model, input_ids, eos_token_id, max_length=50, alpha=0.6, k=4):\n",
    "    generated = input_ids.clone()\n",
    "    past_hidden_states = []\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, output_hidden_states=True)\n",
    "            next_logits = logits[:, -1, :]\n",
    "            hidden_states = logits.hidden_states[-1][:, -1, :]\n",
    "\n",
    "        past_hidden_states.append(hidden_states)\n",
    "\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "        top_k_probs, top_k_indices = torch.topk(probs, k=k)\n",
    "\n",
    "        candidate_hidden_states = model.get_input_embeddings()[top_k_indices]\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        for i in range(k):\n",
    "            model_conf = top_k_probs[0, i].item()\n",
    "            candidate_h = candidate_hidden_states[0, i]\n",
    "\n",
    "            max_similarity = 0.0\n",
    "            if len(past_hidden_states) > 0:\n",
    "                past_h = torch.stack(past_hidden_states).squeeze(1)\n",
    "                \n",
    "                # Compute cosine similarity\n",
    "                similarities = torch.nn.functional.cosine_similarity(\n",
    "                    candidate_h.unsqueeze(0), \n",
    "                    past_h,                    \n",
    "                    dim=-1\n",
    "                )\n",
    "                max_similarity = similarities.max().item()\n",
    "            \n",
    "            score = (1 - alpha) * model_conf - alpha * max_similarity\n",
    "            scores.append(score)\n",
    "\n",
    "        best_idx = torch.argmax(torch.tensor(scores))\n",
    "        next_token = top_k_indices[0, best_idx].unsqueeze(0).unsqueeze(0)  # [1, 1]\n",
    "        \n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "        \n",
    "        if next_token.item() == eos_token_id:\n",
    "            break\n",
    "\n",
    "    return generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
