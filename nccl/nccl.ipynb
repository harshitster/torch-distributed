{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32410138",
   "metadata": {},
   "source": [
    "### Implementations of collective communications (Tree & Ring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c7d1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f4b500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCCL:\n",
    "    def __init__(self, rank, world_size):\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        dist.init_process_group(\n",
    "            backend='nccl',\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        torch.cuda.set_device(device=self.rank)\n",
    "\n",
    "    def all_reduce_ring(self, tensor: torch.Tensor, op=dist.ReduceOp.SUM):\n",
    "        if self.world_size == 1:\n",
    "            return tensor\n",
    "        \n",
    "        send_rank = (self.rank + 1) % self.world_size\n",
    "        recv_rank = (self.rank - 1 + self.world_size) % self.world_size\n",
    "\n",
    "        chunk_size = tensor.numel() // self.world_size\n",
    "        remainder = tensor.numel % self.world_size\n",
    "\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        for i in range(self.world_size):\n",
    "            size = chunk_size + (1 if i < remainder else 0)\n",
    "            chunks.append(tensor[start : start + size])\n",
    "            start += size\n",
    "\n",
    "        recv_buffer = torch.zeros_like(chunks[0], device=tensor.device)\n",
    "\n",
    "        # reduce_scatter\n",
    "        for step in range(self.world_size - 1):\n",
    "            send_chunk_idx = (self.rank - step + self.world_size) % self.world_size\n",
    "            recv_chunk_idx = (self.rank - step - 1 + self.world_size) % self.world_size\n",
    "\n",
    "            send_chunk = chunks[send_chunk_idx]\n",
    "            recv_chunk = chunks[recv_chunk_idx]\n",
    "\n",
    "            if recv_chunk.numel() != recv_buffer.numel():\n",
    "                recv_buffer = torch.zeros_like(recv_chunk, device=tensor.device)\n",
    "\n",
    "            send_handle = dist.isend(send_chunk, dst=send_rank)\n",
    "            recv_handle = dist.irecv(recv_buffer, src=recv_rank)\n",
    "\n",
    "            send_handle.wait()\n",
    "            recv_handle.wait()\n",
    "\n",
    "            if op == dist.ReduceOp.SUM:\n",
    "                recv_chunk.add_(recv_buffer)\n",
    "\n",
    "        # all_gather\n",
    "        for step in range(self.world_size - 1):\n",
    "            send_chunk_idx = (self.rank - step - 1 + self.world_size) % self.world_size\n",
    "            recv_chunk_idx = (self.rank - step + self.world_size) % self.world_size\n",
    "\n",
    "            send_chunk = chunks[send_chunk_idx]\n",
    "            recv_chunk = chunks[recv_chunk_idx]\n",
    "\n",
    "            if recv_chunk.numel() != recv_buffer.numel():\n",
    "                recv_buffer = torch.zeros_like(recv_chunk, device=tensor.device)\n",
    "\n",
    "            send_handle = dist.isend(send_chunk, dst=send_rank)\n",
    "            recv_handle = dist.irecv(recv_buffer, src=recv_rank)\n",
    "\n",
    "            send_handle.wait()\n",
    "            recv_handle.wait()\n",
    "\n",
    "            if op == dist.ReduceOp.SUM:\n",
    "                recv_chunk.copy_(recv_buffer)\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    def broadcast_mst(self, tensor, root):\n",
    "        if self.world_size == 1:\n",
    "            return tensor\n",
    "        \n",
    "        left_child = 2 * self.rank + 1\n",
    "        right_child = 2 * self.rank + 2\n",
    "\n",
    "        if self.rank != root:\n",
    "            parent = (self.rank - 1) // 2\n",
    "            dist.recv(tensor, src=parent)\n",
    "\n",
    "        if left_child < self.world_size:\n",
    "            dist.send(tensor, dst=left_child)\n",
    "        if right_child < self.world_size:\n",
    "            dist.send(tensor, dst=right_child)\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    def scatter_tree(self, tensor, root):\n",
    "        if self.world_size == 1:\n",
    "            return tensor\n",
    "        \n",
    "        def get_parent(rank):\n",
    "            return (rank - 1) // 2\n",
    "        \n",
    "        def get_children(rank):\n",
    "            left = 2 * rank + 1\n",
    "            right = 2 * rank + 2\n",
    "\n",
    "            children = []\n",
    "            if left < self.world_size:\n",
    "                children.append(left)\n",
    "            if right < self.world_size:\n",
    "                children.append(right)\n",
    "\n",
    "            return children\n",
    "        \n",
    "        def get_subtree_ranks(rank):\n",
    "            ranks = [rank]\n",
    "            children = get_children(rank)\n",
    "\n",
    "            for child in children:\n",
    "                ranks.extend(get_subtree_ranks(child))\n",
    "\n",
    "            return ranks\n",
    "        \n",
    "        chunk_size = tensor.numel() // self.world_size\n",
    "        if self.rank == root:\n",
    "            chunks = tensor.split(chunk_size)\n",
    "\n",
    "            rank_to_chunk = {i: chunks[i] for i in range(self.world_size)}\n",
    "            my_chunk = rank_to_chunk[self.rank]\n",
    "\n",
    "            for child in get_children(self.rank):\n",
    "                subtree_ranks = get_subtree_ranks(child)\n",
    "                subtree_chunks = torch.cat([rank_to_chunk[r] for r in subtree_ranks])\n",
    "                dist.send(subtree_chunks.contiguous(), dst=child)\n",
    "        else:\n",
    "            parent = get_parent(self.rank)\n",
    "            subtree_ranks = get_subtree_ranks(self.rank)\n",
    "            subtree_len = len(subtree_ranks)\n",
    "\n",
    "            chunks_buffer = torch.zeros_like(chunk_size * subtree_len, device=tensor.device)\n",
    "            dist.recv(chunks_buffer, src=parent)\n",
    "\n",
    "            chunks = chunks_buffer.split(subtree_len)\n",
    "            rank_to_chunk = {r: chunks[r] for r in subtree_ranks}\n",
    "\n",
    "            my_chunk = rank_to_chunk[self.rank]\n",
    "\n",
    "            for child in get_children(self.rank):\n",
    "                subtree_ranks = get_subtree_ranks(child)\n",
    "                subtree_chunks = torch.cat([rank_to_chunk[r] for r in subtree_ranks])\n",
    "                dist.send(subtree_chunks.contiguous(), dst=child)\n",
    "\n",
    "        return my_chunk\n",
    "    \n",
    "    def gather_mst(self, tensor, root):\n",
    "        if self.world_size == 1:\n",
    "            return tensor\n",
    "        \n",
    "        def get_parent(rank):\n",
    "            return (rank - 1) // 2\n",
    "        \n",
    "        def get_children(rank):\n",
    "            left = 2 * rank + 1\n",
    "            right = 2 * rank + 2\n",
    "\n",
    "            children = []\n",
    "            if left < self.world_size:\n",
    "                children.append(left)\n",
    "            if right < self.world_size:\n",
    "                children.append(right)\n",
    "\n",
    "            return children\n",
    "        \n",
    "        def get_subtree_size(rank):\n",
    "            size = 1\n",
    "            children = get_children(rank)\n",
    "            for child in children:\n",
    "                size += get_subtree_size(child)\n",
    "\n",
    "            return size\n",
    "        \n",
    "        chunk_size = tensor.numel()\n",
    "        children = get_children(self.rank)\n",
    "        child_data = []\n",
    "\n",
    "        for child in children:\n",
    "            subtree_size = get_subtree_size(child)\n",
    "            buffer = torch.zeros_like(subtree_size * chunk_size, device=tensor.device)\n",
    "            dist.recv(buffer, src=child)\n",
    "            child_data.append(buffer)\n",
    "\n",
    "        data = None\n",
    "        if len(child_data) == 0:\n",
    "            data = tensor\n",
    "        elif len(child_data) == 1:\n",
    "            data = torch.cat([child_data[0], tensor])\n",
    "        else:\n",
    "            data = torch.cat([child_data[0], tensor, child_data[1]])\n",
    "        \n",
    "        if self.rank != root:\n",
    "            parent = get_parent(self.rank)\n",
    "            dist.send(data, dst=parent)\n",
    "            return None\n",
    "        else:\n",
    "            return data\n",
    "        \n",
    "    def reduce_tree(self, tensor: torch.Tensor, root: int, op=dist.ReduceOp.SUM):\n",
    "        if self.world_size == 1:\n",
    "            return tensor\n",
    "\n",
    "        def get_parent(rank):\n",
    "            return (rank - 1) // 2\n",
    "        \n",
    "        def get_children(rank):\n",
    "            left_child = 2 * rank + 1\n",
    "            right_child = 2 * rank + 2\n",
    "\n",
    "            children = []\n",
    "            if left_child < self.world_size:\n",
    "                children.append(left_child)\n",
    "            if right_child < self.world_size:\n",
    "                children.append(right_child)\n",
    "\n",
    "            return children\n",
    "\n",
    "        children = get_children(self.rank)\n",
    "        recv_buffer = torch.zeros_like(tensor, dtype=torch.float32)\n",
    "\n",
    "        for child in children:\n",
    "            dist.recv(recv_buffer, src=child)\n",
    "            \n",
    "            if op == dist.ReduceOp.SUM:\n",
    "                tensor.add_(recv_buffer)\n",
    "\n",
    "        if self.rank != root:\n",
    "            parent = get_parent(self.rank)\n",
    "            dist.send(tensor, dst=parent)\n",
    "            return None\n",
    "        else:\n",
    "            return tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
