{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32410138",
   "metadata": {},
   "source": [
    "### Implementations of collective communications (Tree & Ring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c7d1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a0530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCCL:\n",
    "    def __init__(self, rank, world_size):\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        os.environ['MASTER_ADDR'] = 'localhost'\n",
    "        os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "        dist.init_process_group(\n",
    "            backend='nccl',\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        torch.cuda.set_device(self.rank)\n",
    "\n",
    "    def all_reduce_ring(self, tensor: torch.tensor, op=dist.ReduceOp.SUM):\n",
    "        if self.world_size == 1:\n",
    "            return tensor\n",
    "    \n",
    "        send_rank = (self.rank + 1) % self.world_size\n",
    "        recv_rank = (self.rank - 1 + self.world_size) % self.world_size\n",
    "\n",
    "        chunks = torch.chunk(tensor, chunks=self.world_size)\n",
    "        recv_buffer = torch.zeros_like(chunks[0], device=tensor.device)\n",
    "\n",
    "        # reduce scatter\n",
    "        for step in range(self.world_size - 1):\n",
    "            send_chunk_idx = (self.rank - step) % self.world_size\n",
    "            recv_chunk_idx = (self.rank - step - 1 + self.world_size) % self.world_size\n",
    "\n",
    "            send_chunk = chunks[send_chunk_idx]\n",
    "            recv_chunk = chunks[recv_chunk_idx]\n",
    "\n",
    "            if recv_chunk.numel() != recv_buffer.numel():\n",
    "                recv_buffer = torch.zeros_like(recv_chunk, device=tensor.device)\n",
    "\n",
    "            send_handle = dist.isend(send_chunk, dst=send_rank)\n",
    "            recv_handle = dist.irecv(recv_chunk, src=recv_rank)\n",
    "\n",
    "            send_handle.wait()\n",
    "            recv_handle.wait()\n",
    "\n",
    "            if op == dist.ReduceOp.SUM:\n",
    "                recv_chunk.add_(recv_buffer)\n",
    "\n",
    "        \n",
    "        # all gather\n",
    "        for step in range(self.world_size - 1):\n",
    "            send_chunk_idx = (self.rank - step - 1 + self.world_size) % self.world_size\n",
    "            recv_chunk_idx = (self.rank - step) % self.world_size\n",
    "\n",
    "            if recv_chunk.numel() != recv_buffer.numel():\n",
    "                recv_buffer = torch.zeros_like(recv_chunk, device=tensor.device)\n",
    "\n",
    "            send_handle = dist.isend(send_chunk, dst=send_rank)\n",
    "            recv_handle = dist.irecv(recv_chunk, src=recv_rank)\n",
    "\n",
    "            send_handle.wait()\n",
    "            recv_handle.wait()\n",
    "\n",
    "            recv_chunk.copy_(recv_buffer)\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    def broadcast_mst(self, tensor: torch.tensor, root: int):\n",
    "        if self.world_size == 1:\n",
    "            return \n",
    "\n",
    "        left_child = 2 * self.rank + 1\n",
    "        right_child = 2 * self.rank + 2\n",
    "\n",
    "        if self.rank != root:\n",
    "            parent = (self.rank - 1) // 2\n",
    "            dist.recv(tensor, src=parent)\n",
    "\n",
    "        if left_child < self.world_size:\n",
    "            dist.send(tensor, dst=left_child)\n",
    "        if right_child < self.world_size:\n",
    "            dist.send(tensor, dst=right_child)\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    def scatter_mst(self, tensor: torch.tensor, root: int):\n",
    "        if self.world_size == 1:\n",
    "            return tensor\n",
    "        \n",
    "        def get_parent(rank):\n",
    "            return (rank - 1) // 2\n",
    "        \n",
    "        def get_children(rank):\n",
    "            left_child = 2 * rank + 1\n",
    "            right_child = 2 * rank + 2\n",
    "\n",
    "            children = []\n",
    "            if left_child < self.world_size:\n",
    "                children.append(left_child)\n",
    "            if right_child < self.world_size:\n",
    "                children.append(right_child)\n",
    "\n",
    "            return children\n",
    "        \n",
    "        def get_subtree_ranks(rank):\n",
    "            ranks = [rank]\n",
    "            children = get_children(rank)\n",
    "\n",
    "            for child in children:\n",
    "                ranks.extend(get_subtree_ranks(child))\n",
    "\n",
    "            return ranks\n",
    "        \n",
    "        chunks = torch.chunk(tensor, chunks=self.world_size, dim=-1)\n",
    "\n",
    "        if self.rank != root:\n",
    "            parent = get_parent(self.rank)\n",
    "            subtree_ranks = get_subtree_ranks(self.rank)\n",
    "            subtree_size = len(subtree_ranks)\n",
    "\n",
    "            chunks = torch.zeros(chunks[0].numel() * subtree_size, device=tensor.device)\n",
    "            dist.recv(chunks, src=parent)\n",
    "        \n",
    "        children = get_children(self.rank)\n",
    "        my_chunk = chunks[self.rank]\n",
    "\n",
    "        for child in children:\n",
    "            subtree_ranks = get_subtree_ranks(child)\n",
    "            send_chunks = torch.cat([chunks[r] for r in subtree_ranks], dim=-1)\n",
    "            dist.send(send_chunks, dst=child)\n",
    "\n",
    "        return my_chunk\n",
    "    \n",
    "    def gather_mst(self, tensor: torch.tensor, root: int):\n",
    "        if self.world_size == 1:\n",
    "            return tensor\n",
    "        \n",
    "        def get_parent(rank):\n",
    "            return (rank - 1) // 2\n",
    "        \n",
    "        def get_children(rank):\n",
    "            left_child = 2 * rank + 1\n",
    "            right_child = 2 * rank + 2\n",
    "\n",
    "            children = []\n",
    "            if left_child < self.world_size:\n",
    "                children.append(left_child)\n",
    "            if right_child < self.world_size:\n",
    "                children.append(right_child)\n",
    "\n",
    "            return children\n",
    "        \n",
    "        def get_subtree_size(rank):\n",
    "            size = 1\n",
    "            children = get_children(rank)\n",
    "\n",
    "            for child in children:\n",
    "                size += get_subtree_size(child)\n",
    "\n",
    "            return size\n",
    "        \n",
    "        children = get_children(self.rank)\n",
    "        child_data = []\n",
    "\n",
    "        for child in children:\n",
    "            size = get_subtree_size(child)\n",
    "            recv_buffer = torch.zeros_like(tensor.numel() * size, device=tensor.device)\n",
    "            dist.recv(recv_buffer, src=child)\n",
    "            child_data.append(recv_buffer)\n",
    "\n",
    "        data = None\n",
    "        if len(child_data) == 0:\n",
    "            data = tensor\n",
    "        elif len(child_data) == 1:\n",
    "            data = torch.concat([child_data[0], tensor])\n",
    "        else:\n",
    "            data = torch.concat([child_data[0], tensor, child_data[1]])\n",
    "        \n",
    "        if self.rank != root:\n",
    "            parent = get_parent(self.rank)\n",
    "            dist.send(data, dst=parent)\n",
    "            return None\n",
    "        else:\n",
    "            return data\n",
    "        \n",
    "    def reduce_tree(self, tensor: torch.Tensor, root: int, op=dist.ReduceOp.SUM):\n",
    "        if self.world_size == 1:\n",
    "            return tensor\n",
    "\n",
    "        def get_parent(rank):\n",
    "            return (rank - 1) // 2\n",
    "        \n",
    "        def get_children(rank):\n",
    "            left_child = 2 * rank + 1\n",
    "            right_child = 2 * rank + 2\n",
    "\n",
    "            children = []\n",
    "            if left_child < self.world_size:\n",
    "                children.append(left_child)\n",
    "            if right_child < self.world_size:\n",
    "                children.append(right_child)\n",
    "\n",
    "            return children\n",
    "\n",
    "        children = get_children(self.rank)\n",
    "        recv_buffer = torch.zeros_like(tensor, dtype=torch.float32)\n",
    "\n",
    "        for child in children:\n",
    "            dist.recv(recv_buffer, src=child)\n",
    "            \n",
    "            if op == dist.ReduceOp.SUM:\n",
    "                tensor.add_(recv_buffer)\n",
    "\n",
    "        if self.rank != root:\n",
    "            parent = get_parent(self.rank)\n",
    "            dist.send(tensor, dst=parent)\n",
    "            return None\n",
    "        else:\n",
    "            return tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
