{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3869f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927bbe68",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fef7f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.W = torch.randn(in_features, out_features)\n",
    "        self.b = torch.zeros(out_features)\n",
    "\n",
    "    def mse_loss(self, y_pred, y_true):\n",
    "        return torch.mean(torch.square(y_pred - y_true))\n",
    "    \n",
    "    def gradients(self, y_pred, y_true, X):\n",
    "        diff = (y_pred - y_true)\n",
    "        N = X.shape[0]\n",
    "\n",
    "        dw = (2 / N) * torch.matmul(X.T, diff)\n",
    "        db = (2 / N) * torch.sum(diff, dim=0)\n",
    "\n",
    "        return dw, db\n",
    "    \n",
    "    def backward(self, dw, db, learning_rate):\n",
    "        self.W -= learning_rate * dw\n",
    "        self.b -= learning_rate * db\n",
    "\n",
    "    def forward(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = torch.matmul(X, self.W) + self.b\n",
    "            loss = self.mse_loss(y_pred, y)\n",
    "            dw, db = self.gradients(y_pred, y, X)\n",
    "            self.backward(dw, db, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43bd5c",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, in_features, n_labels):\n",
    "        self.W = torch.randn(in_features, n_labels)\n",
    "        self.b = torch.zeros(n_labels)\n",
    "\n",
    "    def binary_cross_entropy(self, y_pred, y_true):\n",
    "        return - torch.mean(y_true * torch.log(y_pred) + (1. - y_true) * torch.log(1. - y_pred))\n",
    "    \n",
    "    def gradients(self, y_pred, y_true, X):\n",
    "        diff = y_pred - y_true\n",
    "        \n",
    "        dw = torch.matmul(X.T, diff) / X.shape[0]\n",
    "        db = torch.mean(diff, dim=0)\n",
    "\n",
    "        return dw, db\n",
    "    \n",
    "    def backward(self, dw, db, learning_rate):\n",
    "        self.W -= learning_rate * dw\n",
    "        self.b -= learning_rate * db\n",
    "\n",
    "    def forward(self, X, y, epochs, learning_rate):\n",
    "        for epoch in epochs:\n",
    "            y_pred = torch.sigmoid(torch.matmul(X, self.W) + self.b)\n",
    "            loss = self.binary_cross_entropy(y_pred, y)\n",
    "            dw, db = self.gradients(y_pred, y, X)\n",
    "            self.backward(dw, db, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7925653e",
   "metadata": {},
   "source": [
    "#### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62fa35cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters, max_iterations):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "        self.centroids = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = torch.randint(0, n_samples, (self.n_clusters,))\n",
    "\n",
    "        self.centroids = X[indices]\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            distances = torch.stack([torch.norm(X - c) for c in self.centroids], dim=1)\n",
    "            labels = torch.argmin(distances, dim=1)\n",
    "\n",
    "            new_centroids = torch.stack(torch.mean([X[labels == c] for c in range(self.n_clusters)], dim=0), dim=0)\n",
    "\n",
    "            if torch.allclose(new_centroids, self.centroids):\n",
    "                break\n",
    "\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "        return self.centroids\n",
    "    \n",
    "    def predict(self, x, y):\n",
    "        distances = torch.stack([torch.norm(x - c) for c in self.centroids], dim=1)\n",
    "        return torch.argmin(distances, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7645a3b7",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e12dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        for x in X:\n",
    "            distances = torch.norm(self.X - x, dim=1)\n",
    "            _, topk_indices = torch.topk(distances, k=self.k, largest=False)\n",
    "\n",
    "            labels = self.y[topk_indices]\n",
    "            pred = torch.mode(labels).values\n",
    "\n",
    "            predictions.append(pred)\n",
    "\n",
    "        return torch.stack(predictions, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e0d42",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9111f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data, n_components):\n",
    "    mean = torch.mean(data, dim=0)\n",
    "    std = torch.std(data, dim=0)\n",
    "\n",
    "    normalized = (data - mean) / (std + 1e-4)\n",
    "    n_samples = data.shape[0]\n",
    "\n",
    "    cov_matrix = torch.matmul(normalized.t(), normalized) / (n_samples - 1)\n",
    "\n",
    "    eigen_values, eigen_vectors = torch.linalg.eig(cov_matrix)\n",
    "    \n",
    "    sorted_eigen_indices = torch.argsort(eigen_values)[::-1]\n",
    "    sorted_eigen_values = eigen_values[sorted_eigen_indices]\n",
    "    sorted_eigen_vectors = eigen_vectors[:, sorted_eigen_indices]\n",
    "\n",
    "    principle_components = sorted_eigen_vectors[:, : n_components]\n",
    "\n",
    "    reduced_data = torch.matmul(normalized, principle_components)\n",
    "\n",
    "    return reduced_data, principle_components, eigen_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5450f1",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c5856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a02b033",
   "metadata": {},
   "source": [
    "#### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bd4a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    return torch.mean(torch.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b38b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_loss(y_true, y_pred):\n",
    "    return torch.mean(torch.abs(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7bc0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(y_pred, y_true, eps=1e-8):\n",
    "    return - torch.mean(y_true * torch.log(y_pred + eps) + (1. - y_true) * torch.log(1. - y_pred + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true, eps=1e-8):\n",
    "    return - torch.mean(y_true * torch.log(y_pred + eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59fceb2",
   "metadata": {},
   "source": [
    "#### Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4829d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7321004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    exp_x_ = torch.exp(-x)\n",
    "    exp_x = torch.exp(x)\n",
    "\n",
    "    return (exp_x - exp_x_) / (exp_x + exp_x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad69fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return torch.max(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9ad3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha):\n",
    "    return torch.max(x, alpha * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68ad9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PReLU:\n",
    "    def __init__(self):\n",
    "        self.p = torch.tensor(0.25, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.max(x, self.p * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b6db70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(x, alpha):\n",
    "    return torch.where(x > 0, x, alpha * (torch.exp(x) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98cef844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = torch.exp(x - torch.max(x))\n",
    "    return exp_x / torch.sum(exp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffbdc8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - torch.logsumexp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449de60",
   "metadata": {},
   "source": [
    "#### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5aefd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, params, learning_rate):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            p.data -= self.learning_rate * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8abe22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum:\n",
    "    def __init__(self, params, learning_rate, momentum):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.v = [torch.zeros_like(p) for p in params]\n",
    "\n",
    "    def step(self):\n",
    "        for i, p in enumerate(self.params):\n",
    "            self.v[i] = self.momentum * self.v[i] + (1. - self.momentum) * p.grad\n",
    "            p.data -= self.learning_rate * self.v[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, params, learning_rate, eps=1e-8):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eps = eps\n",
    "\n",
    "        self.cache = [torch.zeros_like(p) for p in params]\n",
    "\n",
    "    def step(self):\n",
    "        for i, p in enumerate(self.params):\n",
    "            self.cache[i] += p.grad ** 2\n",
    "            p.data -= (self.learning_rate / torch.sqrt(self.cache[i] + self.eps)) * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae718f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp:\n",
    "    def __init__(self, params, learning_rate, decay_rate, eps=1e-8):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.eps = eps\n",
    "\n",
    "        self.cache = [torch.zeros_like(p) for p in params]\n",
    "\n",
    "    def step(self):\n",
    "        for i, p in enumerate(self.params):\n",
    "            self.cache[i] = self.decay_rate * self.cache[i] + (1. - self.decay_rate) * p.grad ** 2\n",
    "            p.data -= (self.learning_rate / torch.sqrt(self.cache[i] + self.eps)) * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4861638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, params, learning_rate, beta1, beta2, eps=1e-8):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "\n",
    "        self.m = [torch.zeros_like(p) for p in params]\n",
    "        self.v = [torch.zeros_like(p) for p in params]\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "\n",
    "        for i, p in enumerate(self.params):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1. - self.beta1) * p.grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1. - self.beta2) * p.grad ** 2\n",
    "\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            p.data -= (self.learning_rate / torch.sqrt(v_hat + self.eps)) * m_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcdcf6a",
   "metadata": {},
   "source": [
    "### Neural Network Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36637513",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa45e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activations and Gradients\n",
    "### For loss functions: refer above\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1 + torch.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1. - a)\n",
    "\n",
    "def relu(z):\n",
    "    return torch.max(z, 0)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return torch.where(z >= 0, 1, 0)\n",
    "\n",
    "def tanh(a):\n",
    "    return (torch.exp(a) - torch.exp(-a)) / (torch.exp(a) + torch.exp(-a))\n",
    "\n",
    "def tanh_derivative(a):\n",
    "    return 1. - a ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bcb82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_features, activation='sigmoid'):\n",
    "        self.in_features = input_features\n",
    "        self.activation = activation\n",
    "\n",
    "        self.W = torch.randn(input_features)\n",
    "        self.b = 0.0\n",
    "        \n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = torch.dot(x, self.W) + self.b\n",
    "\n",
    "        if self.activation == 'sigmoid':\n",
    "            a = sigmoid(z)\n",
    "\n",
    "        self.cache = {\n",
    "            'x': x,\n",
    "            'z': z,\n",
    "            'a': a\n",
    "        }\n",
    "\n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        x = self.cache['x']\n",
    "        z = self.cache['z']\n",
    "        a = self.cache['a']\n",
    "\n",
    "        if self.activation == 'sigmoid':\n",
    "            dz = da * sigmoid_derivative(a)\n",
    "\n",
    "        self.dw = torch.dot(x.T, dz) / x.shape[0]\n",
    "        self.db = torch.mean(dz)\n",
    "\n",
    "        dx = torch.dot(dz.reshape(-1, 1), self.W.reshape(1, -1))\n",
    "\n",
    "        return dx\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        self.W = self.W - learning_rate * self.dw\n",
    "        self.b = self.b - learning_rate * self.db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49f9cb",
   "metadata": {},
   "source": [
    "#### Q1. Explain forward propagation in a neural network.\n",
    "- It is the process of passing the inputs through a sequence of layers to produce an output which is compared against the true labels using a loss function. At each layer:\n",
    "    - We compute the weighted sum of the inputs\n",
    "    - Apply activation functions to introduce non-linearity\n",
    "    - Pass the output to the next layer\n",
    "\n",
    "#### Q2. Why do we need activation functions? What happens without them?\n",
    "- Without activation functions, neural networks become purely linear. Stacking multiple linear layers is equivalent to a single linear layer.\n",
    "- Activation functions introduce non-linearity in the outputs, that help the networks to learn complex pattern in the data distribution.\n",
    "\n",
    "#### Q3. When would you MSE vs Cross entropy loss? \n",
    "- MSE is for regression problems where the task is produce continuous values - it penalizes errors quadratically.\n",
    "- CE is for classification because its probabilistically motivated (measures divergence between the distributions)\n",
    "\n",
    "#### Q4. What's the dying ReLU problem?\n",
    "- When inputs are consistently negative, ReLU neurons output 0 and have zero gradient, so they never update. \n",
    "- This can happen with bad initialization or very high learning rates. Solutions: Leaky ReLU, He initialization, lower learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50ca98",
   "metadata": {},
   "source": [
    "### Backpropagation Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0320fd9",
   "metadata": {},
   "source": [
    "#### Technical explanation\n",
    "- Backpropagation is an algorithm to compute the gradients of the loss function with respect to all the parameters in the neural network using the chain rule of calculus.\n",
    "- It's called back-propagation because we compute the gradients starting from the output layers and moving backwards towards the inputs\n",
    "    - Before backprop, computing gradients for deep networks required calculating derivatives separately for each parameter - computationally not reasonable\n",
    "\n",
    "#### Computational Graphs\n",
    "- A computation graph is a directed acyclic graph where: \n",
    "    - Nodes = operations or variables\n",
    "    - Edges = data dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3068c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50dcf14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1. / (1 + torch.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1. - a)\n",
    "\n",
    "def relu(z):\n",
    "    return torch.max(z, 0)\n",
    "\n",
    "def relu_derivative(a):\n",
    "    return torch.where(a > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc32d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNetwork:\n",
    "    def __init__(self, in_features, hidden_features, out_features, learning_rate):\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.W1 = torch.randn(in_features, hidden_features)\n",
    "        self.b1 = torch.zeros(hidden_features)\n",
    "\n",
    "        self.W2 = torch.randn(hidden_features, out_features)\n",
    "        self.b2 = torch.zeros(out_features)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        z1 = torch.matmul(x, self.W1) + self.b1\n",
    "        a1 = relu(z1)\n",
    "\n",
    "        z2 = torch.matmul(a1, self.W2) + self.b2\n",
    "        a2 = sigmoid(z2)\n",
    "\n",
    "        self.cache = {\n",
    "            'x': x,\n",
    "            'z1': z1, \n",
    "            'a1': a1,\n",
    "            'z2': z2,\n",
    "            'a2': a2 \n",
    "        }\n",
    "\n",
    "        return a2\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        x = self.cache['x']\n",
    "        z1 = self.cache['z1']\n",
    "        a1 = self.cache['a1']\n",
    "        z2 = self.cache['z2']\n",
    "        a2 = self.cache['a2']\n",
    "\n",
    "        n_samples = x.shape[0]\n",
    "\n",
    "        dldz2 = a2 - y_true\n",
    "\n",
    "        dldw2 = (dldz2 @ a2.T) / n_samples\n",
    "        dldb2 = torch.sum(dldz2, dim=0, keepdim=True) / n_samples\n",
    "\n",
    "        dlda1 = (dldz2 @ self.W2.T)\n",
    "        dldz1 = dlda1 * relu_derivative(z1)\n",
    "\n",
    "        dldw1 = (dldz1 @ a1.T) / n_samples\n",
    "        dldb1 = torch.sum(dldz1, dim=0, keepdim=True) / n_samples\n",
    "\n",
    "        self.gradients = {\n",
    "            'dw1': dldw1,\n",
    "            'db1': dldb1,\n",
    "            'dw2': dldw2,\n",
    "            'db2': dldb2\n",
    "        }\n",
    "\n",
    "        return self.gradients\n",
    "    \n",
    "    def update_paramaters(self):\n",
    "        self.W1 -= self.lr * self.gradients['dw1']\n",
    "        self.b1 -= self.lr * self.gradients['db1']\n",
    "        self.W2 -= self.lr * self.gradients['dw2']\n",
    "        self.b2 -= self.lr * self.gradients['db2']\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true, eps=1e-8):\n",
    "        loss = - torch.mean(y_true * torch.log(y_pred + eps) + (1. - y_true) * torch.log(1. - y_pred + eps))\n",
    "        return loss\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        y_pred = self.forward(X)\n",
    "        loss = self.compute_loss(y_pred, y)\n",
    "        self.backward(y)\n",
    "        self.update_paramaters()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0956d896",
   "metadata": {},
   "source": [
    "#### Q5. Explain Backpropagation\n",
    "- Backpropagation is an algorithm to compute the gradients of the loss function with respect to all the parameters using the chain rule of the calculus.\n",
    "- Starting from the loss, we compute how much each parameter contributed to the error by propagating gradients backward through the network.\n",
    "- At each layer: \n",
    "    - Receive gradient from the layer above\n",
    "    - Compute local gradient through activation\n",
    "    - Compute parameter gradients \n",
    "    - Pass gradient to layer below\n",
    "\n",
    "#### Q6. Why do we need to cache values during the forward pass?\n",
    "- During backprop, we need values from the forward pass to compute gradients.\n",
    "- Without caching, we'd have to recompute the forward pass during backprop, doubling computation.\n",
    "\n",
    "#### Q7. What's the vanishing gradient problem? How does it relate to backprop?\n",
    "- The vanishing gradient problem occurs when gradients become exponentially small as they propagate backward through many layers. This happens because\n",
    "    - Chain rule multiplies gradients\n",
    "    - Sigmoid/Tanh derivatives are <1\n",
    "    - Deep networks suffer the most\n",
    "\n",
    "#### Q8. Why do we divide gradients by batch size?\n",
    "- Scale invariance: Loss should be comparable regardless of batch size. \n",
    "- If we sum instead of average, larger batches would have larger gradients, requiring different learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32f9ca",
   "metadata": {},
   "source": [
    "## Core Layer Types\n",
    "\n",
    "#### Dense Layers\n",
    "- A dense layer performs a transformation followed by an activation: \n",
    "    - output = activation(Wx + b)\n",
    "- Every input is connected to every output - a fully connected topology.\n",
    "\n",
    "#### Convolution Layers\n",
    "- Convolution applies a learnable filter across spatial dimensions\n",
    "- Key Properties\n",
    "    - Local connectivity: Each output neuron only look at a small region (receptive field)\n",
    "    - Parameter sharing: Same filter used across entire spatial dimension\n",
    "    - Translation equivariance: If input shifts, output shifts equivalently\n",
    "- Why convolution for images? \n",
    "    - Spatial structure: Pixels nearby are related\n",
    "    - Parameter Efficiency\n",
    "    - Translation equivariance: Car detector works anywhere in the image\n",
    "\n",
    "- H_out: (H - K + 2P) / S + 1\n",
    "- W_out: (W - K + 2P) / S + 1\n",
    "\n",
    "- Padding Types: \n",
    "    - Valid: No padding, output shrinks\n",
    "    - Same: Pad so output size = input_size\n",
    "\n",
    "#### Pooling Layers\n",
    "- Pooling performs downsampling by aggregating values in a local region\n",
    "- Benefits: \n",
    "    - Reduces spatial dimension\n",
    "    - Reduces computation\n",
    "    - No learnable parameters\n",
    "- Types\n",
    "    - Average Pooling\n",
    "    - Max Pooling\n",
    "    - Min Pooling\n",
    "    - Global Average Pooling: \n",
    "\n",
    "#### Initialization Approaches\n",
    "1. Xavier\n",
    "2. He\n",
    "3. LeCun\n",
    "\n",
    "- Why initialization matters?\n",
    "    - Too small -> vanishing gradients\n",
    "    - Too big -> exploding gradients\n",
    "    - Goal: Keep variance of activations roughly constant across layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c75fcf",
   "metadata": {},
   "source": [
    "## Normalization Techniques\n",
    "- Problem: During training, the distribution of the layer inputs changes as parameters get updated (called Internal Covariant Shift): This causes: \n",
    "    - Unstable training\n",
    "    - Slow convergence\n",
    "    - Careful initializations\n",
    "\n",
    "- Solution: Normalize activations to have consistent statistics\n",
    "\n",
    "#### Batch Normalization: \n",
    "- Normalize each feature across the batch to have mean = 0, variance = 1, then learn optimal mean/variance with trainable parameters\n",
    "- Why scale and shift ? \n",
    "    - Pure normalizattion to N(0, 1) might be too restrictive. alpha and beta let the network learn the optimal distribution.\n",
    "- Training vs Inference\n",
    "    - Training: Use batch statistics\n",
    "    - Inference: Use running averages accumulated during training (exponential moving average)\n",
    "- Suited for CNNs\n",
    "\n",
    "#### Layer Normalization: \n",
    "- BatchNorm normalizes across the batch dimension. LayerNorm normalizes across the feature dimension for each sample independently\n",
    "- Why LayerNorm ?\n",
    "    - Better for Sequences\n",
    "    - Used in Transformers\n",
    "- Suited for RNNs and Transformers\n",
    "\n",
    "#### Instance Normalization: \n",
    "- Normalize each channel independently for each sample. \n",
    "- Like LayerNorm but for spatial data.\n",
    "- Helps with image generation in case of GANs\n",
    "\n",
    "#### Group Normalization\n",
    "- Divide channels into groups and normalize with each group. Middle ground between LayerNorm and InstanceNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ff8225",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc0aeedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm: \n",
    "    def __init__(self, d_model, momentum, eps):\n",
    "        self.d_model = d_model\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = torch.ones(d_model)\n",
    "        self.beta = torch.zeros(d_model)\n",
    "\n",
    "        self.running_mean = torch.zeros(d_model)\n",
    "        self.running_var = torch.ones(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "            mean = torch.mean(x, dim=(0, 1))\n",
    "            var = torch.var(x, dim=(0, 1))\n",
    "\n",
    "            x_normalised = (x - mean) / (torch.sqrt(var) + self.eps)\n",
    "\n",
    "            gamma_reshaped = self.gamma.reshape(1, 1, d_model)\n",
    "            beta_reshaped = self.beta.reshape(1, 1, d_model)\n",
    "\n",
    "            output = gamma_reshaped * x_normalised + beta_reshaped\n",
    "\n",
    "            self.running_mean = self.momentum * self.running_mean + (1. - self.momentum) * mean\n",
    "            self.running_var = self.momentum * self.running_var + (1. - self.momentum) * var\n",
    "        else:\n",
    "            mean_reshaped = torch.reshape(self.running_mean, (1, 1, d_model))\n",
    "            var_reshaped = torch.reshape(self.running_var, (1, 1, d_model))\n",
    "\n",
    "            x_normalized = (x - mean_reshaped) / (torch.sqrt(var_reshaped) + self.eps)\n",
    "\n",
    "            gamma_reshaped = self.gamma.reshape(1, 1, d_model)\n",
    "            beta_reshaped = self.beta.reshape(1, 1, d_model)\n",
    "\n",
    "            output = gamma_reshaped * x_normalised + beta_reshaped\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd41be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm: \n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        self.d_model = d_model\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = torch.ones(d_model)\n",
    "        self.beta = torch.zeros(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, axis=-1, keepdims=True)\n",
    "        var = torch.var(x, axis=-1, keepdims=True)\n",
    "        \n",
    "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        out = self.gamma * x_normalized + self.beta\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a00f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p=0.7):\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            self.mask = (torch.randn(*x.shape) < self.p)\n",
    "            out = x * self.mask / self.p\n",
    "        else:\n",
    "            out = x\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        if self.training:\n",
    "            return dout * self.mask / self.p\n",
    "        else:\n",
    "            return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd41b50",
   "metadata": {},
   "source": [
    "#### Q1. Explain dropout and why its preventing overfitting.\n",
    "- Dropout randomly sets a fraction of the neuron activations to zero during training. \n",
    "- For each training iteration, we use a different sub-network.\n",
    "\n",
    "Why it prevents overfitting ?\n",
    "- Prevents co-adaptation where the neurons rely less on the other neurons\n",
    "- Ensemble effect: Training 2^N different sub-networks, final model is an ensemble\n",
    "- Forces neurons to more independent and useful\n",
    "\n",
    "#### Q2. Whats the difference between L1 and L2 regularization? \n",
    "- Both add a penatly term but with different effects\n",
    "- L2: \n",
    "    - lambda * W^2\n",
    "    - Weights are never zeroed-out\n",
    "    - Generalizes using all the weights\n",
    "    - Use when: \n",
    "        - Want a general regularization\n",
    "        - All features are potentially relevant\n",
    "- L1: \n",
    "    - lambda * |W|\n",
    "    - Feature selection\n",
    "    - Drives weights towards zero\n",
    "    - Use when: \n",
    "        - Want feature selection\n",
    "        - High-dimensional data with irrelevant features\n",
    "\n",
    "#### Q3. Explain inverted dropout. Why scale during training instead of testing? \n",
    "- During training, a sub-network contributes to the output and during inference, the full ensemble model is contributing to the output.\n",
    "- The output of the sub-network and the full ensemble model need to be in the same range, as a result we need to scale during training.\n",
    "\n",
    "#### Q4. Why does early stopping work as regularization?\n",
    "- Early stopping acts as implicit regularization by limiting training time:\n",
    "    - Early in training: model learns simple patterns (high bias, low variance)\n",
    "    - Later in training: model fits noise (low bias, high variance)\n",
    "    - Stopping early finds the sweet spot\n",
    "\n",
    "#### Q5: What's label smoothing and why does it help?\n",
    "- Label smoothing replaces hard target with soft target: \n",
    "    - To smoothen the target distribution\n",
    "    - Prevent model over-confidence\n",
    "    - Empirically improves test accuracy\n",
    "\n",
    "#### Q6: Why shouldn't you use both Dropout and BatchNorm together?\n",
    "- BatchNorm computes statistics over the batch\n",
    "- Dropout randomly zeros activations\n",
    "- BatchNorm statistics become unreliable with dropout's noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da32d4",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "Technical: Minimize the loss function to reach the global minimum for all the parameters\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "- Move in the direction opposite to the gradient (steepest descent)\n",
    "- Batch GD, Stochastic GD, Mini-Batch GD\n",
    "- Outliers cause un-stable gradients and learning\n",
    "- Oscillating gradients in case of all positive or all negative activations\n",
    "\n",
    "#### Stochastic Gradient Descent with Momentum\n",
    "- Include a velocity which is a exponential moving average of the gradients\n",
    "- Reduces the affects of the outlier gradients since the velocity value dominates\n",
    "- Causes dampening oscillating gradients\n",
    "- Accelerates in consistent direction\n",
    "- Helps escape local minimum\n",
    "\n",
    "#### AdaGrad\n",
    "- Different parameters learn and update at different rates in the training process\n",
    "- As a result, they need dynamic learning rate.\n",
    "- Goal: higher learning rate for slow-converging parameters\n",
    "- Goal: lower learning rate for fast-converging parameters\n",
    "- Problem: G only accumulated and never decreases, as a result learning rate approaches zero\n",
    "\n",
    "#### RMSProp\n",
    "- Fix AdaGrad by using exponential moving average for the accumulating G^2 values\n",
    "- Learning can increase or decrease depening on scenario\n",
    "\n",
    "#### Adam\n",
    "- Combines momentum and adaptive learning rate approaches for effective covergence\n",
    "- Bias correction to handle the incorrect gradient values in the beginning of the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39fa5c5",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    "- Bad initialization causes: \n",
    "    - Vanishing or Exploding gradients\n",
    "    - Symmetry Problems \n",
    "    - Slow convergence\n",
    "\n",
    "- The problems: \n",
    "    - All zeros: Symmetry - all neurons learn the same thing\n",
    "    - Too large: Exploding activation / gradients\n",
    "    - Too small: Vanishing activation / gradients\n",
    "\n",
    "Goal: We want the variance of activations and gradients to stay roughly constant across layers\n",
    "\n",
    "1. Xavier\n",
    "    - Use with sigmoid / tanh\n",
    "    - All neurons active, balance forward and backward\n",
    "    - sigmoid / tanh have a symmetric nature to forward and backward pass, so both the directions need to be considered\n",
    "\n",
    "2. He\n",
    "    - Use with ReLU / LeakyReLU\n",
    "    - Half neurons die, need 2x variance\n",
    "    - During the forward, ~50% neurons are deactivated by ReLU, as a result the variance is halved (needs compensation)\n",
    "    - Asymmetric nature of forward and the backward processes, forward pass is important for ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846691ac",
   "metadata": {},
   "source": [
    "## Learning Rate Schedulers\n",
    "- Fixed learning rate is suboptimal.\n",
    "- Early in training, we need large steps to explore\n",
    "- Late in training, we need small steps for precise convergence\n",
    "\n",
    "#### Step Decay\n",
    "- Simple, works well\n",
    "- Requires tuning drop points\n",
    "\n",
    "#### Exponential Decay\n",
    "- Smooth, continuous\n",
    "- Choosing decay rate - tricky\n",
    "\n",
    "#### Cosine annealing\n",
    "- Smooth, \n",
    "- Requires knowing total epochs\n",
    "\n",
    "#### Warmup - The Critical Addition\n",
    "- Linearly increase LR from near-zero to target over initial - training steps\n",
    "- Prevents early instability from large gradients and random initialization\n",
    "- Why warmup is essential ? \n",
    "    - Prevent unstable training in the initial steps\n",
    "    - Required for large batch size\n",
    "    - Might help in escaping from local - minimum or saddle points\n",
    "\n",
    "#### Common combinations\n",
    "- Warmup + Cosine Annealing (BERT, Vision Transformers, SOTA models)\n",
    "- Warmup + step decay (CNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef872881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
