{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5879ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9391677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    rank = os.environ['RANK']\n",
    "    world_size = os.environ['WORLD_SIZE']\n",
    "\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    dist.init_process_group(\n",
    "        backend='nccl',\n",
    "        rank=rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "    torch.cuda.set_device(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "668bc17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2248784",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineStage(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7be32722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stages(hidden_stages, n_layers, world_size):\n",
    "    n_layers_per_stage = n_layers // world_size\n",
    "\n",
    "    all_layers = []\n",
    "    for _ in range(n_layers):\n",
    "        layers = nn.Sequential(\n",
    "            nn.Linear(hidden_stages, hidden_stages),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        all_layers.append(layers)\n",
    "\n",
    "    stages = []\n",
    "    for idx in range(world_size):\n",
    "        start_idx = idx * n_layers_per_stage\n",
    "        end_idx = (idx + 1) * n_layers_per_stage\n",
    "\n",
    "        layers = all_layers[start_idx : end_idx]\n",
    "        stages.append(PipelineStage(layers))\n",
    "\n",
    "    return stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a79fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPipePipeline:\n",
    "    \"\"\"\n",
    "    GPipe-style pipeline parallelism.\n",
    "    \n",
    "    Phases:\n",
    "    1. Fill: Forward all micro-batches (GPU 0 → GPU N)\n",
    "    2. Drain: Backward all micro-batches (GPU N → GPU 0)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        stage: PipelineStage, \n",
    "        rank: int, \n",
    "        world_size: int, \n",
    "        n_micro_batches: int,\n",
    "        micro_batch_size: int,\n",
    "        input_shape: tuple, \n",
    "        dtype=torch.float32\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            stage: The model stage (layers) for this rank\n",
    "            rank: This GPU's rank\n",
    "            world_size: Total number of pipeline stages\n",
    "            micro_batches: Number of micro-batches per training step\n",
    "            micro_batch_size: Size of each micro-batch\n",
    "            input_shape: Shape of input WITHOUT batch dimension\n",
    "            dtype: Data type for tensors\n",
    "        \"\"\"\n",
    "        self.stage = stage\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.n_micro_batches = n_micro_batches\n",
    "        self.micro_batch_size = micro_batch_size\n",
    "        self.input_shape = input_shape\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.next_rank = self.rank + 1 if self.rank < self.world_size - 1 else None\n",
    "        self.prev_rank = self.rank - 1 if self.rank > 0 else None\n",
    "\n",
    "    def send(self, tensor: torch.Tensor, send_rank: int):\n",
    "        if send_rank is not None:\n",
    "            dist.send(tensor.contiguous(), dst=send_rank)\n",
    "        \n",
    "    def recv(self, tensor_buffer: torch.tensor, recv_rank: int):\n",
    "        if self.prev_rank is not None:\n",
    "            dist.recv(tensor_buffer, src=recv_rank)\n",
    "            return tensor_buffer\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def forward(self, micro_batch: torch.tensor):\n",
    "        micro_batch.requires_grad = True\n",
    "        output = self.stage(micro_batch)\n",
    "        \n",
    "        return output, micro_batch\n",
    "    \n",
    "    def backward(self, grad: torch.tensor, output: torch.tensor, saved_input: torch.tensor):\n",
    "        output.backward(grad)\n",
    "        return saved_input.grad\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"\n",
    "        GPipe training step with two phases.\n",
    "        \n",
    "        Args:\n",
    "            batch: Input batch (only used by rank 0, can be None for other ranks)\n",
    "        \"\"\"\n",
    "\n",
    "        saved_inputs = []\n",
    "        outputs = []\n",
    "\n",
    "        for idx in range(self.n_micro_batches):\n",
    "            if self.rank == 0:\n",
    "                start_idx = idx * self.micro_batch_size\n",
    "                end_idx = (idx + 1) * self.micro_batch_size\n",
    "\n",
    "                micro_batch = batch[start_idx : end_idx]\n",
    "            else:\n",
    "                tensor_buffer = torch.zeros((self.micro_batch_size, *self.input_shape), dtype=self.dtype)\n",
    "                micro_batch = self.recv(tensor_buffer, self.prev_rank)\n",
    "\n",
    "            output, saved_input = self.forward(micro_batch)\n",
    "\n",
    "            outputs.append(output)\n",
    "            saved_inputs.append(saved_input)\n",
    "\n",
    "            self.send(saved_input, self.next_rank)\n",
    "\n",
    "        for idx in range(self.micro_batch_size - 1, -1, -1):\n",
    "            if self.rank == self.world_size - 1:\n",
    "                grad_input = torch.ones_like(outputs[idx], dtype=self.dtype)\n",
    "            else:\n",
    "                tensor_buffer = torch.zeros_like(output[idx], dtype=self.dtype)\n",
    "                grad_input = self.recv(tensor_buffer, self.next_rank)\n",
    "\n",
    "            grad_output = self.backward(grad_input, outputs[idx], saved_input[idx])\n",
    "\n",
    "            self.send(grad_output, self.prev_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2097e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneFOneBPipeline:\n",
    "    def __init__(\n",
    "        self, \n",
    "        stage: PipelineStage, \n",
    "        rank: int, \n",
    "        world_size: int, \n",
    "        n_micro_batches: int,\n",
    "        micro_batch_size: int,\n",
    "        input_shape: tuple, \n",
    "        dtype=torch.float32\n",
    "    ):\n",
    "        self.stage = stage\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.n_micro_batches = n_micro_batches\n",
    "        self.micro_batch_size = micro_batch_size\n",
    "        self.input_shape = input_shape\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.next_rank = self.rank + 1 if self.rank < self.world_size - 1 else None\n",
    "        self.prev_rank = self.rank - 1 if self.rank > 0 else None\n",
    "\n",
    "        self.warmup_states = world_size - rank - 1\n",
    "        self.active_states = n_micro_batches - self.warmup_states\n",
    "\n",
    "    def send(self, tensor: torch.tensor, send_rank: int):\n",
    "        if send_rank is not None:\n",
    "            dist.send(tensor.contiguous(), dst=send_rank)\n",
    "\n",
    "    def recv(self, tensor_buffer: torch.tensor, recv_rank: int):\n",
    "        if recv_rank is not None:\n",
    "            dist.recv(tensor_buffer, src=recv_rank)\n",
    "            return tensor_buffer\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def forward(self, micro_batch: torch.tensor):\n",
    "        micro_batch.requires_grad = True\n",
    "        output = self.stage(micro_batch)\n",
    "\n",
    "        return output, micro_batch\n",
    "    \n",
    "    def backward(self, grad_input: torch.tensor, output: torch.tensor, saved_input: torch.tensor):\n",
    "        output.backward(grad_input)\n",
    "        return saved_input.grad\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        saved_inputs = []\n",
    "        outputs = []\n",
    "\n",
    "        for idx in range(self.warmup_states):\n",
    "            if self.rank == 0:\n",
    "                start_idx = idx * self.micro_batch_size\n",
    "                end_idx = (idx + 1) * self.micro_batch_size\n",
    "\n",
    "                micro_batch = batch[start_idx : end_idx]\n",
    "            else:\n",
    "                tensor_buffer = torch.zeros((self.micro_batch_size, *self.input_shape), dtype=self.dtype)\n",
    "                micro_batch = self.recv(tensor_buffer, self.prev_rank)\n",
    "\n",
    "            output, saved_input = self.forward(micro_batch)\n",
    "\n",
    "            outputs.append(output)\n",
    "            saved_inputs.append(saved_input)\n",
    "\n",
    "            self.send(output, self.next_rank)\n",
    "\n",
    "        for idx in range(self.active_states):\n",
    "            if self.rank == 0:\n",
    "                start_idx = (idx + self.warmup_states) * self.micro_batch_size\n",
    "                end_idx = (idx + 1 + self.warmup_states) * self.micro_batch_size\n",
    "\n",
    "                micro_batch = batch[start_idx : end_idx]\n",
    "            else:\n",
    "                tensor_buffer = torch.zeros((self.micro_batch_size, *self.input_shape), dtype=self.dtype)\n",
    "                micro_batch = self.recv(tensor_buffer, self.prev_rank)\n",
    "\n",
    "            output, saved_input = self.forward(micro_batch)\n",
    "\n",
    "            outputs.append(output)\n",
    "            saved_inputs.append(saved_input)\n",
    "\n",
    "            self.send(output, self.next_rank)\n",
    "\n",
    "            output = outputs.pop(0)\n",
    "            saved_input = saved_inputs.pop(0)\n",
    "\n",
    "            if self.rank == self.world_size - 1:\n",
    "                grad_input = torch.ones_like(output, dtype=self.dtype)\n",
    "            else:\n",
    "                tensor_buffer = torch.zeros_like(output, dtype=self.dtype)\n",
    "                grad_input = self.recv(tensor_buffer, self.next_rank)\n",
    "\n",
    "            grad_output = self.backward(grad_input, output, saved_input)\n",
    "\n",
    "            self.send(grad_output, self.prev_rank)\n",
    "\n",
    "        for idx in range(self.warmup_states):\n",
    "            output = outputs[idx]\n",
    "            saved_input = saved_inputs[idx]\n",
    "\n",
    "            if self.rank == self.world_size - 1:\n",
    "                grad_input = torch.ones_like(output, dtype=self.dtype)\n",
    "            else:\n",
    "                tensor_buffer = torch.zeros_like(output, dtype=self.dtype)\n",
    "                grad_input = self.recv(tensor_buffer, self.next_rank)\n",
    "\n",
    "            grad_output = self.backward(grad_input, output, saved_input)\n",
    "\n",
    "            self.send(grad_output, self.prev_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "299ae89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpipe():\n",
    "    rank = os.environ['rank']\n",
    "    world_size = os.environ['world_size']\n",
    "\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    hidden_size = 512\n",
    "    num_layers = 16\n",
    "    batch_size = 32\n",
    "    num_micro_batches = 8\n",
    "    micro_batch_size = batch_size // num_micro_batches  # = 4\n",
    "    \n",
    "    stages = create_stages(hidden_size, num_layers, world_size)\n",
    "    stage = stages[rank].to(rank)\n",
    "\n",
    "    pipeline = GPipePipeline(\n",
    "        stage=stage,\n",
    "        rank=rank,\n",
    "        world_size=world_size,\n",
    "        micro_batches=num_micro_batches,\n",
    "        micro_batch_size=micro_batch_size,\n",
    "        input_shape=(hidden_size,),\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(stage.parameters(), lr=1e-3)\n",
    "\n",
    "    for i in range(10):\n",
    "        if rank == 0:\n",
    "            batch = torch.randn(batch_size, hidden_size, device=rank)\n",
    "        else:\n",
    "            batch = None\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pipeline.train_step(batch)\n",
    "        optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
