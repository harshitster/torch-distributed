{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19072899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be7b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rank(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    dist.init_process_group(\n",
    "        backend='nccl',\n",
    "        rank=rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "    torch.cuda.set_device(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4745de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_rank():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "005c670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineStage(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffea2990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_stages(hidden_size, n_layers, world_size):\n",
    "    n_layers_per_stage = n_layers // world_size\n",
    "\n",
    "    all_layers = []\n",
    "    for _ in range(n_layers):\n",
    "        all_layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.GELU()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    stages = []\n",
    "    for i in range(world_size):\n",
    "        start_idx = i * n_layers_per_stage\n",
    "        end_idx = start_idx + n_layers_per_stage\n",
    "\n",
    "        stage_layers = all_layers[start_idx: end_idx]\n",
    "        stages.append(PipelineStage(stage_layers))\n",
    "\n",
    "    return stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140bc4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPipePipeline:\n",
    "    \"\"\"\n",
    "    GPipe-style pipeline parallelism.\n",
    "    \n",
    "    Phases:\n",
    "    1. Fill: Forward all micro-batches (GPU 0 → GPU N)\n",
    "    2. Drain: Backward all micro-batches (GPU N → GPU 0)\n",
    "    \"\"\"\n",
    "    def __init__(self, stage, rank, world_size, n_micro_batches, micro_batch_size, input_size, dtype=torch.float32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            stage: The model stage (layers) for this rank\n",
    "            rank: This GPU's rank\n",
    "            world_size: Total number of pipeline stages\n",
    "            micro_batches: Number of micro-batches per training step\n",
    "            micro_batch_size: Size of each micro-batch\n",
    "            input_shape: Shape of input WITHOUT batch dimension\n",
    "            dtype: Data type for tensors\n",
    "        \"\"\"\n",
    "        self.stage = stage\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.n_micro_batches = n_micro_batches\n",
    "        self.micro_batch_size = micro_batch_size\n",
    "        self.input_size = input_size\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.prev_rank = self.rank - 1 if self.rank > 0 else None\n",
    "        self.next_rank = self.rank + 1 if self.rank < self.world_size - 1 else None\n",
    "\n",
    "    def send(self, activation):\n",
    "        if self.next_rank is not None:\n",
    "            dist.send(activation.contiguous(), dst=self.next_rank)\n",
    "\n",
    "    def recv(self, shape):\n",
    "        if self.prev_rank is not None:\n",
    "            tensor = torch.zeros(shape, dtype=self.dtype, device=self.rank)\n",
    "            dist.recv(tensor, src=self.prev_rank)\n",
    "            return tensor\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def forward_step(self, micro_batch):\n",
    "        micro_batch.required_grad = True\n",
    "        batch_output = self.stage(micro_batch)\n",
    "\n",
    "        return batch_output, micro_batch\n",
    "    \n",
    "    def backward_step(self, output, grad_output, input):\n",
    "        output.backward(grad_output)\n",
    "        return input.grad\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"\n",
    "        GPipe training step with two phases.\n",
    "        \n",
    "        Args:\n",
    "            batch: Input batch (only used by rank 0, can be None for other ranks)\n",
    "        \"\"\"\n",
    "\n",
    "        saved_inputs = []\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(self.n_micro_batches):\n",
    "            if self.rank == 0:\n",
    "                start_idx = i * self.micro_batch_size\n",
    "                end_idx = start_idx + self.micro_batch_size\n",
    "\n",
    "                micro_batch = batch[start_idx : end_idx]\n",
    "            else:\n",
    "                micro_batch = self.recv((self.micro_batch_size, *self.input_size))\n",
    "\n",
    "            saved_inputs.append(micro_batch)\n",
    "            output = self.forward_step(micro_batch)\n",
    "            outputs.append(output)\n",
    "\n",
    "            self.forward(output)\n",
    "\n",
    "        for i in range(self.n_micro_batches - 1, -1, -1):\n",
    "            saved_input = saved_inputs[i]\n",
    "            output = outputs[i]\n",
    "\n",
    "            if self.rank == self.world_size - 1:\n",
    "                grad_output = torch.ones_like(output, dtype=self.dtype, device=self.rank)\n",
    "            else:\n",
    "                grad_output = self.recv(output.shape)\n",
    "\n",
    "            grad_input = self.backward_step(output, grad_output, saved_input)\n",
    "            if grad_input is not None:\n",
    "                self.send(grad_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d56fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneFOneBPipeline:\n",
    "    def __init__(self, stage, rank, world_size, n_micro_batches, micro_batch_size, input_size, dtype=torch.float32):\n",
    "        self.stage = stage\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.n_micro_batches = n_micro_batches\n",
    "        self.micro_batch_size = micro_batch_size\n",
    "        self.input_size = input_size\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.prev_rank = self.rank - 1 if self.rank > 0 else None\n",
    "        self.next_rank = self.rank + 1 if self.rank < self.world_size - 1 else None\n",
    "\n",
    "        self.warmup_states = world_size - rank - 1\n",
    "        self.steady_states = n_micro_batches - self.warmup_states\n",
    "\n",
    "    def send(self, activations):\n",
    "        if self.next_rank is not None:\n",
    "            dist.send(activations.contiguous(), dst=self.next_rank)\n",
    "\n",
    "    def recv(self, shape):\n",
    "        if self.prev_rank is not None:\n",
    "            tensor = torch.zeros(shape, dtype=self.dtype, device=self.rank)\n",
    "            dist.recv(tensor, src=self.prev_rank)\n",
    "            return tensor\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def forward_step(self, micro_batch):\n",
    "        micro_batch.requires_grad = True\n",
    "        output = self.stage(micro_batch)\n",
    "        return output, micro_batch\n",
    "    \n",
    "    def backward_step(self, output, grad_output, input):\n",
    "        output.backwards(grad_output)\n",
    "        return input.grad\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"\n",
    "        1F1B training step with three phases.\n",
    "        \n",
    "        Args:\n",
    "            batch: Input batch (only used by rank 0, can be None for other ranks)\n",
    "        \"\"\"\n",
    "\n",
    "        saved_inputs = []\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(self.warmup_states):\n",
    "            if self.rank == 0:\n",
    "                start_idx = i * self.micro_batch_size\n",
    "                end_idx = start_idx + self.micro_batch_size\n",
    "\n",
    "                micro_batch = batch[start_idx : end_idx]\n",
    "            else:\n",
    "                micro_batch = self.recv((self.micro_batch_size, *self.input_size))\n",
    "            \n",
    "            output = self.forward_step(micro_batch)\n",
    "            self.send(output)\n",
    "\n",
    "            saved_inputs.append(micro_batch)\n",
    "            outputs.append(output)\n",
    "\n",
    "        for i in range(self.steady_states):\n",
    "            micro_batch_idx = i + self.warmup_states\n",
    "\n",
    "            if self.rank == 0:\n",
    "                start_idx = micro_batch_idx * self.micro_batch_size\n",
    "                end_idx = start_idx + self.micro_batch_size\n",
    "\n",
    "                micro_batch = batch[start_idx : end_idx]\n",
    "            else:\n",
    "                micro_batch = self.recv((self.micro_batch_size, *self.input_size))\n",
    "\n",
    "            output = self.forward_step(micro_batch)\n",
    "            self.send(output)\n",
    "\n",
    "            saved_inputs.append(micro_batch)\n",
    "            outputs.append(output)\n",
    "\n",
    "            output = outputs.pop(0)\n",
    "            saved_input = saved_inputs.pop(0)\n",
    "\n",
    "            if self.rank == self.world_size - 1:\n",
    "                grad_output = torch.ones_like(output, dtype=self.dtype, device=self.rank)\n",
    "            else:\n",
    "                grad_output = self.recv(output.shape)\n",
    "\n",
    "            grad_input = self.backward_step(output, grad_output, saved_input)\n",
    "            if grad_input is not None:\n",
    "                self.send(grad_input)\n",
    "\n",
    "        for i in range(self.warmup_states):\n",
    "            output = outputs.pop(0)\n",
    "            saved_input = saved_inputs.pop(0)\n",
    "\n",
    "            if self.rank == self.world_size - 1:\n",
    "                grad_output = torch.ones_like(output, dtype=self.dtype, device=self.rank)\n",
    "            else:\n",
    "                grad_output = self.recv(output.shape)\n",
    "\n",
    "            grad_input = self.backward_step(output, grad_output, saved_input)\n",
    "            if grad_input is not None:\n",
    "                self.send(grad_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpipe():\n",
    "    rank = os.environ['rank']\n",
    "    world_size = os.environ['world_size']\n",
    "\n",
    "    setup_rank(rank, world_size)\n",
    "\n",
    "    hidden_size = 512\n",
    "    num_layers = 16\n",
    "    batch_size = 32\n",
    "    num_micro_batches = 8\n",
    "    micro_batch_size = batch_size // num_micro_batches  # = 4\n",
    "    \n",
    "    stages = create_model_stages(hidden_size, num_layers, world_size)\n",
    "    stage = stages[rank].to(rank)\n",
    "\n",
    "    pipeline = GPipePipeline(\n",
    "        stage=stage,\n",
    "        rank=rank,\n",
    "        world_size=world_size,\n",
    "        micro_batches=num_micro_batches,\n",
    "        micro_batch_size=micro_batch_size,\n",
    "        input_shape=(hidden_size,),\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(stage.parameters(), lr=1e-3)\n",
    "\n",
    "    for i in range(10):\n",
    "        if rank == 0:\n",
    "            batch = torch.randn(batch_size, hidden_size, device=rank)\n",
    "        else:\n",
    "            batch = None\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pipeline.train_step(batch)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    cleanup_rank()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
