{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de99e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "258766ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12359'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b019d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_stages(hidden_size, num_layers, world_size):\n",
    "    \"\"\"Split model into pipeline stages.\"\"\"\n",
    "    layers_per_stage = num_layers // world_size\n",
    "    \n",
    "    all_layers = []\n",
    "    for i in range(num_layers):\n",
    "        all_layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    stages = []\n",
    "    for rank in range(world_size):\n",
    "        start = rank * layers_per_stage\n",
    "        end = start + layers_per_stage\n",
    "        stage_layers = all_layers[start:end]\n",
    "        stages.append(PipelineStage(stage_layers))\n",
    "    \n",
    "    return stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "057c8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineStage(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0be15d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPipePipeline:\n",
    "    \"\"\"\n",
    "    GPipe-style pipeline parallelism.\n",
    "    \n",
    "    Phases:\n",
    "    1. Fill: Forward all micro-batches (GPU 0 → GPU N)\n",
    "    2. Drain: Backward all micro-batches (GPU N → GPU 0)\n",
    "    \"\"\"\n",
    "    def __init__(self, stage, rank, world_size, micro_batches, micro_batch_size, input_size, dtype=torch.float32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            stage: The model stage (layers) for this rank\n",
    "            rank: This GPU's rank\n",
    "            world_size: Total number of pipeline stages\n",
    "            micro_batches: Number of micro-batches per training step\n",
    "            micro_batch_size: Size of each micro-batch\n",
    "            input_shape: Shape of input WITHOUT batch dimension\n",
    "            dtype: Data type for tensors\n",
    "        \"\"\"\n",
    "\n",
    "        self.stage = stage\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.micro_batches = micro_batches\n",
    "        self.micro_batch_size = micro_batch_size\n",
    "        self.input_size = input_size\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.prev_rank = rank - 1 if rank > 0 else None\n",
    "        self.next_rank = rank + 1 if rank < world_size - 1 else None\n",
    "\n",
    "    def send_forward(self, tensor):\n",
    "        if self.next_rank is not None:\n",
    "            dist.send(tensor.contiguous(), dst=self.next_rank)\n",
    "\n",
    "    def recv_forward(self):\n",
    "        if self.prev_ranl is not None:\n",
    "            shape = (self.micro_batch_size, *self.input_size)\n",
    "            tensor = torch.zeros(shape, dtype=self.dtype, device=f'cuda:{self.rank}')\n",
    "            dist.recv(tensor, src=self.next_rank)\n",
    "            return tensor\n",
    "\n",
    "        return None\n",
    "\n",
    "    def send_backward(self, tensor):\n",
    "        if self.prev_rank is not None:\n",
    "            dist.send(tensor.contiguous(), dst=self.prev_rank)\n",
    "\n",
    "    def recv_backward(self, output_shape):\n",
    "        if self.next_rank is not None:\n",
    "            tensor = torch.zeros(output_shape, dtype=self.dtype, device=f'cuda:{self.rank}')\n",
    "            dist.recv(tensor, src=self.next_rank)\n",
    "            return tensor\n",
    "\n",
    "        return None\n",
    "\n",
    "    def forward_step(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Forward through this stage.\n",
    "        \n",
    "        Returns:\n",
    "            output: Output activation\n",
    "            input_tensor: Saved input for backward\n",
    "        \"\"\"\n",
    "        input_tensor.requires_grad = True\n",
    "        output = self.stage(input_tensor)\n",
    "        return output, input_tensor\n",
    "\n",
    "    def backward_step(self, output, grad_output, input):\n",
    "        \"\"\"\n",
    "        Backward through this stage.\n",
    "        \n",
    "        Returns:\n",
    "            grad_input: Gradient w.r.t. input\n",
    "        \"\"\"\n",
    "        output.backward(grad_output)\n",
    "        return input.grad\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"\n",
    "        GPipe training step with two phases.\n",
    "        \n",
    "        Args:\n",
    "            batch: Input batch (only used by rank 0, can be None for other ranks)\n",
    "        \"\"\"\n",
    "        input_tensor = []\n",
    "        output_tensor = []\n",
    "\n",
    "        for i in range(self.micro_batches):\n",
    "            if self.rank == 0:\n",
    "                start_idx = i * self.micro_batch_size\n",
    "                end_idx = (i + 1) * self.micro_batch_size\n",
    "                micro_batch = batch[start_idx:end_idx].to(f'cuda:{self.rank}')\n",
    "            else:\n",
    "                input_tensor = self.recv_forward()\n",
    "\n",
    "            output, saved_input = self.forward_step(input_tensor)\n",
    "\n",
    "            input_tensor.append(saved_input)\n",
    "            output_tensor.append(output)\n",
    "\n",
    "            self.send_forward(output)\n",
    "\n",
    "        for i in range(self.micro_batches - 1, -1, -1):\n",
    "            output = output_tensor[i]\n",
    "            saved_input = input_tensor[i]\n",
    "\n",
    "            if self.rank == self.world_size - 1:\n",
    "                grad_output = torch.onee_like(output)\n",
    "            else:\n",
    "                grad_output = self.recv_backward(output.shape)\n",
    "\n",
    "            grad_input = self.backward_step(output, grad_output, saved_input)\n",
    "\n",
    "            if grad_input is not None:\n",
    "                self.send_backward(grad_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65e18528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpipe():\n",
    "    rank = os.environ['rank']\n",
    "    world_size = os.environ['world_size']\n",
    "\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    hidden_size = 512\n",
    "    num_layers = 16\n",
    "    batch_size = 32\n",
    "    num_micro_batches = 8\n",
    "    micro_batch_size = batch_size // num_micro_batches  # = 4\n",
    "    \n",
    "    stages = create_model_stages(hidden_size, num_layers, world_size)\n",
    "    stage = stages[rank].to(rank)\n",
    "\n",
    "    pipeline = GPipePipeline(\n",
    "        stage=stage,\n",
    "        rank=rank,\n",
    "        world_size=world_size,\n",
    "        micro_batches=num_micro_batches,\n",
    "        micro_batch_size=micro_batch_size,\n",
    "        input_shape=(hidden_size,),\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(stage.parameters(), lr=1e-3)\n",
    "\n",
    "    for i in range(10):\n",
    "        if rank == 0:\n",
    "            batch = torch.randn(batch_size, hidden_size, device=rank)\n",
    "        else:\n",
    "            batch = None\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pipeline.train_step(batch)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "394475c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneFOneBPipeline(nn.Module):\n",
    "    def __init__(self, stage, rank, world_size, micro_batches, micro_batch_size, input_shape, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stage = stage\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.micro_batches = micro_batches\n",
    "        self.micro_batch_size = micro_batch_size\n",
    "        self.input_shape = input_shape\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.prev_rank = rank - 1 if rank > 0 else None\n",
    "        self.next_rank = rank + 1 if rank < world_size - 1 else None\n",
    "\n",
    "        self.warmup_states = world_size - rank - 1\n",
    "        self.steady_states = micro_batches - self.warmup_states\n",
    "\n",
    "    def send_forward(self, tensor):\n",
    "        if self.next_rank is not None:\n",
    "            dist.send(tensor.contiguous(), dst=self.next_rank)\n",
    "\n",
    "    def recv_forward(self, output_shape):\n",
    "        if self.prev_rank is not None:\n",
    "            shape = (self.micro_batch_size, *self.input_shape)\n",
    "            tensor = torch.zeros(shape, dtype=self.dtype, device=f'cuda:{self.rank}')\n",
    "            dist.recv(tensor, src=self.prev_rank)\n",
    "            return tensor\n",
    "\n",
    "        return None\n",
    "\n",
    "    def send_backward(self, tensor):\n",
    "        if self.prev_rank is not None:\n",
    "            dist.send(tensor.contiguous(), dst=self.prev_rank)\n",
    "        \n",
    "    def recv_backward(self, output_shape):\n",
    "        if self.next_rank is not None:\n",
    "            tensor = torch.zeros(output_shape, dtype=self.dtype, device=f'cuda:{self.rank}')\n",
    "            dist.recv(tensor, src=self.next_rank)\n",
    "            return tensor\n",
    "\n",
    "        return None\n",
    "\n",
    "    def forward_step(self, micro_batch):\n",
    "        micro_batch.requires_grad = True\n",
    "        output = self.stage(micro_batch)\n",
    "        return output, micro_batch\n",
    "\n",
    "    def backward_step(self, output, grad_output, input_tensor):\n",
    "        output.backward(grad_output)\n",
    "        return input_tensor.grad\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"\n",
    "        1F1B training step with three phases.\n",
    "        \n",
    "        Args:\n",
    "            batch: Input batch (only used by rank 0, can be None for other ranks)\n",
    "        \"\"\"\n",
    "\n",
    "        input_tensors = []\n",
    "        output_tensors = []\n",
    "\n",
    "        for i in range(self.warmup_states):\n",
    "            if self.rank == 0:\n",
    "                start_idx = i * self.micro_batch_size\n",
    "                end_idx = (i + 1) * self.micro_batch_size\n",
    "                micro_batch = batch[start_idx:end_idx].to(f'cuda:{self.rank}')\n",
    "            else:\n",
    "                micro_batch = self.recv_forward()\n",
    "\n",
    "            output, input = self.forward_step(micro_batch)\n",
    "\n",
    "            input_tensors.append(input)\n",
    "            output_tensors.append(output)\n",
    "\n",
    "            self.send_forward(output)\n",
    "\n",
    "        for i in range(self.steady_states):\n",
    "            mb_idx = self.warmup_states * i\n",
    "\n",
    "            if self.rank == 0:\n",
    "                start_idx = mb_idx * self.micro_batch_size\n",
    "                end_idx = (mb_idx + 1) * self.micro_batch_size\n",
    "                micro_batch = batch[start_idx:end_idx].to(f'cuda:{self.rank}')\n",
    "            else:\n",
    "                micro_batch = self.recv_forward()\n",
    "\n",
    "            output, input = self.forward_step(micro_batch)\n",
    "\n",
    "            input_tensors.append(input)\n",
    "            output_tensors.append(output)\n",
    "\n",
    "            self.send_forward(output)\n",
    "\n",
    "            output = output_tensors.pop(0)\n",
    "            saved_input = input_tensors.pop(0)\n",
    "\n",
    "            if self.rank == self.world_size - 1:\n",
    "                grad_output = torch.ones_like(output, dtype=self.dtype, device=f'cuda:{self.rank}')\n",
    "            else:\n",
    "                grad_output = self.recv_backward(output.shape)\n",
    "\n",
    "            grad_input = self.backward_step(output, grad_output, saved_input)\n",
    "\n",
    "            if grad_input is not None:\n",
    "                self.send_backward(grad_input)\n",
    "\n",
    "        for _ in range(self.warmup_states):\n",
    "            output = output_tensors.pop(0)\n",
    "            saved_input = input_tensors.pop(0)\n",
    "            \n",
    "            if self.rank == self.world_size - 1:\n",
    "                grad_output = torch.ones_like(output)\n",
    "            else:\n",
    "                grad_output = self.recv_backward(output.shape)\n",
    "            \n",
    "            grad_input = self.backward_step(output, grad_output, saved_input)\n",
    "            \n",
    "            if grad_input is not None:\n",
    "                self.send_backward(grad_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
