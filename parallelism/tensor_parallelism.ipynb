{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff170315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29a3b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelism(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, world_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.out_features_per_rank = out_features // world_size\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(in_features, self.out_features_per_rank)\n",
    "        )\n",
    "\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.zeros(self.out_features_per_rank)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.matmul(x, self.weight) + self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1db47e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowParallelism(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.in_features_per_rank = in_features // world_size\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(self.in_features_per_rank, out_features)\n",
    "        )\n",
    "\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.zeros(self.out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.matmul(x, self.weight) + self.bias\n",
    "        dist.all_reduce(out, op=dist.ReduceOp.SUM)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5b6ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.n_heads_per_rank = n_heads // world_size\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.qkv_proj = ColumnParallelism(\n",
    "            in_features=d_model,\n",
    "            out_features=3 * d_model,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        self.out_proj = RowParallelism(\n",
    "            in_features=d_model,\n",
    "            out_features=d_model,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv_proj(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.n_heads_per_rank, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scores = q @ k.tranpose(-2, -1) / (self.head_dim ** 0.5)\n",
    "        attn_scores = nn.functional.softmax(scores, dim=-1)\n",
    "        attn_output = attn_scores @ v\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.n_heads_per_rank * self.head_dim) \n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b023e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelMLP(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.fc1 = ColumnParallelism(\n",
    "            in_features=d_model,\n",
    "            out_features=d_ff,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        self.fc2 = RowParallelism(\n",
    "            in_features=d_ff,\n",
    "            out_features=d_model,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6718f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelismBlock(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.attention_layer = TensorParallelAttention(\n",
    "            n_heads=n_heads,\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        self.mlp = TensorParalleismMLP(\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            world_size=world_size,\n",
    "            rank=rank\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = out + self.dropout1(self.attention_layer(self.norm1(out)))\n",
    "        out = out + self.dropout2(self.attention_layer(self.norm2(out)))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a49cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fdad5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelismFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_tensor, weight, bias, rank, world_size):\n",
    "        ctx.saved_for_backward = input_tensor, weight\n",
    "        ctx.rank = rank\n",
    "        ctx.world_size = world_size\n",
    "\n",
    "        out = torch.matmul(input_tensor, weight)\n",
    "        if bias is not None:\n",
    "            out += bias\n",
    "\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_tensor, weight = ctx.saved_tensors\n",
    "\n",
    "        grad_weight = torch.matmul(\n",
    "            input_tensor.reshape(-1, input_tensor.shape[-1]).t(),\n",
    "            grad_output.reshape(-1, grad_output.reshape[-1])\n",
    "        )\n",
    "\n",
    "        grad_bias = grad_output.sum(dim=list(range(len(grad_output.shape) - 1)))\n",
    "        grad_input = torch.matmul(grad_output, weight.t())\n",
    "\n",
    "        dist.all_reduce(grad_input, op=dist.ReduceOp.SUM)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "257508b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelism(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, world_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.out_features_per_rank = out_features // world_size\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(in_features, self.out_features_per_rank)\n",
    "        )\n",
    "\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.zeros(self.out_features_per_rank)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return ColumnParallelFunction.apply(x, self.weight, self.bias, self.rank, self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16c03829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowParallelismFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_tensor, weight, bias, rank, world_size):\n",
    "        ctx.save_for_backward = input_tensor, weight\n",
    "        ctx.rank = rank\n",
    "        ctx.world_size = world_size\n",
    "        ctx.use_bias = bias is not None\n",
    "\n",
    "        output = torch.matmul(input_tensor, weight) \n",
    "        if bias is not None:\n",
    "            output += bias\n",
    "\n",
    "        dist.all_reduce(output, op=dist.ReduceOp.SUM)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_tensor, weight = ctx.saved_tensors\n",
    "        use_bias = ctx.use_bias\n",
    "\n",
    "        grad_weight = torch.matmul(\n",
    "            input_tensor.reshape(-1, input_tensor.shape[-1]).t(),\n",
    "            grad_output.reshape(-1, grad_output.reshape[-1])\n",
    "        )\n",
    "\n",
    "        if use_bias:\n",
    "            grad_bias = grad_output.sum(dim=list(range(len(grad_output.shape) - 1)))\n",
    "            dist.all_reduce(grad_bias, op=dist.ReduceOp.SUM)\n",
    "        else:\n",
    "            grad_bias = None\n",
    "\n",
    "        grad_input = torch.matmul(grad_output, weight.t())\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f063e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowParallelism(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.in_features_per_rank = in_features // world_size\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(self.in_features_per_rank, out_features)\n",
    "        )\n",
    "\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.zeros(self.out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return RowParallelismFunction(x, self.weight, self.bias, self.rank, self.world_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
