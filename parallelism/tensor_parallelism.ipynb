{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0dc68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa107d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelism(nn.Module):\n",
    "    def __init__(self, input_size, output_size, world_size, rank):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.world_size = world_size\n",
    "        self.rank = rank\n",
    "\n",
    "        self.output_size_per_rank = output_size // world_size\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(input_size, self.output_size_per_rank)\n",
    "        )\n",
    "\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.zeros(self.output_size_per_rank)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.matmul(x, self.weight) + self.bias\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c4029b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowParallelism(nn.Module):\n",
    "    def __init__(self, input_size, output_size, world_size, rank):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.world_size = world_size\n",
    "        self.rank = rank\n",
    "\n",
    "        self.input_size_per_rank = input_size // world_size\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(self.input_size_per_rank, output_size)\n",
    "        )\n",
    "        self.bias = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_partial = torch.matmul(x, self.weight)\n",
    "\n",
    "        dist.all_reduce(output_partial, op=dist.ReduceOp.SUM)\n",
    "\n",
    "        if self.rank == 0:\n",
    "            output_partial += self.bias\n",
    "\n",
    "        return output_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46d57421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelismAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, world_size, rank):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.world_size = world_size\n",
    "        self.rank = rank\n",
    "\n",
    "        self.num_heads_per_rank = n_heads // world_size\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.qkv_proj = ColumnParallelism(\n",
    "            d_model,\n",
    "            d_model * 3,\n",
    "            world_size,\n",
    "            rank\n",
    "        )\n",
    "\n",
    "        self.out_prof = RowParallelism(\n",
    "            d_model,\n",
    "            d_model,\n",
    "            world_size,\n",
    "            rank\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads_per_partition, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 1, 3, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scores = q @ k.transpose(-2, -1) / (self.head_dim ** 0.5)\n",
    "        attn_weights = nn.functional.softmax(scores, dim=-1)\n",
    "        attn_output = attn_weights @ v\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff1e34ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParalleismMLP(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, world_size, rank):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.world_size = world_size\n",
    "        self.rank = rank\n",
    "\n",
    "        self.fc1 = ColumnParallelism(\n",
    "            d_model, \n",
    "            d_ff, \n",
    "            world_size,\n",
    "            rank\n",
    "        )\n",
    "\n",
    "        self.fc2 = RowParallelism(\n",
    "            d_ff,\n",
    "            d_model,\n",
    "            world_size,\n",
    "            rank\n",
    "        )\n",
    "\n",
    "        self.activation = nn.GeLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x \n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740a7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelismTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, world_size, rank):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attention = TensorParallelismAttention(d_model, n_heads, world_size, rank)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = TensorParallelismMLP(d_model, d_ff, world_size, rank)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = out + self.attention(self.norm1(out))\n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29934b63",
   "metadata": {},
   "source": [
    "#### Backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd3deaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecb41f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias, world_size, rank):\n",
    "        ctx.save_for_backward(input, weight)\n",
    "        ctx.world_size = world_size\n",
    "        ctx.rank = rank\n",
    "\n",
    "        output = torch.matmul(input, weight)\n",
    "        if bias is not None:\n",
    "            output += bias\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight = ctx.saved_tensor\n",
    "\n",
    "        grad_weight = torch.matmul(\n",
    "            input.reshape(-1, input.shape[-1]).t(), \n",
    "            grad_output.reshape(-1, grad_output.shape[-1])\n",
    "        )\n",
    "\n",
    "        grad_bias = grad_output.sum(dim=list(range(len(grad_output.shape) - 1)))\n",
    "\n",
    "        grad_input = torch.matmul(grad_output, weight.t())\n",
    "\n",
    "        dist.all_reduce(grad_input, op=dist.ReduceOp.SUM)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a39114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelim(nn.Module):\n",
    "    def __init__(self, input_size, output_size, world_size, rank):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.world_size = world_size\n",
    "        self.rank = rank\n",
    "\n",
    "        self.output_size_per_rank = output_size // world_size\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(input_size, self.output_size_per_rank)\n",
    "        )\n",
    "\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.zeros(self.output_size_per_rank)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return ColumnParallelFunction.apply(\n",
    "            x self.weight, self.bias, self.world_size, self.rank\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a1d1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowParallelFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias, world_size, rank):\n",
    "        ctx.save_for_backward(input, weight)\n",
    "        ctx.world_size = world_size\n",
    "        ctx.rank = rank\n",
    "        ctx.use_bias = bias is not None\n",
    "        \n",
    "        output = torch.matmul(input, weight)\n",
    "        \n",
    "        dist.all_reduce(output, op=dist.ReduceOp.SUM)\n",
    "        \n",
    "        if bias is not None and rank == 0:\n",
    "            output = output + bias\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight = ctx.saved_tensors\n",
    "        world_size = ctx.world_size\n",
    "        rank = ctx.rank\n",
    "        use_bias = ctx.use_bias\n",
    "        \n",
    "        grad_weight = torch.matmul(\n",
    "            input.reshape(-1, input.shape[-1]).t(),\n",
    "            grad_output.reshape(-1, grad_output.shape[-1])\n",
    "        )\n",
    "        \n",
    "        grad_bias = None\n",
    "        if use_bias:\n",
    "            grad_bias = grad_output.sum(dim=list(range(len(grad_output.shape) - 1)))\n",
    "            dist.all_reduce(grad_bias, op=dist.ReduceOp.SUM)\n",
    "\n",
    "        grad_input = torch.matmul(grad_output, weight.t())\n",
    "        \n",
    "        return grad_input, grad_weight, grad_bias, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b80b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowParallelism(nn.Module):\n",
    "    def __init__(self, input_size, output_size, world_size, rank):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.world_size = world_size\n",
    "        self.rank = rank\n",
    "\n",
    "        self.input_size_per_rank = input_size // world_size\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(self.input_size_per_rank, output_size)\n",
    "        )\n",
    "        self.bias = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return RowParallelFunction.apply(\n",
    "            x, self.weight, self.bias, self.world_size, self.rank\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
