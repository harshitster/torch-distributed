{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3999468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c0eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0131dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKRouter(nn.Module):\n",
    "    def __init__(self, d_model, n_experts, top_k=2, capacity_factor=1.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_experts = n_experts\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factor = capacity_factor\n",
    "\n",
    "        self.router = nn.Linear(d_model, n_experts, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, hidden_size = x.shape\n",
    "        num_tokens = batch_size * seq_len\n",
    "\n",
    "        x = x.reshape(num_tokens, hidden_size)\n",
    "        router_logits = self.router(x)\n",
    "\n",
    "        router_probs = torch.softmax(router_logits, dim=-1)\n",
    "\n",
    "        expert_weights, expert_indices = torch.topk(router_probs, self.top_k, dim=-1)\n",
    "\n",
    "        expert_weights = expert_weights / expert_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        load_balancing_loss = self._compute_load_balancing_loss(router_probs)\n",
    "\n",
    "        return expert_indices, expert_weights, load_balancing_loss\n",
    "\n",
    "    def _compute_load_balancing_loss(self, router_probs):\n",
    "        num_tokens = router_probs.shape[0]\n",
    "\n",
    "        expert_assignment = torch.argmax(router_probs, dim=-1)\n",
    "        expert_count = torch.bincount(\n",
    "            expert_assignment\n",
    "        ).float()\n",
    "\n",
    "        f_i = expert_count / num_tokens\n",
    "\n",
    "        p_i = router_probs.mean(dim=0)\n",
    "\n",
    "        loss = self.n_experts * (f_i * p_i).sum()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab5021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedMoELayer(nn.Module):\n",
    "    def __init__(self, d_model, n_experts, d_ff, world_size, rank, top_k=2, capacity_factor=1.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_experts = n_experts\n",
    "        self.d_ff = d_ff\n",
    "        self.world_size = world_size\n",
    "        self.rank = rank\n",
    "\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factor = capacity_factor\n",
    "\n",
    "        self.n_local_experts = n_experts // world_size\n",
    "\n",
    "        self.router = TopKRouter(d_model, n_experts, top_k, capacity_factor)\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(d_model, d_ff)\n",
    "            for _ in range(self.n_local_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        num_tokens = batch_size * seq_len\n",
    "\n",
    "        x = x.reshape(batch_size, seq_len)\n",
    "\n",
    "        expert_weights, expert_indices, load_balancing_loss = self.router(x)\n",
    "\n",
    "        capacity = int((num_tokens / self.n_experts) * self.capacity_factor)\n",
    "\n",
    "        dispatch_data, combine_weights = self._prepare_all_to_all(\n",
    "            x, expert_indices, expert_weights, capacity\n",
    "        )\n",
    "\n",
    "        received_data = self._all_to_all_scatter(dispatch_data)\n",
    "\n",
    "        expert_output = self._process_local_experts(received_data)\n",
    "\n",
    "        gathered_output = self._all_to_all_gather(expert_output)\n",
    "\n",
    "        output = self._combine_outputs(gathered_output, combine_weights)\n",
    "\n",
    "        output = output.reshape(batch_size, seq_len, d_model)\n",
    "\n",
    "        return output, load_balancing_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70013ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoETransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete transformer block with MoE instead of dense FFN.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, num_experts, intermediate_size,\n",
    "                 top_k=2, capacity_factor=1.25):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Layer norms\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Attention (standard multi-head attention)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            hidden_size, \n",
    "            num_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # MoE layer (replaces dense FFN)\n",
    "        self.moe = DistributedMoELayer(\n",
    "            hidden_size, \n",
    "            num_experts, \n",
    "            intermediate_size,\n",
    "            top_k, \n",
    "            capacity_factor\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq, hidden)\n",
    "            attn_mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq, hidden)\n",
    "            aux_loss: Load balancing loss\n",
    "        \"\"\"\n",
    "        # Attention block\n",
    "        normed = self.ln1(x)\n",
    "        attn_output, _ = self.attention(normed, normed, normed, attn_mask=attn_mask)\n",
    "        x = x + attn_output\n",
    "        \n",
    "        # MoE block\n",
    "        normed = self.ln2(x)\n",
    "        moe_output, aux_loss = self.moe(normed)\n",
    "        x = x + moe_output\n",
    "        \n",
    "        return x, aux_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
