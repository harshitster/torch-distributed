{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b69a777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, Dict, Tuple, List, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7be18f1",
   "metadata": {},
   "source": [
    "#### Question 1: Efficient Attention Mask Creation\n",
    "You're implementing a decoder-only transformer and need to create causal attention masks efficiently.\n",
    "\n",
    "Implement a function that creates a causal attention mask for a batch of sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1395a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_length: int, batch_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal attention mask for decoder-only transformers.\n",
    "    \n",
    "    Args:\n",
    "        seq_length: Length of the sequence\n",
    "        batch_size: Number of sequences in the batch\n",
    "    \n",
    "    Returns:\n",
    "        A tensor of shape (batch_size, seq_length, seq_length) where:\n",
    "        - mask[i, j, k] = 0 if position j can attend to position k\n",
    "        - mask[i, j, k] = -inf if position j cannot attend to position k (k > j)\n",
    "    \n",
    "    Example:\n",
    "        For seq_length=3, one slice should look like:\n",
    "        [[0, -inf, -inf],\n",
    "         [0,    0, -inf],\n",
    "         [0,    0,    0]]\n",
    "    \"\"\"\n",
    "    mask = torch.tril(\n",
    "        torch.ones(seq_length, seq_length), diagonal=1\n",
    "    )\n",
    "    mask = mask.masked_fill(mask == 1, value=float('-inf'))\n",
    "\n",
    "    # do not expand. PyTorch implicitly broadcasts during attention computation\n",
    "    mask = mask.unsqueeze(0)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d75d78",
   "metadata": {},
   "source": [
    "#### Question 2: KV Cache Management\n",
    "You're implementing an inference engine for a decoder-only transformer. To avoid recomputing key and value tensors for previous tokens, you need to implement a KV cache.\n",
    "\n",
    "Implement a function that updates the KV cache during autoregressive generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cda5522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_kv_cache(\n",
    "    cache: Optional[Dict[int, Dict[str, torch.Tensor]]],\n",
    "    new_keys: torch.Tensor,\n",
    "    new_values: torch.Tensor,\n",
    "    layer_idx: int\n",
    ") -> Dict[int, Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Update KV cache with new key and value tensors for a specific layer.\n",
    "    \n",
    "    Args:\n",
    "        cache: Existing cache or None. Structure:\n",
    "               {\n",
    "                   0: {'keys': tensor, 'values': tensor},\n",
    "                   1: {'keys': tensor, 'values': tensor},\n",
    "                   ...\n",
    "               }\n",
    "        new_keys: shape (batch_size, 1, num_heads, head_dim)\n",
    "        new_values: shape (batch_size, 1, num_heads, head_dim)\n",
    "        layer_idx: Which transformer layer (0-indexed)\n",
    "    \n",
    "    Returns:\n",
    "        Updated cache dictionary\n",
    "    \"\"\"\n",
    "    if cache is None:\n",
    "        cache = {}\n",
    "    \n",
    "    if layer_idx not in cache:\n",
    "        cache[layer_idx] = {'keys': new_keys, 'values': new_values}\n",
    "    else:\n",
    "        cache[layer_idx]['keys'] = torch.cat(\n",
    "            [cache[layer_idx]['keys'], new_keys], dim=1\n",
    "        )\n",
    "        cache[layer_idx]['values'] = torch.cat(\n",
    "            [cache[layer_idx]['values'], new_values], dim=1\n",
    "        )\n",
    "    \n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a337a",
   "metadata": {},
   "source": [
    "#### Question 3: Weight Quantization\n",
    "You're building an inference engine and need to implement INT8 quantization to reduce model size and speed up inference.\n",
    "\n",
    "Implement a function that quantizes FP32 weights to INT8 using symmetric quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8d61e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_weights_int8(\n",
    "    weights: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Quantize FP32 weights to INT8 using symmetric quantization.\n",
    "    \n",
    "    Symmetric quantization maps the range [-max_abs, max_abs] to [-127, 127]\n",
    "    \n",
    "    Args:\n",
    "        weights: FP32 tensor of any shape\n",
    "    \n",
    "    Returns:\n",
    "        quantized_weights: INT8 tensor (same shape as input)\n",
    "        scale: FP32 scalar tensor used for dequantization\n",
    "        \n",
    "    The relationship is: weights ≈ quantized_weights * scale\n",
    "    \n",
    "    Example:\n",
    "        weights = torch.tensor([[-2.0, 1.5], [3.0, -1.0]])\n",
    "        quantized, scale = quantize_weights_int8(weights)\n",
    "        # scale = 3.0 / 127 ≈ 0.0236\n",
    "        # quantized ≈ [[-85, 64], [127, -42]]\n",
    "        \n",
    "        # Dequantize: quantized * scale ≈ original weights\n",
    "    \"\"\"\n",
    "    max_val = weights.abs().max()\n",
    "\n",
    "    scale = max_val / 127\n",
    "    if scale == 0.0:\n",
    "        scale = 1.0\n",
    "\n",
    "    quantized_weights = torch.clamp(\n",
    "        torch.round(weights / scale),\n",
    "        -127, 127\n",
    "    ).to(torch.int8)\n",
    "\n",
    "    return quantized_weights, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d12575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_weights_int8_per_channel(\n",
    "    weights: torch.Tensor,  # Shape: (out_features, in_features)\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Per-channel quantization: each row gets its own scale.\n",
    "    \n",
    "    Returns:\n",
    "        quantized_weights: INT8 tensor, shape (out_features, in_features)\n",
    "        scales: FP32 tensor, shape (out_features,) - one scale per output channel\n",
    "    \"\"\"\n",
    "    quantized_channels = []\n",
    "    channel_scales = []\n",
    "\n",
    "    q_max = 127\n",
    "\n",
    "    for idx in range(weights.shape[0]):\n",
    "        channel_data = weights[idx,:]\n",
    "\n",
    "        max_val = channel_data.abs().max()\n",
    "        scale = max_val / q_max\n",
    "\n",
    "        if scale == 0.0:\n",
    "            scale = 1.0\n",
    "\n",
    "        quantized_channel_data = torch.clamp(\n",
    "            torch.round(channel_data / scale), \n",
    "            - q_max, q_max\n",
    "        ).to(torch.int8)\n",
    "\n",
    "        quantized_channels.append(quantized_channel_data)\n",
    "        channel_scales.append(scale)\n",
    "\n",
    "    quantized_channels = torch.stack(quantized_channels, dim=0)\n",
    "    channel_scales = torch.stack(channel_scales, dim=0)\n",
    "\n",
    "    return quantized_channels, channel_scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae264c",
   "metadata": {},
   "source": [
    "#### Question 4: Batched Attention with Variable Sequence Lengths\n",
    "You're implementing batched inference where different sequences in the batch have different lengths. You need to compute attention while properly masking padded positions.\n",
    "\n",
    "Implement a function that computes scaled dot-product attention for a batch with variable-length sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_attention_with_padding(\n",
    "    query: torch.Tensor,      # (batch_size, seq_len, d_model)\n",
    "    key: torch.Tensor,        # (batch_size, seq_len, d_model)\n",
    "    value: torch.Tensor,      # (batch_size, seq_len, d_model)\n",
    "    seq_lengths: List[int],   # Actual length of each sequence (without padding)\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention with padding mask.\n",
    "    \n",
    "    Args:\n",
    "        query: Query tensor (batch_size, seq_len, d_model)\n",
    "        key: Key tensor (batch_size, seq_len, d_model)\n",
    "        value: Value tensor (batch_size, seq_len, d_model)\n",
    "        seq_lengths: List of actual sequence lengths for each batch element\n",
    "                     e.g., [5, 3, 7] means batch has 3 sequences with lengths 5, 3, 7\n",
    "                     \n",
    "    Returns:\n",
    "        output: Attention output (batch_size, seq_len, d_model)\n",
    "        \n",
    "    Note: \n",
    "    - Padded positions should not contribute to attention (use -inf in attention scores)\n",
    "    - Use scaled dot-product: softmax(Q @ K^T / sqrt(d_model)) @ V\n",
    "    \n",
    "    Example:\n",
    "        batch_size=2, seq_len=4, d_model=8\n",
    "        seq_lengths = [3, 2]  # First sequence has 3 valid tokens, second has 2\n",
    "        \n",
    "        Attention mask should be:\n",
    "        [[1, 1, 1, 0],   # First sequence: positions 0,1,2 are valid\n",
    "         [1, 1, 0, 0]]   # Second sequence: positions 0,1 are valid\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9a968",
   "metadata": {},
   "source": [
    "#### Question 5: Tensor Parallelism - Column-wise Weight Split\n",
    "You're implementing tensor parallelism to distribute a large linear layer across multiple GPUs. In tensor parallelism, we split weight matrices across devices.\n",
    "\n",
    "Implement a function that performs a column-parallel linear layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3056b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_parallel_linear(\n",
    "    input: torch.Tensor,          # (batch_size, seq_len, hidden_size)\n",
    "    weight: torch.Tensor,         # (output_size, hidden_size) - FULL weight on this rank\n",
    "    rank: int,                    # Current GPU rank (0, 1, 2, ...)\n",
    "    world_size: int,              # Total number of GPUs\n",
    "    bias: Optional[torch.Tensor] = None,  # (output_size,) - FULL bias\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Perform column-parallel linear layer where weight matrix is split by columns.\n",
    "    \n",
    "    In column parallelism:\n",
    "    - Input is the SAME on all GPUs (replicated)\n",
    "    - Weight is SPLIT by columns across GPUs\n",
    "    - Each GPU computes a partial output\n",
    "    - Outputs are CONCATENATED (via all-gather)\n",
    "    \n",
    "    Example with world_size=2, output_size=8:\n",
    "        GPU 0 gets weight[:, 0:4]  -> computes output[:, :, 0:4]\n",
    "        GPU 1 gets weight[:, 4:8]  -> computes output[:, :, 4:8]\n",
    "        \n",
    "        Then all-gather to get full output of size (batch, seq_len, 8)\n",
    "    \n",
    "    Args:\n",
    "        input: Input tensor (same on all GPUs)\n",
    "        weight: FULL weight matrix on this rank - you need to slice it\n",
    "        rank: Which GPU this is (0-indexed)\n",
    "        world_size: Total number of GPUs\n",
    "        bias: Optional full bias vector\n",
    "        \n",
    "    Returns:\n",
    "        output: Full output tensor (batch_size, seq_len, output_size)\n",
    "        \n",
    "    Note: For this exercise, assume you have access to:\n",
    "        - torch.distributed.all_gather() for gathering tensors\n",
    "        - Ignore actual distributed setup, focus on the sharding logic\n",
    "    \"\"\"\n",
    "    output_features = weight.shape[0]\n",
    "    output_features_per_rank = output_features // world_size\n",
    "\n",
    "    rank_weight = weight[rank * output_features_per_rank: (rank + 1) * output_features_per_rank, :] \n",
    "    rank_bias = None\n",
    "    if bias is not None:\n",
    "        rank_bias = bias[rank * output_features_per_rank: (rank + 1) * output_features_per_rank]\n",
    "\n",
    "    rank_output = input @ rank_weight.T\n",
    "    if rank_bias is not None:\n",
    "        rank_output += rank_bias\n",
    "\n",
    "    tensor_list = [torch.zeros_like(rank_output) for _ in range(world_size)]\n",
    "    torch.distributed.all_gather(tensor_list, rank_output)\n",
    "\n",
    "    output = torch.cat(tensor_list, dim=-1)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cfb365",
   "metadata": {},
   "source": [
    "#### Question 7: Token Sampling Strategies\n",
    "You're building an inference engine and need to implement different sampling strategies for text generation.\n",
    "\n",
    "Implement three sampling functions: greedy, top-k, and top-p (nucleus) sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39278e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_sample(logits: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Greedy sampling - select token with highest probability.\n",
    "    \n",
    "    Args:\n",
    "        logits: Logits tensor of shape (batch_size, vocab_size)\n",
    "        \n",
    "    Returns:\n",
    "        next_tokens: Token IDs of shape (batch_size,)\n",
    "    \"\"\"\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    pred = torch.argmax(probs, dim=-1)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def top_k_sample(\n",
    "    logits: torch.Tensor,\n",
    "    k: int,\n",
    "    temperature: float = 1.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Top-K sampling - sample from the k most likely tokens.\n",
    "    \n",
    "    Args:\n",
    "        logits: Logits tensor of shape (batch_size, vocab_size)\n",
    "        k: Number of top tokens to consider\n",
    "        temperature: Softmax temperature (higher = more random)\n",
    "        \n",
    "    Returns:\n",
    "        next_tokens: Token IDs of shape (batch_size,)\n",
    "        \n",
    "    Steps:\n",
    "    1. Apply temperature scaling: logits / temperature\n",
    "    2. Find top-k logits and their indices\n",
    "    3. Set all other logits to -inf\n",
    "    4. Apply softmax to get probabilities\n",
    "    5. Sample from the distribution\n",
    "    \"\"\"\n",
    "    logits = logits / temperature\n",
    "\n",
    "    topk_values, topk_indices = torch.topk(logits, k=k, dim=-1)\n",
    "\n",
    "    filtered_logits = torch.full_like(logits, fill_value=float('-inf'))\n",
    "    filtered_logits.scatter_(dim=-1, index=topk_indices, src=topk_values)\n",
    "\n",
    "    probs = torch.softmax(filtered_logits, dim=-1)\n",
    "    pred = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    return pred.unsqueeze(-1)\n",
    "\n",
    "\n",
    "def top_p_sample(\n",
    "    logits: torch.Tensor,\n",
    "    p: float,\n",
    "    temperature: float = 1.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Top-P (nucleus) sampling - sample from smallest set of tokens whose\n",
    "    cumulative probability exceeds p.\n",
    "    \n",
    "    Args:\n",
    "        logits: Logits tensor of shape (batch_size, vocab_size)\n",
    "        p: Cumulative probability threshold (e.g., 0.9)\n",
    "        temperature: Softmax temperature\n",
    "        \n",
    "    Returns:\n",
    "        next_tokens: Token IDs of shape (batch_size,)\n",
    "        \n",
    "    Steps:\n",
    "    1. Apply temperature scaling\n",
    "    2. Convert to probabilities with softmax\n",
    "    3. Sort probabilities in descending order\n",
    "    4. Compute cumulative probabilities\n",
    "    5. Find cutoff where cumsum > p\n",
    "    6. Mask out tokens beyond cutoff\n",
    "    7. Renormalize and sample\n",
    "    \"\"\"\n",
    "    logits = logits / temperature\n",
    "\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "    sorted_probs = torch.softmax(sorted_logits, dim=-1)\n",
    "\n",
    "    probs_cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "    mask = probs_cumsum > p\n",
    "    mask[...,0] = False\n",
    "    sorted_logits[mask] = float('-inf')\n",
    "\n",
    "    filtered_logits = torch.full_like(logits, fill_value=float('-inf'))\n",
    "    filtered_logits.scatter_(dim=-1, index=sorted_indices, src=sorted_logits)\n",
    "\n",
    "    probs = torch.softmax(filtered_logits, dim=-1)\n",
    "    token_ids = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    return token_ids.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5897292d",
   "metadata": {},
   "source": [
    "#### Question 8: Gradient Accumulation\n",
    "You're training a large model but GPU memory is limited. You need to implement gradient accumulation to simulate a larger batch size.\n",
    "\n",
    "Implement a training step with gradient accumulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3ec7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_with_grad_accumulation(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    data_loader: DataLoader,\n",
    "    accumulation_steps: int,\n",
    "    device: str = 'cuda'\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Perform one training epoch with gradient accumulation.\n",
    "    \n",
    "    Gradient accumulation allows training with effective batch size of:\n",
    "        actual_batch_size * accumulation_steps\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        optimizer: Optimizer (e.g., Adam)\n",
    "        data_loader: DataLoader providing batches\n",
    "        accumulation_steps: Number of batches to accumulate before optimizer step\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        average_loss: Average loss over the epoch\n",
    "        \n",
    "    Key points:\n",
    "    - Gradients accumulate across multiple forward/backward passes\n",
    "    - Optimizer step happens every accumulation_steps batches\n",
    "    - Need to scale loss appropriately\n",
    "    - Zero gradients at the right time\n",
    "    \n",
    "    Example:\n",
    "        If batch_size=8 and accumulation_steps=4:\n",
    "        - Effective batch size = 32\n",
    "        - Do 4 forward passes with batch_size=8\n",
    "        - Accumulate gradients from all 4\n",
    "        - Then do optimizer.step()\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for idx, (batch, y) in enumerate(DataLoader):\n",
    "        batch = batch.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(batch).to(device)\n",
    "        loss = criterion(output, y) / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    if (idx + 1) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f750f88",
   "metadata": {},
   "source": [
    "#### Question 9: Mixed Precision Training (FP16/BF16)\n",
    "You're implementing mixed precision training to reduce memory usage and speed up training. This involves using FP16/BF16 for forward/backward passes while keeping FP32 master weights.\n",
    "\n",
    "Implement a training step with mixed precision and gradient scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fb804f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_mixed_precision(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    inputs: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    scaler: torch.cuda.amp.GradScaler,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Perform one training step with automatic mixed precision (AMP).\n",
    "    \n",
    "    Mixed precision training:\n",
    "    - Forward pass in FP16/BF16 (faster, less memory)\n",
    "    - Loss in FP16/BF16\n",
    "    - Gradients scaled up to prevent underflow\n",
    "    - Optimizer step with FP32 master weights\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        optimizer: Optimizer\n",
    "        inputs: Input tensor (batch_size, ...)\n",
    "        labels: Label tensor (batch_size,)\n",
    "        scaler: GradScaler for scaling gradients\n",
    "        use_amp: Whether to use automatic mixed precision\n",
    "        \n",
    "    Returns:\n",
    "        loss_value: Scalar loss value\n",
    "        \n",
    "    Key concepts:\n",
    "    - torch.cuda.amp.autocast() for automatic dtype casting\n",
    "    - GradScaler to prevent gradient underflow in FP16\n",
    "    - Gradient scaling: scale up before backward, unscale before optimizer step\n",
    "    \n",
    "    Why gradient scaling?\n",
    "    - FP16 range: ~6e-5 to 65504\n",
    "    - Small gradients (e.g., 1e-7) underflow to 0 in FP16\n",
    "    - Solution: Scale gradients by large factor (e.g., 2^16) during backward\n",
    "    - Unscale before optimizer step\n",
    "    \"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.amp.autocast(device_type=torch.float16):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "    scaler.scale(loss)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424ff49",
   "metadata": {},
   "source": [
    "#### Question 10: Continuous Batching for Inference\n",
    "You're building an inference server that needs to handle multiple requests efficiently. Traditional batching waits for a fixed batch to fill up, but continuous batching dynamically adds/removes sequences as they complete.\n",
    "\n",
    "Implement a simplified continuous batching scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf50f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousBatchScheduler:\n",
    "    \"\"\"\n",
    "    Continuous batching scheduler for LLM inference.\n",
    "    \n",
    "    Key idea:\n",
    "    - Don't wait for all sequences to finish\n",
    "    - As soon as one sequence completes, add a new request\n",
    "    - Maximizes GPU utilization\n",
    "    \n",
    "    Example timeline:\n",
    "        Time 0: [Req1, Req2, Req3, Req4] (batch=4, all start)\n",
    "        Time 5: Req2 finishes (generated EOS token)\n",
    "        Time 5: [Req1, Req5, Req3, Req4] (Req5 added immediately)\n",
    "        Time 8: Req3 finishes\n",
    "        Time 8: [Req1, Req5, Req6, Req4] (Req6 added immediately)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_batch_size: int, max_seq_length: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_batch_size: Maximum number of sequences in a batch\n",
    "            max_seq_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.active_requests = {}  # request_id -> request_data\n",
    "        self.waiting_queue = []    # Queue of pending requests\n",
    "    \n",
    "    def add_request(self, request_id: str, prompt_tokens: List[int]):\n",
    "        \"\"\"\n",
    "        Add a new request to the queue.\n",
    "        \n",
    "        Args:\n",
    "            request_id: Unique identifier for this request\n",
    "            prompt_tokens: Input token IDs\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        request_data = {\n",
    "            'tokens': prompt_tokens.copy(),\n",
    "            'length': len(prompt_tokens)\n",
    "        }\n",
    "\n",
    "        if len(self.active_requests) >= self.max_batch_size:\n",
    "            self.waiting_queue.append(request_data)\n",
    "        else:\n",
    "            self.active_requests[request_id] = request_data\n",
    "    \n",
    "    def get_batch(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get the current batch for inference.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "            - 'request_ids': List of active request IDs\n",
    "            - 'input_ids': Tensor of shape (batch_size, seq_len) - padded\n",
    "            - 'attention_mask': Tensor of shape (batch_size, seq_len)\n",
    "            - 'sequence_lengths': List of current lengths for each sequence\n",
    "            \n",
    "        Note: Sequences will have different lengths, so padding is needed.\n",
    "        \"\"\"\n",
    "        if not self.active_requests:\n",
    "            return {\n",
    "                'request_ids': [],\n",
    "                'input_ids': torch.tensor([]),\n",
    "                'attention_mask': torch.tensor([]),\n",
    "                'sequence_lengths': []\n",
    "            }\n",
    "        \n",
    "        request_ids = list(self.active_requests.keys())\n",
    "        all_tokens = [self.active_requests[rid]['tokens'] for rid in request_ids]\n",
    "        sequence_lengths = [self.active_requests[rid]['length'] for rid in request_ids]\n",
    "\n",
    "        max_length = max(sequence_lengths)\n",
    "        batch_size = len(request_ids)\n",
    "\n",
    "        input_ids = torch.zeros((batch_size, max_length))\n",
    "        attention_mask = torch.zeros(batch_size, max_length, dtype=torch.long)\n",
    "        \n",
    "        for i, tokens in enumerate(all_tokens):\n",
    "            seq_len = len(tokens)\n",
    "            input_ids[i, :seq_len] = torch.tensor(tokens)\n",
    "            attention_mask[i, :seq_len] = 1\n",
    "        \n",
    "        return {\n",
    "            'request_ids': request_ids,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'sequence_lengths': sequence_lengths\n",
    "        }\n",
    "    \n",
    "    def update_batch(self, request_id: str, new_token: int, is_finished: bool):\n",
    "        \"\"\"\n",
    "        Update a request with newly generated token.\n",
    "        \n",
    "        Args:\n",
    "            request_id: Which request to update\n",
    "            new_token: Newly generated token ID\n",
    "            is_finished: Whether this sequence is complete (EOS or max_length)\n",
    "            \n",
    "        If is_finished=True, remove from active_requests and try to add\n",
    "        a new request from waiting_queue to maintain batch size.\n",
    "        \"\"\"\n",
    "        if not is_finished:\n",
    "            self.active_requests[request_id]['tokens'].append(new_token)\n",
    "            self.active_requests[request_id]['length'] += 1\n",
    "\n",
    "            if self.active_requests[request_id]['length'] > self.max_seq_length:\n",
    "                self.update_batch(request_id, None, is_finished)\n",
    "        else:\n",
    "            del self.active_requests[request_id]\n",
    "            \n",
    "            if self.waiting_queue and len(self.active_requests) < self.max_batch_size:\n",
    "                new_req_id, prompt_tokens = self.waiting_queue.pop(0)\n",
    "                self.active_requests[new_req_id] = {\n",
    "                    'tokens': prompt_tokens.copy(),\n",
    "                    'length': len(prompt_tokens)\n",
    "                }\n",
    "    \n",
    "    def can_add_more_requests(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if we can add more requests to the current batch.\n",
    "        \n",
    "        Returns:\n",
    "            True if batch is not full and waiting queue has requests\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        return len(self.active_requests) < self.max_batch_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
