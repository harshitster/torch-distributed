{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce38d84",
   "metadata": {},
   "source": [
    "### KV Cache - Version 1 (Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5995427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0a9878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWithCache(nn.Module):\n",
    "    def __init__(self, n_heads, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, kv_cache=None, use_cache=False):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv_proj(x).view(batch_size, seq_len, 3, self.n_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        past_len = 0\n",
    "        if kv_cache:\n",
    "            k_prev, v_prev = kv_cache\n",
    "            past_len = k_prev.shape[2]\n",
    "            k = torch.cat([k_prev, k], dim=2)\n",
    "            v = torch.cat([v_prev, v], dim=2)\n",
    "\n",
    "        attn = q @ k.transpose(-2, -1) / (self.head_dim ** 0.5)\n",
    "\n",
    "        if seq_len > 1 or past_len > 0:\n",
    "            past_mask = torch.ones(seq_len, past_len)\n",
    "            current_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "\n",
    "            mask = torch.cat([past_mask, current_mask], dim=1)\n",
    "            mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "            mask = mask == 0\n",
    "\n",
    "            attn = torch.masked_fill(attn, mask, value=float('-inf'))\n",
    "\n",
    "        attn = nn.functional.softmax(attn, dim=-1)\n",
    "        scores = attn @ v\n",
    "\n",
    "        scores = scores.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_proj(scores)\n",
    "\n",
    "        if use_cache:\n",
    "            return output, (k, v)\n",
    "        else:\n",
    "            return output, None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c85c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.attention_layer = MultiHeadAttentionWithCache(n_heads, d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, kv_cache=None, use_cache=False):\n",
    "        residual = x\n",
    "        out = self.norm1(x)\n",
    "        out, new_kv_cache = self.attention_layer(out, kv_cache, use_cache)\n",
    "        out = residual + self.dropout1(out)\n",
    "\n",
    "        residual = out\n",
    "        out = self.norm2(out)\n",
    "        out = self.mlp(out)\n",
    "        out = residual + self.dropout2(out)\n",
    "\n",
    "        return out, new_kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc116e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_heads, d_model, n_layers, d_ff, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.max_len = max_len\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.positional_encoding = nn.Embedding(max_len, d_model)\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(n_heads, d_model, d_ff)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, past_kv_caches=None, use_cache=False):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        if past_kv_caches is not None and past_kv_caches[0] is not None:\n",
    "            past_len = past_kv_caches[0][0].shape[2]\n",
    "        else:\n",
    "            past_len = 0\n",
    "\n",
    "        positions = torch.arange(past_len, past_len + seq_len).unsqueeze(0)\n",
    "\n",
    "        x = self.embedding(input_ids) + self.positional_encoding(positions)\n",
    "\n",
    "        new_kv_caches = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer_cache = past_kv_caches[i] if past_kv_caches is not None else None\n",
    "\n",
    "            x, new_cache = layer(x, layer_cache, use_cache)\n",
    "\n",
    "            if use_cache:\n",
    "                new_kv_caches.append(new_cache)\n",
    "\n",
    "        x = self.lm_head(self.ln_norm(x))\n",
    "\n",
    "        return x, new_kv_caches if use_cache else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e8ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_with_cache(model, prompt_ids, max_new_tokens, eos_token_id, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Autoregressive generation using KV cache.\n",
    "    \n",
    "    Args:\n",
    "        model: SimpleTransformer model\n",
    "        prompt_ids: [batch, prompt_len] - input token IDs\n",
    "        max_new_tokens: number of tokens to generate\n",
    "        temperature: sampling temperature\n",
    "        \n",
    "    Returns:\n",
    "        generated_ids: [batch, prompt_len + max_new_tokens]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch_size = prompt_ids.shape[0]\n",
    "    \n",
    "    generated = prompt_ids.clone()\n",
    "    \n",
    "    past_kv_caches = None\n",
    "    \n",
    "    for step in range(max_new_tokens):\n",
    "        if step == 0:\n",
    "            input_ids = prompt_ids\n",
    "        else:\n",
    "            input_ids = generated[:, -1:] \n",
    "        \n",
    "        logits, past_kv_caches = model(\n",
    "            input_ids, \n",
    "            past_kv_caches=past_kv_caches,\n",
    "            use_cache=True\n",
    "        )\n",
    "        \n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        if (next_token == eos_token_id).all():\n",
    "            break\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58275950",
   "metadata": {},
   "source": [
    "### Multi-Layer & Batched KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4802944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a14ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    def __init__(self, key_cache: torch.Tensor, val_cache: torch.Tensor):\n",
    "        self.key_cache = key_cache\n",
    "        self.val_cache = val_cache\n",
    "\n",
    "    @property\n",
    "    def seq_len(self) -> int:\n",
    "        \"\"\"Current sequence length in cache\"\"\"\n",
    "        return self.key_cache.shape[2]\n",
    "    \n",
    "    @property\n",
    "    def batch_size(self) -> int:\n",
    "        return self.key_cache.shape[0]\n",
    "    \n",
    "    @property\n",
    "    def num_heads(self) -> int:\n",
    "        return self.key_cache.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def head_dim(self) -> int:\n",
    "        return self.key_cache.shape[3]\n",
    "\n",
    "    def update(self, new_key: torch.Tensor, new_val: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Append new K,V to cache.\n",
    "        \n",
    "        Args:\n",
    "            new_key: [batch, num_heads, new_len, head_dim]\n",
    "            new_value: [batch, num_heads, new_len, head_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Updated KVCache (new object)\n",
    "        \"\"\"\n",
    "        self.key_cache = torch.cat([self.key_cache, new_key], dim=2)\n",
    "        self.val_cache = torch.cat([self.val_cache, new_val], dim=2)\n",
    "\n",
    "        return self.key_cache, self.val_cache\n",
    "\n",
    "    def get(self):\n",
    "        return self.key_cache, self.val_cache\n",
    "\n",
    "    def get_memory_usage(self) -> dict:\n",
    "        \"\"\"Calculate memory usage in MB\"\"\"\n",
    "        k_bytes = self.key_cache.numel() * self.key_cache.element_size()\n",
    "        v_bytes = self.value_cache.numel() * self.value_cache.element_size()\n",
    "        \n",
    "        return {\n",
    "            'key_mb': k_bytes / (1024 ** 2),\n",
    "            'value_mb': v_bytes / (1024 ** 2),\n",
    "            'total_mb': (k_bytes + v_bytes) / (1024 ** 2)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a075763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerKVCache:\n",
    "    def __init__(self, num_layers: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_layers: number of transformer layers\n",
    "        \"\"\"\n",
    "        self.num_layers = num_layers\n",
    "        self.caches = [None] * num_layers\n",
    "\n",
    "    def update(self, layer_idx: int, new_key: torch.Tensor, new_val: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Update cache for a specific layer.\n",
    "        \n",
    "        Args:\n",
    "            layer_idx: which layer to update (0 to num_layers-1)\n",
    "            new_key: [batch, num_heads, seq_len, head_dim]\n",
    "            new_value: [batch, num_heads, seq_len, head_dim]\n",
    "        \"\"\"\n",
    "        if self.caches[layer_idx] is None:\n",
    "            self.caches[layer_idx] = KVCache(new_key, new_val)\n",
    "        else:\n",
    "            new_key, new_val = self.caches[layer_idx].update(new_key, new_val)\n",
    "\n",
    "        return new_key, new_val\n",
    "\n",
    "    def get(self, layer_idx: int):\n",
    "        \"\"\"\n",
    "        Get cache for a specific layer.\n",
    "        \n",
    "        Args:\n",
    "            layer_idx: which layer's cache to retrieve\n",
    "        \n",
    "        Returns:\n",
    "            KVCache object or None if not initialized\n",
    "        \"\"\"\n",
    "        return self.caches[layer_idx]\n",
    "\n",
    "    def get_seq_len(self) -> int:\n",
    "        \"\"\"\n",
    "        Get current sequence length (same across all layers).\n",
    "        \n",
    "        Returns:\n",
    "            sequence length, or 0 if no cache exists\n",
    "        \"\"\"\n",
    "        if self.caches[0] is None:\n",
    "            return 0\n",
    "        return self.caches[0].seq_len\n",
    "    \n",
    "    def get_total_memory_usage(self) -> dict:\n",
    "        \"\"\"Calculate total memory across all layers\"\"\"\n",
    "        total_mb = 0\n",
    "        num_active_layers = 0\n",
    "        \n",
    "        for cache in self.caches:\n",
    "            if cache is not None:\n",
    "                mem = cache.get_memory_usage()\n",
    "                total_mb += mem['total_mb']\n",
    "                num_active_layers += 1\n",
    "        \n",
    "        avg_per_layer = total_mb / num_active_layers if num_active_layers > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'total_mb': total_mb,\n",
    "            'total_gb': total_mb / 1024,\n",
    "            'per_layer_mb': avg_per_layer,\n",
    "            'num_active_layers': num_active_layers\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd043d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBatched(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        QKV = self.qkv_proj(x).view(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        QKV = QKV.permute(2, 0, 3, 1, 4)\n",
    "        Q, K, V = QKV[0], QKV[1], QKV[2]\n",
    "        \n",
    "        if kv_cache is not None:\n",
    "            K, V = kv_cache.update(K, V)\n",
    "        \n",
    "        scores = Q @ K.transpose(-2, -1) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_output = attn_weights @ V\n",
    "        \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        new_cache = None\n",
    "        if not kv_cache:\n",
    "            new_cache = KVCache(K, V)\n",
    "        \n",
    "        return output, new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "867827a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len, past_len):\n",
    "    \"\"\"\n",
    "    Create causal attention mask for autoregressive generation.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: length of current sequence\n",
    "        past_len: length of cached sequence\n",
    "        device: torch device\n",
    "        \n",
    "    Returns:\n",
    "        mask: [1, 1, seq_len, past_len + seq_len]\n",
    "              1 = attend, 0 = mask\n",
    "    \"\"\"\n",
    "    past_mask = torch.ones(seq_len, past_len)\n",
    "    current_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "\n",
    "    mask = torch.cat([past_mask, current_mask], dim=1)\n",
    "    mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3b31ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(input_length: torch.Tensor, max_len: int, past_len: int = 0):\n",
    "    \"\"\"\n",
    "    Create padding mask for batched sequences with different lengths.\n",
    "    \n",
    "    Args:\n",
    "        input_lengths: [batch] - actual length of each sequence (without padding)\n",
    "        max_len: maximum sequence length in batch (without padding)\n",
    "        past_len: length of cached sequence\n",
    "        \n",
    "    Returns:\n",
    "        mask: [batch, 1, 1, past_len + max_len]\n",
    "              1 = attend (real token), 0 = mask (padding)\n",
    "    \"\"\"\n",
    "    batch_size = input_length.shape[0]\n",
    "    total_len = past_len + max_len\n",
    "\n",
    "    positions = torch.arange(total_len).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "    pad_amounts = total_len - (input_length + past_len)\n",
    "\n",
    "    mask = positions >= pad_amounts.unsqueeze(1)\n",
    "\n",
    "    return mask.unsqueeze(1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "363161d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_masks(causal_mask: torch.Tensor, padding_mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Combine causal and padding masks.\n",
    "    Both must be 1 for attention, 0 for masking.\n",
    "    \"\"\"\n",
    "    # Broadcasting: causal_mask [1, 1, seq_len, total_len]\n",
    "    #               padding_mask [batch, 1, 1, total_len]\n",
    "    # Result: [batch, 1, seq_len, total_len]\n",
    "    return causal_mask & padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fa17b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayerWithCache(nn.Module):\n",
    "    \"\"\"Transformer layer using new cache structure\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttentionBatched(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "        use_cache: bool = False\n",
    "    ):\n",
    "        attn_out, new_cache = self.self_attn(\n",
    "            self.norm1(x),\n",
    "            attention_mask=attention_mask,\n",
    "            kv_cache=kv_cache,\n",
    "            use_cache=use_cache\n",
    "        )\n",
    "        x = x + self.dropout1(attn_out)\n",
    "        x = x + self.dropout2(self.ffn(self.norm2(x)))\n",
    "        \n",
    "        return x, new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47bde70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithCache(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer with proper cache management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayerWithCache(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[MultiLayerKVCache] = None,\n",
    "        use_cache: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: [batch, seq_len]\n",
    "            attention_mask: [batch, 1, seq_len, total_len] or None\n",
    "            kv_cache: MultiLayerKVCache or None\n",
    "            use_cache: bool\n",
    "            \n",
    "        Returns:\n",
    "            logits: [batch, seq_len, vocab_size]\n",
    "            new_cache: MultiLayerKVCache or None\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        past_len = kv_cache.get_seq_len() if kv_cache is not None else 0\n",
    "        \n",
    "        position_ids = torch.arange(\n",
    "            past_len, past_len + seq_len,\n",
    "            device=input_ids.device\n",
    "        ).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        x = self.token_embedding(input_ids) + self.pos_embedding(position_ids)\n",
    "        new_cache = MultiLayerKVCache(self.num_layers) if use_cache else None\n",
    "        \n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            layer_cache = kv_cache.get(layer_idx) if kv_cache is not None else None\n",
    "            \n",
    "            x, layer_new_cache = layer(\n",
    "                x,\n",
    "                attention_mask=attention_mask,\n",
    "                kv_cache=layer_cache,\n",
    "                use_cache=use_cache\n",
    "            )\n",
    "            \n",
    "            if use_cache:\n",
    "                new_cache.caches[layer_idx] = layer_new_cache\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits, new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee96566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_batched(\n",
    "    model: TransformerWithCache,\n",
    "    prompt_ids_list: List[torch.Tensor],\n",
    "    max_new_tokens: int,\n",
    "    temperature: float = 1.0,\n",
    "    pad_token_id: int = 0\n",
    ") -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Batched generation with variable-length prompts.\n",
    "    \n",
    "    Args:\n",
    "        model: TransformerWithCache\n",
    "        prompt_ids_list: list of [1, prompt_len] tensors (different lengths OK!)\n",
    "        max_new_tokens: how many tokens to generate\n",
    "        temperature: sampling temperature\n",
    "        pad_token_id: ID for padding token\n",
    "        \n",
    "    Returns:\n",
    "        list of generated sequences [1, total_len]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    batch_size = len(prompt_ids_list)\n",
    "    \n",
    "    prompt_lengths = torch.tensor([p.shape[1] for p in prompt_ids_list], device=device)\n",
    "    max_prompt_len = prompt_lengths.max().item()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Batched Generation: {batch_size} sequences\")\n",
    "    print(f\"Prompt lengths: {prompt_lengths.tolist()}\")\n",
    "    print(f\"Max prompt: {max_prompt_len}, Generating: {max_new_tokens}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    padded_prompts = []\n",
    "    for prompt in prompt_ids_list:\n",
    "        prompt = prompt.to(device)\n",
    "        pad_len = max_prompt_len - prompt.shape[1]\n",
    "        if pad_len > 0:\n",
    "            padding = torch.full((1, pad_len), pad_token_id, device=device, dtype=prompt.dtype)\n",
    "            prompt = torch.cat([padding, prompt], dim=1)\n",
    "        padded_prompts.append(prompt)\n",
    "    \n",
    "    input_ids = torch.cat(padded_prompts, dim=0)\n",
    "    \n",
    "    finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "    \n",
    "    kv_cache = None\n",
    "    \n",
    "    all_generated = input_ids.clone()\n",
    "    \n",
    "    for step in range(max_new_tokens):\n",
    "        past_len = kv_cache.get_seq_len() if kv_cache is not None else 0\n",
    "        \n",
    "        if step == 0:\n",
    "            current_input = input_ids\n",
    "            seq_len = max_prompt_len\n",
    "            \n",
    "            padding_mask = create_padding_mask(prompt_lengths, max_prompt_len, past_len=0)\n",
    "            causal_mask = create_causal_mask(seq_len, past_len=0, device=device)\n",
    "            attention_mask = combine_masks(causal_mask, padding_mask)\n",
    "            \n",
    "            print(f\"PREFILL: Processing {seq_len} tokens (batch={batch_size})\")\n",
    "        else:\n",
    "            current_input = all_generated[:, -1:]\n",
    "            seq_len = 1\n",
    "            \n",
    "            total_lengths = prompt_lengths + step \n",
    "            padding_mask = create_padding_mask(prompt_lengths, seq_len, past_len=past_len)\n",
    "            causal_mask = create_causal_mask(seq_len, past_len=past_len, device=device)\n",
    "            attention_mask = combine_masks(causal_mask, padding_mask)\n",
    "            \n",
    "            print(f\" DECODE {step}: cache_len={past_len}\")\n",
    "        \n",
    "        logits, kv_cache = model(\n",
    "            current_input,\n",
    "            attention_mask=attention_mask,\n",
    "            kv_cache=kv_cache,\n",
    "            use_cache=True\n",
    "        )\n",
    "        \n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        next_tokens = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        next_tokens = torch.where(\n",
    "            finished.unsqueeze(1),\n",
    "            torch.tensor(pad_token_id, device=device),\n",
    "            next_tokens\n",
    "        )\n",
    "        \n",
    "        all_generated = torch.cat([all_generated, next_tokens], dim=1)\n",
    "        \n",
    "    print(f\"\\n Generation complete!\")\n",
    "    print(f\"Final cache: {kv_cache}\")\n",
    "    \n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a457d",
   "metadata": {},
   "source": [
    "#### Memory-Efficient Cache Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0825d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowCache(MultiLayerKVCache):\n",
    "    \"\"\"\n",
    "    KV Cache with sliding window - keeps only last N tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers: int, window_size: int, num_sink_tokens: int = 4):\n",
    "        super().__init__(num_layers)\n",
    "        self.window_size = window_size\n",
    "        self.num_sink_tokens = num_sink_tokens \n",
    "    \n",
    "    def update(self, layer_idx: int, new_key: torch.Tensor, new_value: torch.Tensor):\n",
    "        \"\"\"Update with sliding window logic\"\"\"\n",
    "        if self.caches[layer_idx] is None:\n",
    "            self.caches[layer_idx] = KVCache(key_cache=new_key, value_cache=new_value)\n",
    "        else:\n",
    "            cache = self.caches[layer_idx]\n",
    "            updated_k = torch.cat([cache.key_cache, new_key], dim=2)\n",
    "            updated_v = torch.cat([cache.value_cache, new_value], dim=2)\n",
    "            \n",
    "            current_len = updated_k.shape[2]\n",
    "            if current_len > self.window_size:\n",
    "                keep_len = self.window_size - self.num_sink_tokens\n",
    "                \n",
    "                sink_k = updated_k[:, :, :self.num_sink_tokens, :]\n",
    "                sink_v = updated_v[:, :, :self.num_sink_tokens, :]\n",
    "                \n",
    "                window_k = updated_k[:, :, -keep_len:, :]\n",
    "                window_v = updated_v[:, :, -keep_len:, :]\n",
    "                \n",
    "                updated_k = torch.cat([sink_k, window_k], dim=2)\n",
    "                updated_v = torch.cat([sink_v, window_v], dim=2)\n",
    "            \n",
    "            self.caches[layer_idx] = KVCache(key_cache=updated_k, value_cache=updated_v)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        base_repr = super().__repr__()\n",
    "        return f\"SlidingWindow{base_repr}, window={self.window_size}, sinks={self.num_sink_tokens}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2108fe9c",
   "metadata": {},
   "source": [
    "#### KV Cache Quantization (INT8/FP8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67fb4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedKVCache:\n",
    "    \"\"\"\n",
    "    INT8 quantized KV cache with adaptive re-quantization.\n",
    "    Re-quantizes only when new tokens exceed current scale.\n",
    "    \"\"\"\n",
    "    def __init__(self, key_cache_fp: torch.Tensor, val_cache_fp: torch.Tensor, quantize: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            key_cache_fp: [batch, num_heads, seq_len, head_dim] in FP32\n",
    "            val_cache_fp: [batch, num_heads, seq_len, head_dim] in FP32\n",
    "            quantize: whether to quantize (False for testing)\n",
    "        \"\"\"\n",
    "        self.device = key_cache_fp.device\n",
    "        self.dtype_original = key_cache_fp.dtype\n",
    "\n",
    "        if quantize:\n",
    "            self.key_cache, self.key_scale = self._quantize_to_int8(key_cache_fp)\n",
    "            self.val_cache, self.val_scale = self._quantize_to_int8(val_cache_fp)\n",
    "        else:\n",
    "            self.key_cache = key_cache_fp\n",
    "            self.val_cache = val_cache_fp\n",
    "            self.key_scale = None\n",
    "            self.val_scale = None\n",
    "\n",
    "        self.is_quantized = quantize\n",
    "        self.num_requantizations = 0 \n",
    "\n",
    "    def _quantize_to_int8(self, tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Symmetric per-tensor quantization to INT8.\n",
    "        \n",
    "        Args:\n",
    "            tensor: FP32 tensor to quantize\n",
    "            \n",
    "        Returns:\n",
    "            quantized: INT8 tensor\n",
    "            scale: FP32 scalar scale factor\n",
    "        \"\"\"\n",
    "        q_max = 127.0\n",
    "        \n",
    "        max_abs = tensor.abs().max()\n",
    "        \n",
    "        if max_abs == 0:\n",
    "            scale = torch.tensor(1.0, device=tensor.device, dtype=torch.float32)\n",
    "        else:\n",
    "            scale = max_abs / q_max\n",
    "        \n",
    "        quantized = torch.clamp(\n",
    "            torch.round(tensor / scale), \n",
    "            -127, 127\n",
    "        ).to(torch.int8)\n",
    "        \n",
    "        return quantized, scale\n",
    "\n",
    "    def _dequantize_from_int8(self, quantized: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Dequantize INT8 tensor back to FP32.\"\"\"\n",
    "        return quantized.to(torch.float32) * scale\n",
    "\n",
    "    def update(self, new_key: torch.Tensor, new_val: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Append new K,V to cache with adaptive re-quantization.\n",
    "        \n",
    "        Re-quantizes entire cache only if new tokens have larger scale.\n",
    "        \"\"\"\n",
    "        if self.is_quantized:\n",
    "            # Compute what scales the new tokens would need\n",
    "            _, new_k_scale = self._quantize_to_int8(new_key)\n",
    "            _, new_v_scale = self._quantize_to_int8(new_val)\n",
    "            \n",
    "            # Check if we need to re-quantize\n",
    "            needs_requant_k = new_k_scale > self.key_scale\n",
    "            needs_requant_v = new_v_scale > self.val_scale\n",
    "            \n",
    "            if needs_requant_k or needs_requant_v:\n",
    "                # Re-quantize entire cache with new data\n",
    "                print(f\"Re-quantizing cache (K: {needs_requant_k}, V: {needs_requant_v})\")\n",
    "                self.num_requantizations += 1\n",
    "                \n",
    "                # Dequantize existing cache\n",
    "                K_fp = self._dequantize_from_int8(self.key_cache, self.key_scale)\n",
    "                V_fp = self._dequantize_from_int8(self.val_cache, self.val_scale)\n",
    "                \n",
    "                # Concatenate in FP32\n",
    "                K_fp = torch.cat([K_fp, new_key], dim=2)\n",
    "                V_fp = torch.cat([V_fp, new_val], dim=2)\n",
    "                \n",
    "                # Re-quantize entire cache with new unified scale\n",
    "                self.key_cache, self.key_scale = self._quantize_to_int8(K_fp)\n",
    "                self.val_cache, self.val_scale = self._quantize_to_int8(V_fp)\n",
    "            else:\n",
    "                # Use existing scale (safe - won't clip)\n",
    "                q_key = torch.clamp(\n",
    "                    torch.round(new_key / self.key_scale), \n",
    "                    -127, 127\n",
    "                ).to(torch.int8)\n",
    "                q_val = torch.clamp(\n",
    "                    torch.round(new_val / self.val_scale), \n",
    "                    -127, 127\n",
    "                ).to(torch.int8)\n",
    "                \n",
    "                self.key_cache = torch.cat([self.key_cache, q_key], dim=2)\n",
    "                self.val_cache = torch.cat([self.val_cache, q_val], dim=2)\n",
    "        else:\n",
    "            self.key_cache = torch.cat([self.key_cache, new_key], dim=2)\n",
    "            self.val_cache = torch.cat([self.val_cache, new_val], dim=2)\n",
    "    \n",
    "    def get_kv_fp(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get K,V in original floating point precision.\"\"\"\n",
    "        if self.is_quantized:\n",
    "            K = self._dequantize_from_int8(self.key_cache, self.key_scale)\n",
    "            V = self._dequantize_from_int8(self.val_cache, self.val_scale)\n",
    "        else:\n",
    "            K = self.key_cache\n",
    "            V = self.val_cache\n",
    "        \n",
    "        return K, V\n",
    "\n",
    "    def get_memory_usage(self) -> dict:\n",
    "        \"\"\"Calculate memory usage\"\"\"\n",
    "        if self.is_quantized:\n",
    "            k_bytes = self.key_cache.numel() * 1\n",
    "            v_bytes = self.val_cache.numel() * 1\n",
    "            scale_bytes = (self.key_scale.numel() * 4 + self.val_scale.numel() * 4)\n",
    "        else:\n",
    "            k_bytes = self.key_cache.numel() * self.key_cache.element_size()\n",
    "            v_bytes = self.val_cache.numel() * self.val_cache.element_size()\n",
    "            scale_bytes = 0\n",
    "        \n",
    "        total_mb = (k_bytes + v_bytes + scale_bytes) / (1024 ** 2)\n",
    "        \n",
    "        return {\n",
    "            'key_mb': k_bytes / (1024 ** 2),\n",
    "            'value_mb': v_bytes / (1024 ** 2),\n",
    "            'scale_mb': scale_bytes / (1024 ** 2),\n",
    "            'total_mb': total_mb,\n",
    "            'num_requantizations': self.num_requantizations\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def seq_len(self) -> int:\n",
    "        \"\"\"Current sequence length in cache\"\"\"\n",
    "        return self.key_cache.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c0d8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedKVCachePerToken:\n",
    "    \"\"\"\n",
    "    INT8 quantized KV cache with per-token scales.\n",
    "    Stores separate scale for each sequence position for maximum accuracy.\n",
    "    \"\"\"\n",
    "    def __init__(self, key_cache_fp: torch.Tensor, val_cache_fp: torch.Tensor, quantize: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            key_cache_fp: [batch, num_heads, seq_len, head_dim] in FP32\n",
    "            val_cache_fp: [batch, num_heads, seq_len, head_dim] in FP32\n",
    "            quantize: whether to quantize (False for testing)\n",
    "        \"\"\"\n",
    "        self.device = key_cache_fp.device\n",
    "        self.dtype_original = key_cache_fp.dtype\n",
    "        self.is_quantized = quantize\n",
    "        \n",
    "        if quantize:\n",
    "            # Quantize each token separately and store its scale\n",
    "            batch, num_heads, seq_len, head_dim = key_cache_fp.shape\n",
    "            \n",
    "            self.key_cache = torch.zeros(\n",
    "                (batch, num_heads, seq_len, head_dim), \n",
    "                dtype=torch.int8, \n",
    "                device=self.device\n",
    "            )\n",
    "            self.val_cache = torch.zeros(\n",
    "                (batch, num_heads, seq_len, head_dim), \n",
    "                dtype=torch.int8, \n",
    "                device=self.device\n",
    "            )\n",
    "            \n",
    "            # Store scales per sequence position\n",
    "            self.key_scales = []  # List of [batch, num_heads, 1, 1] tensors\n",
    "            self.val_scales = []\n",
    "            \n",
    "            # Quantize each position\n",
    "            for i in range(seq_len):\n",
    "                k_token = key_cache_fp[:, :, i:i+1, :]\n",
    "                v_token = val_cache_fp[:, :, i:i+1, :]\n",
    "                \n",
    "                q_k, scale_k = self._quantize_to_int8_per_token(k_token)\n",
    "                q_v, scale_v = self._quantize_to_int8_per_token(v_token)\n",
    "                \n",
    "                self.key_cache[:, :, i:i+1, :] = q_k\n",
    "                self.val_cache[:, :, i:i+1, :] = q_v\n",
    "                \n",
    "                self.key_scales.append(scale_k)\n",
    "                self.val_scales.append(scale_v)\n",
    "        else:\n",
    "            self.key_cache = key_cache_fp\n",
    "            self.val_cache = val_cache_fp\n",
    "            self.key_scales = None\n",
    "            self.val_scales = None\n",
    "\n",
    "    def _quantize_to_int8_per_token(self, token: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Quantize a single token position.\n",
    "        \n",
    "        Args:\n",
    "            token: [batch, num_heads, 1, head_dim]\n",
    "            \n",
    "        Returns:\n",
    "            quantized: INT8 tensor [batch, num_heads, 1, head_dim]\n",
    "            scale: FP32 tensor [batch, num_heads, 1, 1] - per head scale\n",
    "        \"\"\"\n",
    "        q_max = 127.0\n",
    "        \n",
    "        # Compute scale per head: [batch, num_heads, 1, 1]\n",
    "        max_abs = token.abs().amax(dim=-1, keepdim=True)  # [batch, num_heads, 1, 1]\n",
    "        \n",
    "        scale = torch.where(\n",
    "            max_abs == 0, \n",
    "            torch.tensor(1.0, device=token.device, dtype=torch.float32),\n",
    "            max_abs / q_max\n",
    "        )\n",
    "        \n",
    "        quantized = torch.clamp(\n",
    "            torch.round(token / scale), \n",
    "            -127, 127\n",
    "        ).to(torch.int8)\n",
    "        \n",
    "        return quantized, scale\n",
    "\n",
    "    def update(self, new_key: torch.Tensor, new_val: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Append new K,V to cache, quantizing each token separately.\n",
    "        \n",
    "        Args:\n",
    "            new_key: [batch, num_heads, seq_len_new, head_dim]\n",
    "            new_val: [batch, num_heads, seq_len_new, head_dim]\n",
    "        \"\"\"\n",
    "        if self.is_quantized:\n",
    "            seq_len_new = new_key.shape[2]\n",
    "            \n",
    "            # Pre-allocate space for new tokens\n",
    "            new_k_cache = torch.zeros(\n",
    "                (new_key.shape[0], new_key.shape[1], seq_len_new, new_key.shape[3]),\n",
    "                dtype=torch.int8,\n",
    "                device=self.device\n",
    "            )\n",
    "            new_v_cache = torch.zeros_like(new_k_cache)\n",
    "            \n",
    "            # Quantize each new token position\n",
    "            for i in range(seq_len_new):\n",
    "                k_token = new_key[:, :, i:i+1, :]\n",
    "                v_token = new_val[:, :, i:i+1, :]\n",
    "                \n",
    "                q_k, scale_k = self._quantize_to_int8_per_token(k_token)\n",
    "                q_v, scale_v = self._quantize_to_int8_per_token(v_token)\n",
    "                \n",
    "                new_k_cache[:, :, i:i+1, :] = q_k\n",
    "                new_v_cache[:, :, i:i+1, :] = q_v\n",
    "                \n",
    "                self.key_scales.append(scale_k)\n",
    "                self.val_scales.append(scale_v)\n",
    "            \n",
    "            # Concatenate to existing cache\n",
    "            self.key_cache = torch.cat([self.key_cache, new_k_cache], dim=2)\n",
    "            self.val_cache = torch.cat([self.val_cache, new_v_cache], dim=2)\n",
    "        else:\n",
    "            self.key_cache = torch.cat([self.key_cache, new_key], dim=2)\n",
    "            self.val_cache = torch.cat([self.val_cache, new_val], dim=2)\n",
    "    \n",
    "    def get_kv_fp(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get K,V in original floating point precision.\"\"\"\n",
    "        if self.is_quantized:\n",
    "            seq_len = self.key_cache.shape[2]\n",
    "            \n",
    "            # Dequantize each position with its own scale\n",
    "            K_parts = []\n",
    "            V_parts = []\n",
    "            \n",
    "            for i in range(seq_len):\n",
    "                k_token_int8 = self.key_cache[:, :, i:i+1, :]\n",
    "                v_token_int8 = self.val_cache[:, :, i:i+1, :]\n",
    "                \n",
    "                k_scale = self.key_scales[i]\n",
    "                v_scale = self.val_scales[i]\n",
    "                \n",
    "                k_fp = k_token_int8.to(torch.float32) * k_scale\n",
    "                v_fp = v_token_int8.to(torch.float32) * v_scale\n",
    "                \n",
    "                K_parts.append(k_fp)\n",
    "                V_parts.append(v_fp)\n",
    "            \n",
    "            K = torch.cat(K_parts, dim=2)\n",
    "            V = torch.cat(V_parts, dim=2)\n",
    "        else:\n",
    "            K = self.key_cache\n",
    "            V = self.val_cache\n",
    "        \n",
    "        return K, V\n",
    "\n",
    "    def get_memory_usage(self) -> dict:\n",
    "        \"\"\"Calculate memory usage\"\"\"\n",
    "        if self.is_quantized:\n",
    "            k_bytes = self.key_cache.numel() * 1\n",
    "            v_bytes = self.val_cache.numel() * 1\n",
    "            \n",
    "            # Scales: num_tokens * (batch * num_heads * 1 * 1) * 4 bytes each\n",
    "            num_scales = len(self.key_scales)\n",
    "            scale_bytes = num_scales * (self.key_scales[0].numel() * 4 * 2)  # K and V\n",
    "        else:\n",
    "            k_bytes = self.key_cache.numel() * self.key_cache.element_size()\n",
    "            v_bytes = self.val_cache.numel() * self.val_cache.element_size()\n",
    "            scale_bytes = 0\n",
    "        \n",
    "        total_mb = (k_bytes + v_bytes + scale_bytes) / (1024 ** 2)\n",
    "        \n",
    "        return {\n",
    "            'key_mb': k_bytes / (1024 ** 2),\n",
    "            'value_mb': v_bytes / (1024 ** 2),\n",
    "            'scale_mb': scale_bytes / (1024 ** 2),\n",
    "            'total_mb': total_mb,\n",
    "            'num_scales': len(self.key_scales) if self.is_quantized else 0\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def seq_len(self) -> int:\n",
    "        \"\"\"Current sequence length in cache\"\"\"\n",
    "        return self.key_cache.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a640f",
   "metadata": {},
   "source": [
    "### PagedAttention Concepts (vLLM)\n",
    "\n",
    "PagedAttention Key Ideas:\n",
    "\n",
    "1. **Problem with Continuous Cache:**\n",
    "   - Traditional: KV cache is contiguous tensor\n",
    "   - Issue: Memory fragmentation, can't easily share cache between sequences\n",
    "   \n",
    "2. **PagedAttention Solution:**\n",
    "   - Split KV cache into fixed-size \"pages\" (blocks)\n",
    "   - Like virtual memory in OS!\n",
    "   - Pages can be non-contiguous in physical memory\n",
    "   \n",
    "3. **Benefits:**\n",
    "   - Near-zero memory fragmentation\n",
    "   - Easy cache sharing (beam search, parallel sampling)\n",
    "   - Dynamic memory allocation\n",
    "\n",
    "Visual: \n",
    "1. Traditional:\n",
    "- [Seq1: KKKKKK...] [Seq2: KKKKKK...] [Wasted Space] [Seq3: KKK...]\n",
    "\n",
    "2. PagedAttention:\n",
    "- Physical Memory: [Block0][Block1][Block2][Block3][Block4][Block5]\n",
    "- Seq1 mapping: Block0 → Block2 → Block4\n",
    "- Seq2 mapping: Block1 → Block3\n",
    "- Seq3 mapping: Block5\n",
    "(No fragmentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74c8d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagedKVCache:\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_blocks: int,\n",
    "        block_size: int,\n",
    "        num_heads: int,\n",
    "        head_dim: int,\n",
    "        dtype=torch.float32\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_blocks: Total number of blocks in memory pool\n",
    "            block_size: Number of tokens per block\n",
    "            num_heads: Number of KV heads\n",
    "            head_dim: Dimension per head\n",
    "        \"\"\"\n",
    "        self.num_blocks = num_blocks\n",
    "        self.block_size = block_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        \n",
    "        self.blocks = torch.zeros(\n",
    "            num_blocks, 2, block_size, num_heads, head_dim, dtype=dtype\n",
    "        )\n",
    "\n",
    "        self.free_blocks = set(range(num_blocks))\n",
    "\n",
    "        self.seq_to_blocks = {}\n",
    "        self.seq_lengths = {}\n",
    "\n",
    "    def allocate_blocks(self, seq_id: int, num_blocks_needed: int) -> bool:\n",
    "        \"\"\"\n",
    "        Allocate blocks for a sequence.\n",
    "        \n",
    "        Returns:\n",
    "            True if successful, False if not enough free blocks\n",
    "        \"\"\"\n",
    "        if len(self.free_blocks) < num_blocks_needed:\n",
    "            return False\n",
    "\n",
    "        allocated = []\n",
    "        for _ in range(num_blocks_needed):\n",
    "            block_id = self.free_blocks.pop()\n",
    "            allocated.append(block_id)\n",
    "\n",
    "        self.seq_to_blocks[seq_id] = allocated\n",
    "        self.seq_lengths[seq_id] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def append_tokens(self, seq_id: int, new_k: torch.Tensor, new_v: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Append new K,V to sequence's cache.\n",
    "        \n",
    "        Args:\n",
    "            seq_id: Sequence ID\n",
    "            new_k: [num_tokens, num_heads, head_dim]\n",
    "            new_v: [num_tokens, num_heads, head_dim]\n",
    "        \n",
    "        Returns:\n",
    "            True if successful, False if need more blocks\n",
    "        \"\"\"\n",
    "        if seq_id not in self.seq_to_blocks or seq_id not in self.seq_lengths:\n",
    "            return True\n",
    "\n",
    "        num_new_tokens = new_k.shape[0]\n",
    "        current_len = self.seq_lengths[seq_id]\n",
    "        new_len = current_len + num_new_tokens\n",
    "\n",
    "        blocks_needed = (new_len + self.block_size - 1) // self.block_size\n",
    "        blocks_allocated = len(self.seq_to_blocks[seq_id])\n",
    "\n",
    "        if blocks_needed > blocks_allocated:\n",
    "            additional_blocks = blocks_needed - blocks_allocated\n",
    "            if len(self.free_blocks) < additional_blocks:\n",
    "                return True\n",
    "\n",
    "            for _ in range(additional_blocks):\n",
    "                block_id = self.free_blocks.pop()\n",
    "                self.seq_to_blocks[seq_id].append(block_id)\n",
    "\n",
    "        \n",
    "        for i, (k_token, v_token) in enumerate(zip(new_k, new_v)):\n",
    "            token_idx = current_len + i\n",
    "            block_idx = token_idx // self.block_size\n",
    "            offset = token_idx % self.block_size\n",
    "\n",
    "            block_id = self.seq_to_blocks[seq_id][block_idx]\n",
    "\n",
    "            self.blocks[block_id, 0, offset] = k_token\n",
    "            self.blocks[block_id, 1, offset] = v_token\n",
    "\n",
    "        self.seq_lengths[seq_id] = new_len\n",
    "        return True\n",
    "\n",
    "    def get_kv(self, seq_id: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Gather K,V for a sequence from its blocks.\n",
    "        \n",
    "        Returns:\n",
    "            K: [seq_len, num_heads, head_dim]\n",
    "            V: [seq_len, num_heads, head_dim]\n",
    "        \"\"\"\n",
    "        if seq_id not in self.seq_to_blocks:\n",
    "            raise ValueError(f\"Sequence {seq_id} not found\")\n",
    "\n",
    "        seq_len = self.seq_lengths[seq_id]\n",
    "        block_ids = self.seq_to_blocks[seq_id]\n",
    "\n",
    "        K_list = []\n",
    "        V_list = []\n",
    "\n",
    "        for block_idx, block_id in enumerate(blocks_ids):\n",
    "            start_token = block_idx * self.block_size\n",
    "            end_token = min(start_token + self.block_size, seq_len)\n",
    "\n",
    "            num_tokens = end_token - start_token\n",
    "\n",
    "            if num_tokens > 0:\n",
    "                K_list.append(self.blocks[block_id, 0, :num_tokens])\n",
    "                V_list.append(self.blocks[block_id, 1, :num_tokens])\n",
    "\n",
    "        K = torch.cat(K_list, dim=0)\n",
    "        V = torch.cat(V_list, dim=0)\n",
    "\n",
    "        return K, V\n",
    "\n",
    "    def free_sequence(self, seq_id: int):\n",
    "        \"\"\"Free all blocks associated with a sequence\"\"\"\n",
    "        if seq_id in self.seq_to_blocks:\n",
    "            for block_id in self.seq_to_blocks[seq_id]:\n",
    "                self.free_blocks.add(block_id)\n",
    "            \n",
    "            del self.seq_to_blocks[seq_id]\n",
    "            del self.seq_lengths[seq_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd80b5",
   "metadata": {},
   "source": [
    "#### Grouped Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12848ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped-Query Attention (GQA) - used in LLaMA-2, Mistral, etc.\n",
    "    \n",
    "    Reduces KV cache by sharing K,V across groups of query heads.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_query_heads: int,\n",
    "        num_kv_heads: int,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_query_heads == 0, \"d_model must be divisible by num_query_heads\"\n",
    "        assert num_query_heads % num_kv_heads == 0, \"num_query_heads must be divisible by num_kv_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_query_heads = num_query_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = d_model // num_query_heads\n",
    "        \n",
    "        # Number of query heads per KV head\n",
    "        self.num_queries_per_kv = num_query_heads // num_kv_heads\n",
    "        \n",
    "        # Projections\n",
    "        self.W_q = nn.Linear(d_model, num_query_heads * self.head_dim, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, num_kv_heads * self.head_dim, bias=False)  # Smaller!\n",
    "        self.W_v = nn.Linear(d_model, num_kv_heads * self.head_dim, bias=False)  # Smaller!\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, d_model]\n",
    "            attention_mask: [batch, 1, seq_len, total_len]\n",
    "            kv_cache: (K, V) where K,V are [batch, num_kv_heads, past_len, head_dim]\n",
    "            use_cache: bool\n",
    "            \n",
    "        Returns:\n",
    "            output: [batch, seq_len, d_model]\n",
    "            new_cache: (K, V) or None\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x)  # [batch, seq_len, num_query_heads * head_dim]\n",
    "        K = self.W_k(x)  # [batch, seq_len, num_kv_heads * head_dim]\n",
    "        V = self.W_v(x)  # [batch, seq_len, num_kv_heads * head_dim]\n",
    "        \n",
    "        # Reshape Q: [batch, num_query_heads, seq_len, head_dim]\n",
    "        Q = Q.view(batch_size, seq_len, self.num_query_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Reshape K,V: [batch, num_kv_heads, seq_len, head_dim]\n",
    "        K = K.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Handle cache\n",
    "        if kv_cache is not None:\n",
    "            K_cache, V_cache = kv_cache\n",
    "            K = torch.cat([K_cache, K], dim=2)\n",
    "            V = torch.cat([V_cache, V], dim=2)\n",
    "        \n",
    "        # CRITICAL: Repeat K,V to match number of query heads\n",
    "        # Each KV head is shared across num_queries_per_kv query heads\n",
    "        # [batch, num_kv_heads, seq_len, head_dim] → [batch, num_query_heads, seq_len, head_dim]\n",
    "        K_repeated = K.repeat_interleave(self.num_queries_per_kv, dim=1)\n",
    "        V_repeated = V.repeat_interleave(self.num_queries_per_kv, dim=1)\n",
    "        \n",
    "        # Now K_repeated and V_repeated have same num_heads as Q\n",
    "        # Compute attention\n",
    "        scores = Q @ K_repeated.transpose(-2, -1) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn_output = attn_weights @ V_repeated\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        # Return cache (store original K,V, NOT repeated versions!)\n",
    "        new_cache = (K, V) if use_cache else None\n",
    "        \n",
    "        return output, new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7915534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Query Attention (MQA) - extreme version where num_kv_heads = 1.\n",
    "    Used in PaLM, StarCoder, Falcon.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_query_heads: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_query_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_query_heads = num_query_heads\n",
    "        self.head_dim = d_model // num_query_heads\n",
    "        \n",
    "        # Q has full heads, K,V have only 1 head\n",
    "        self.W_q = nn.Linear(d_model, num_query_heads * self.head_dim, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, self.head_dim, bias=False)  # Single head!\n",
    "        self.W_v = nn.Linear(d_model, self.head_dim, bias=False)  # Single head!\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            kv_cache: (K, V) where K,V are [batch, 1, past_len, head_dim]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Q: [batch, num_query_heads, seq_len, head_dim]\n",
    "        Q = Q.view(batch_size, seq_len, self.num_query_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # K,V: [batch, 1, seq_len, head_dim]\n",
    "        K = K.view(batch_size, seq_len, 1, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, 1, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Handle cache\n",
    "        if kv_cache is not None:\n",
    "            K_cache, V_cache = kv_cache\n",
    "            K = torch.cat([K_cache, K], dim=2)\n",
    "            V = torch.cat([V_cache, V], dim=2)\n",
    "        \n",
    "        # Repeat K,V to match Q heads\n",
    "        K_repeated = K.expand(-1, self.num_query_heads, -1, -1)\n",
    "        V_repeated = V.expand(-1, self.num_query_heads, -1, -1)\n",
    "        \n",
    "        # Attention\n",
    "        scores = Q @ K_repeated.transpose(-2, -1) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn_output = attn_weights @ V_repeated\n",
    "        \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        new_cache = (K, V) if use_cache else None\n",
    "        \n",
    "        return output, new_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a49c05",
   "metadata": {},
   "source": [
    "### Flash Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0676d9e0",
   "metadata": {},
   "source": [
    "Flash Attention Key Ideas:\n",
    "\n",
    "Standard Attention Problem:\n",
    "1. Compute scores: Q @ K^T (materialize NxN matrix)\n",
    "2. Softmax: softmax(scores) (materialize NxN matrix)\n",
    "3. Output: attn @ V\n",
    "\n",
    "Memory: O(N^2) - stores full attention matrix\n",
    "\n",
    "Flash Attention Solution:\n",
    "- Tile Q, K, V\n",
    "- Compute attention in blocks\n",
    "- Never materialize full NxN matrix\n",
    "- Memory: O(N) instead of O(N^2)\n",
    "\n",
    "With KV Cache:\n",
    "- Flash Attention + KV cache is THE standard for production\n",
    "- vLLM, TensorRT-LLM all use this combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a767a2",
   "metadata": {},
   "source": [
    "Flash Attention + KV Cache Best Practices:\n",
    "\n",
    "1. **Always use Flash Attention for long sequences (>512 tokens)**\n",
    "   - 2-4x speedup on attention computation\n",
    "   - Enables longer contexts (up to 100k+ tokens)\n",
    "\n",
    "2. **Combine with GQA for maximum efficiency**\n",
    "   - GQA reduces cache size\n",
    "   - Flash Attention reduces compute\n",
    "   - Together: massive throughput gains\n",
    "\n",
    "3. **Production Stack:**\n",
    "   - vLLM: PagedAttention + Flash Attention + GQA + INT8 cache\n",
    "   - TensorRT-LLM: Flash Attention + GQA + FP8 cache\n",
    "   - SGLang: RadixAttention (prefix caching) + Flash Attention\n",
    "\n",
    "4. **When to use what:**\n",
    "   - Batch=1, latency-critical: Flash Attention + FP16 cache\n",
    "   - High throughput serving: Flash + PagedAttention + INT8 cache\n",
    "   - Long context (>8k): Flash + GQA + sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a261399",
   "metadata": {},
   "source": [
    "#### Continuous Batching Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bbac920f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 Continuous Batching + KV Cache:\n",
      "\n",
      "Key Insight: PagedAttention makes continuous batching efficient!\n",
      "\n",
      "Without PagedAttention:\n",
      "- Hard to dynamically resize cache tensors\n",
      "- Memory fragmentation when swapping sequences\n",
      "- Copying overhead\n",
      "\n",
      "With PagedAttention:\n",
      "- Just update block mappings (O(1) operation)\n",
      "- No memory copying\n",
      "- Perfect for continuous batching\n",
      "\n",
      "This is why vLLM achieves 10-20x higher throughput than HuggingFace!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Continuous Batching (used in vLLM, TensorRT-LLM):\n",
    "\n",
    "Traditional Batching:\n",
    "- Wait for N requests\n",
    "- Process all together\n",
    "- All finish at different times → wasted compute\n",
    "\n",
    "Continuous Batching:\n",
    "- As soon as one request finishes, add new one\n",
    "- Dynamically change batch composition\n",
    "- Maximum GPU utilization\n",
    "\n",
    "KV Cache Challenges:\n",
    "1. Variable sequence lengths in batch\n",
    "2. Sequences finishing at different times\n",
    "3. Need to efficiently add/remove from batch\n",
    "\n",
    "Solutions:\n",
    "1. PagedAttention: Easy to swap sequences in/out\n",
    "2. Padding + masking: Handle variable lengths\n",
    "3. Separate prefill and decode batches\n",
    "\"\"\"\n",
    "\n",
    "class ContinuousBatchManager:\n",
    "    \"\"\"\n",
    "    Manages continuous batching with KV cache.\n",
    "    Simplified version of what vLLM does.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, max_batch_size: int, paged_cache: PagedKVCache):\n",
    "        self.model = model\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.cache = paged_cache\n",
    "        \n",
    "        # Active sequences\n",
    "        self.active_sequences = {}  # seq_id -> sequence info\n",
    "        \n",
    "    def add_sequence(self, seq_id: int, prompt: torch.Tensor):\n",
    "        \"\"\"Add new sequence to batch\"\"\"\n",
    "        if len(self.active_sequences) >= self.max_batch_size:\n",
    "            return False  # Batch full\n",
    "        \n",
    "        # Allocate cache blocks\n",
    "        prompt_len = prompt.shape[0]\n",
    "        blocks_needed = (prompt_len + self.cache.block_size - 1) // self.cache.block_size\n",
    "        \n",
    "        if not self.cache.allocate_blocks(seq_id, blocks_needed):\n",
    "            return False  # Not enough cache memory\n",
    "        \n",
    "        self.active_sequences[seq_id] = {\n",
    "            'prompt': prompt,\n",
    "            'generated': [],\n",
    "            'finished': False\n",
    "        }\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def remove_sequence(self, seq_id: int):\n",
    "        \"\"\"Remove finished sequence from batch\"\"\"\n",
    "        if seq_id in self.active_sequences:\n",
    "            self.cache.free_sequence(seq_id)\n",
    "            del self.active_sequences[seq_id]\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        One generation step for all active sequences.\n",
    "        \n",
    "        In practice, this would:\n",
    "        1. Separate prefill vs decode\n",
    "        2. Batch compatible sequences together\n",
    "        3. Use PagedAttention to gather K,V\n",
    "        \"\"\"\n",
    "        # Pseudocode for continuous batching step\n",
    "        pass\n",
    "\n",
    "print(\"\"\"\n",
    "💡 Continuous Batching + KV Cache:\n",
    "\n",
    "Key Insight: PagedAttention makes continuous batching efficient!\n",
    "\n",
    "Without PagedAttention:\n",
    "- Hard to dynamically resize cache tensors\n",
    "- Memory fragmentation when swapping sequences\n",
    "- Copying overhead\n",
    "\n",
    "With PagedAttention:\n",
    "- Just update block mappings (O(1) operation)\n",
    "- No memory copying\n",
    "- Perfect for continuous batching\n",
    "\n",
    "This is why vLLM achieves 10-20x higher throughput than HuggingFace!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
