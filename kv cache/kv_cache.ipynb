{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce38d84",
   "metadata": {},
   "source": [
    "### KV Cache - Version 1 (Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5995427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09a51fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWithCache(nn.Module):\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, kv_cache=None, use_cache=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, d_model] - input tokens\n",
    "            kv_cache: tuple of (K_cache, V_cache) or None\n",
    "                K_cache: [batch, num_heads, past_len, head_dim]\n",
    "                V_cache: [batch, num_heads, past_len, head_dim]\n",
    "            use_cache: whether to return updated cache\n",
    "            \n",
    "        Returns:\n",
    "            output: [batch, seq_len, d_model]\n",
    "            new_cache: tuple of (K, V) if use_cache else None\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dimension).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dimension).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dimension).transpose(1, 2)\n",
    "\n",
    "        if use_cahe:\n",
    "            K_all, V_all = kv_cache\n",
    "            K = torch.cat([K_all, K], dim=2)\n",
    "            V = torch.cat([V_all, V], dim=2)\n",
    "\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if seq_len > 1:\n",
    "            causal_mask = torch.tril(\n",
    "                torch.ones(seq_len, K.shape[2])\n",
    "            ).unsqueeze(0).unsqueeze(0)\n",
    "            scores = scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = attn_weights @ V\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "\n",
    "        output = self.W_o(attn_output)\n",
    "\n",
    "        if use_cache:\n",
    "            return output, (K, V)\n",
    "        else:\n",
    "            return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d00719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn_layer = MultiHeadAttentionWithCache(num_heads, d_model)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GeLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, kv_cache=None, use_cache=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, d_model]\n",
    "            kv_cache: tuple of (K_cache, V_cache) or None\n",
    "            use_cache: bool\n",
    "            \n",
    "        Returns:\n",
    "            output: [batch, seq_len, d_model]\n",
    "            new_cache: tuple or None\n",
    "        \"\"\"\n",
    "        x_skip = x\n",
    "        x = self.norm1(x)\n",
    "        x, new_kv_cache = self.attn_layer(x)\n",
    "        x = x_skip + self.dropout1(x)\n",
    "\n",
    "\n",
    "        x_skip = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x_skip + self.dropout2(x)\n",
    "\n",
    "        return x, new_kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531bf4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, num_heads, d_model, num_layers, d_ff, max_seq_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(num_heads, d_model, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, past_kv_caches=None, use_cache=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: [batch, seq_len] - token indices\n",
    "            past_kv_caches: list of (K_cache, V_cache) tuples, one per layer\n",
    "            use_cache: bool\n",
    "            \n",
    "        Returns:\n",
    "            logits: [batch, seq_len, vocab_size]\n",
    "            new_kv_caches: list of (K, V) tuples if use_cache else None\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        if past_kv_caches is not None and past_kv_caches[0] is not None:\n",
    "            past_len = past_kv_caches[0][0].shape[2]\n",
    "        else:\n",
    "            past_len = 0\n",
    "\n",
    "        positions = torch.arange(\n",
    "            past_len, \n",
    "            past_len + seq_len\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        x = self.token_embeddings(x) + self.positional_encoding(x)\n",
    "\n",
    "        new_kv_caches = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer_cache = past_kv_caches[i] if past_kv_caches is not None else None\n",
    "            \n",
    "            x, new_cache = layer(x, kv_cache=layer_caches, use_cache=use_cache)\n",
    "\n",
    "            if use_cache:\n",
    "                new_kv_caches.append(new_cache)\n",
    "\n",
    "        x = self.ln_norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits, new_kv_caches if use else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1e8ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_with_cache(model, prompt_ids, max_new_tokens, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Autoregressive generation using KV cache.\n",
    "    \n",
    "    Args:\n",
    "        model: SimpleTransformer model\n",
    "        prompt_ids: [batch, prompt_len] - input token IDs\n",
    "        max_new_tokens: number of tokens to generate\n",
    "        temperature: sampling temperature\n",
    "        \n",
    "    Returns:\n",
    "        generated_ids: [batch, prompt_len + max_new_tokens]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch_size = prompt_ids.shape[0]\n",
    "    \n",
    "    generated = prompt_ids.clone()\n",
    "    \n",
    "    past_kv_caches = None\n",
    "    \n",
    "    for step in range(max_new_tokens):\n",
    "        if step == 0:\n",
    "            input_ids = prompt_ids\n",
    "        else:\n",
    "            input_ids = generated[:, -1:] \n",
    "        \n",
    "        logits, past_kv_caches = model(\n",
    "            input_ids, \n",
    "            past_kv_caches=past_kv_caches,\n",
    "            use_cache=True\n",
    "        )\n",
    "        \n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58275950",
   "metadata": {},
   "source": [
    "### Multi-Layer & Batched KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4802944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24a14ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    def __init__(self, key_cache: torch.Tensor, val_cache: torch.Tensor):\n",
    "        assert key_cache.shape == value_cache.shape, \\\n",
    "            f\"K and V cache shapes must match: {key_cache.shape} vs {value_cache.shape}\"\n",
    "        \n",
    "        self.key_cache = key_cache\n",
    "        self.value_cache = value_cache\n",
    "\n",
    "    @property\n",
    "    def seq_len(self) -> int:\n",
    "        \"\"\"Current sequence length in cache\"\"\"\n",
    "        return self.key_cache.shape[2]\n",
    "    \n",
    "    @property\n",
    "    def batch_size(self) -> int:\n",
    "        return self.key_cache.shape[0]\n",
    "    \n",
    "    @property\n",
    "    def num_heads(self) -> int:\n",
    "        return self.key_cache.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def head_dim(self) -> int:\n",
    "        return self.key_cache.shape[3]\n",
    "\n",
    "    def update(self, new_key: torch.Tensor, new_val: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Append new K,V to cache.\n",
    "        \n",
    "        Args:\n",
    "            new_key: [batch, num_heads, new_len, head_dim]\n",
    "            new_value: [batch, num_heads, new_len, head_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Updated KVCache (new object)\n",
    "        \"\"\"\n",
    "        self.key_cache = torch.cat([self.key_cache, new_key], dim=2)\n",
    "        self.val_cache = torch.cat([self.val_cache, val_key], dim=2)\n",
    "\n",
    "        return self.key_cache, self.val_cache\n",
    "\n",
    "    def get(self):\n",
    "        return self.key_cache, self.val_cache\n",
    "\n",
    "    def get_memory_usage(self) -> dict:\n",
    "        \"\"\"Calculate memory usage in MB\"\"\"\n",
    "        k_bytes = self.key_cache.numel() * self.key_cache.element_size()\n",
    "        v_bytes = self.value_cache.numel() * self.value_cache.element_size()\n",
    "        \n",
    "        return {\n",
    "            'key_mb': k_bytes / (1024 ** 2),\n",
    "            'value_mb': v_bytes / (1024 ** 2),\n",
    "            'total_mb': (k_bytes + v_bytes) / (1024 ** 2)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a075763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerKVCache:\n",
    "    def __init__(self, num_layers: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_layers: number of transformer layers\n",
    "        \"\"\"\n",
    "        self.num_layers = num_layers\n",
    "        self.caches = [None] * num_layers\n",
    "\n",
    "    def update(self, layer_idx: int, new_key: torch.Tensor, new_val: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Update cache for a specific layer.\n",
    "        \n",
    "        Args:\n",
    "            layer_idx: which layer to update (0 to num_layers-1)\n",
    "            new_key: [batch, num_heads, seq_len, head_dim]\n",
    "            new_value: [batch, num_heads, seq_len, head_dim]\n",
    "        \"\"\"\n",
    "        if self.caches[layer_idx] is None:\n",
    "            self.caches[layer_idx] = KVCache(new_key, new_cache)\n",
    "        else:\n",
    "            new_key, new_val = self.caches[layer_idx].update(new_key, new_val)\n",
    "\n",
    "        return new_key, new_val\n",
    "\n",
    "    def get(self, layer_idx: int):\n",
    "        \"\"\"\n",
    "        Get cache for a specific layer.\n",
    "        \n",
    "        Args:\n",
    "            layer_idx: which layer's cache to retrieve\n",
    "        \n",
    "        Returns:\n",
    "            KVCache object or None if not initialized\n",
    "        \"\"\"\n",
    "        return self.caches[layer_idx]\n",
    "\n",
    "    def get_seq_len(self) -> int:\n",
    "        \"\"\"\n",
    "        Get current sequence length (same across all layers).\n",
    "        \n",
    "        Returns:\n",
    "            sequence length, or 0 if no cache exists\n",
    "        \"\"\"\n",
    "        if self.caches[0] is None:\n",
    "            return 0\n",
    "        return self.caches[0].seq_len\n",
    "    \n",
    "    def get_total_memory_usage(self) -> dict:\n",
    "        \"\"\"Calculate total memory across all layers\"\"\"\n",
    "        total_mb = 0\n",
    "        num_active_layers = 0\n",
    "        \n",
    "        for cache in self.caches:\n",
    "            if cache is not None:\n",
    "                mem = cache.get_memory_usage()\n",
    "                total_mb += mem['total_mb']\n",
    "                num_active_layers += 1\n",
    "        \n",
    "        avg_per_layer = total_mb / num_active_layers if num_active_layers > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'total_mb': total_mb,\n",
    "            'total_gb': total_mb / 1024,\n",
    "            'per_layer_mb': avg_per_layer,\n",
    "            'num_active_layers': num_active_layers\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcd043d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBatched(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention with proper batching support.\n",
    "    Handles variable-length sequences with padding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, d_model]\n",
    "            attention_mask: [batch, 1, seq_len, total_len] \n",
    "                - 1 for positions to attend to, 0 for masked positions\n",
    "                - total_len = past_len + seq_len\n",
    "            kv_cache: KVCache object or None\n",
    "            use_cache: whether to return updated cache\n",
    "            \n",
    "        Returns:\n",
    "            output: [batch, seq_len, d_model]\n",
    "            new_cache: KVCache or None\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        if kv_cache is not None:\n",
    "            K, V = kv_cache.update(K, V)\n",
    "        \n",
    "        scores = Q @ K.transpose(-2, -1) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_output = attn_weights @ V\n",
    "        \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "        output = self.W_o(attn_output)\n",
    "\n",
    "        new_cache = None\n",
    "        if not kv_cache:\n",
    "            new_cache = KVCache(K, V)\n",
    "        \n",
    "        return output, new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867827a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(self, seq_len, past_len):\n",
    "    \"\"\"\n",
    "    Create causal attention mask for autoregressive generation.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: length of current sequence\n",
    "        past_len: length of cached sequence\n",
    "        device: torch device\n",
    "        \n",
    "    Returns:\n",
    "        mask: [1, 1, seq_len, past_len + seq_len]\n",
    "              1 = attend, 0 = mask\n",
    "    \"\"\"\n",
    "    total_len = past_len + seq_len\n",
    "\n",
    "    mask = torch.ones(seq_len, total_len)\n",
    "\n",
    "    if seq_len > 1:\n",
    "        casual_mask = torch.tril(\n",
    "            torch.ones(seq_len, seq_len)\n",
    "        )\n",
    "        mask[:,past_len:] = causal_mask\n",
    "\n",
    "    return mask.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3b31ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(input_length: torch.Tensor, max_len: int, past_len: int = 0):\n",
    "    \"\"\"\n",
    "    Create padding mask for batched sequences with different lengths.\n",
    "    \n",
    "    Args:\n",
    "        input_lengths: [batch] - actual length of each sequence (without padding)\n",
    "        max_len: maximum sequence length in batch (without padding)\n",
    "        past_len: length of cached sequence\n",
    "        \n",
    "    Returns:\n",
    "        mask: [batch, 1, 1, past_len + max_len]\n",
    "              1 = attend (real token), 0 = mask (padding)\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = input_length.shape[0]\n",
    "    total_length = past_len + max_len\n",
    "\n",
    "    positions = torch.arange(total_length).unsqueenze(0).expand(batch_size, -1)\n",
    "\n",
    "    # left padding\n",
    "    pad_amounts = total_length - (input_length + past_len)\n",
    "    mask = positions >= pad_amounts.unsqueeze(1)\n",
    "\n",
    "    return mask.unsqueeze(1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "363161d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_masks(causal_mask: torch.Tensor, padding_mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Combine causal and padding masks.\n",
    "    Both must be 1 for attention, 0 for masking.\n",
    "    \"\"\"\n",
    "    # Broadcasting: causal_mask [1, 1, seq_len, total_len]\n",
    "    #               padding_mask [batch, 1, 1, total_len]\n",
    "    # Result: [batch, 1, seq_len, total_len]\n",
    "    return causal_mask & padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fa17b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayerWithCache(nn.Module):\n",
    "    \"\"\"Transformer layer using new cache structure\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttentionBatched(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "        use_cache: bool = False\n",
    "    ):\n",
    "        # Self-attention\n",
    "        attn_out, new_cache = self.self_attn(\n",
    "            self.norm1(x),\n",
    "            attention_mask=attention_mask,\n",
    "            kv_cache=kv_cache,\n",
    "            use_cache=use_cache\n",
    "        )\n",
    "        x = x + self.dropout1(attn_out)\n",
    "        \n",
    "        # FFN\n",
    "        x = x + self.dropout2(self.ffn(self.norm2(x)))\n",
    "        \n",
    "        return x, new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47bde70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithCache(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer with proper cache management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayerWithCache(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[MultiLayerKVCache] = None,\n",
    "        use_cache: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: [batch, seq_len]\n",
    "            attention_mask: [batch, 1, seq_len, total_len] or None\n",
    "            kv_cache: MultiLayerKVCache or None\n",
    "            use_cache: bool\n",
    "            \n",
    "        Returns:\n",
    "            logits: [batch, seq_len, vocab_size]\n",
    "            new_cache: MultiLayerKVCache or None\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Get past length from cache\n",
    "        past_len = kv_cache.get_seq_len() if kv_cache is not None else 0\n",
    "        \n",
    "        # Position IDs\n",
    "        position_ids = torch.arange(\n",
    "            past_len, past_len + seq_len,\n",
    "            device=input_ids.device\n",
    "        ).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Embeddings\n",
    "        x = self.token_embedding(input_ids) + self.pos_embedding(position_ids)\n",
    "        \n",
    "        # Initialize new cache if needed\n",
    "        new_cache = MultiLayerKVCache(self.num_layers) if use_cache else None\n",
    "        \n",
    "        # Pass through layers\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            layer_cache = kv_cache.get(layer_idx) if kv_cache is not None else None\n",
    "            \n",
    "            x, layer_new_cache = layer(\n",
    "                x,\n",
    "                attention_mask=attention_mask,\n",
    "                kv_cache=layer_cache,\n",
    "                use_cache=use_cache\n",
    "            )\n",
    "            \n",
    "            if use_cache:\n",
    "                new_cache.caches[layer_idx] = layer_new_cache\n",
    "        \n",
    "        # Output\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits, new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_batched(\n",
    "    model: TransformerWithCache,\n",
    "    prompt_ids_list: List[torch.Tensor],\n",
    "    max_new_tokens: int,\n",
    "    temperature: float = 1.0,\n",
    "    pad_token_id: int = 0\n",
    ") -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Batched generation with variable-length prompts.\n",
    "    \n",
    "    Args:\n",
    "        model: TransformerWithCache\n",
    "        prompt_ids_list: list of [1, prompt_len] tensors (different lengths OK!)\n",
    "        max_new_tokens: how many tokens to generate\n",
    "        temperature: sampling temperature\n",
    "        pad_token_id: ID for padding token\n",
    "        \n",
    "    Returns:\n",
    "        list of generated sequences [1, total_len]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    batch_size = len(prompt_ids_list)\n",
    "    \n",
    "    prompt_lengths = torch.tensor([p.shape[1] for p in prompt_ids_list], device=device)\n",
    "    max_prompt_len = prompt_lengths.max().item()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Batched Generation: {batch_size} sequences\")\n",
    "    print(f\"Prompt lengths: {prompt_lengths.tolist()}\")\n",
    "    print(f\"Max prompt: {max_prompt_len}, Generating: {max_new_tokens}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    padded_prompts = []\n",
    "    for prompt in prompt_ids_list:\n",
    "        prompt = prompt.to(device)\n",
    "        pad_len = max_prompt_len - prompt.shape[1]\n",
    "        if pad_len > 0:\n",
    "            padding = torch.full((1, pad_len), pad_token_id, device=device, dtype=prompt.dtype)\n",
    "            prompt = torch.cat([padding, prompt], dim=1)\n",
    "        padded_prompts.append(prompt)\n",
    "    \n",
    "    input_ids = torch.cat(padded_prompts, dim=0)\n",
    "    \n",
    "    finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "    \n",
    "    kv_cache = None\n",
    "    \n",
    "    all_generated = input_ids.clone()\n",
    "    \n",
    "    for step in range(max_new_tokens):\n",
    "        past_len = kv_cache.get_seq_len() if kv_cache is not None else 0\n",
    "        \n",
    "        if step == 0:\n",
    "            current_input = input_ids\n",
    "            seq_len = max_prompt_len\n",
    "            \n",
    "            padding_mask = create_padding_mask(prompt_lengths, max_prompt_len, past_len=0)\n",
    "            causal_mask = create_causal_mask(seq_len, past_len=0, device=device)\n",
    "            attention_mask = combine_masks(causal_mask, padding_mask)\n",
    "            \n",
    "            print(f\"PREFILL: Processing {seq_len} tokens (batch={batch_size})\")\n",
    "        else:\n",
    "            current_input = all_generated[:, -1:]\n",
    "            seq_len = 1\n",
    "            \n",
    "            total_lengths = prompt_lengths + step \n",
    "            padding_mask = create_padding_mask(prompt_lengths, seq_len, past_len=past_len)\n",
    "            causal_mask = create_causal_mask(seq_len, past_len=past_len, device=device)\n",
    "            attention_mask = combine_masks(causal_mask, padding_mask)\n",
    "            \n",
    "            print(f\" DECODE {step}: cache_len={past_len}\")\n",
    "        \n",
    "        logits, kv_cache = model(\n",
    "            current_input,\n",
    "            attention_mask=attention_mask,\n",
    "            kv_cache=kv_cache,\n",
    "            use_cache=True\n",
    "        )\n",
    "        \n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        next_tokens = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        next_tokens = torch.where(\n",
    "            finished.unsqueeze(1),\n",
    "            torch.tensor(pad_token_id, device=device),\n",
    "            next_tokens\n",
    "        )\n",
    "        \n",
    "        all_generated = torch.cat([all_generated, next_tokens], dim=1)\n",
    "        \n",
    "    print(f\"\\n Generation complete!\")\n",
    "    print(f\"Final cache: {kv_cache}\")\n",
    "    \n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a457d",
   "metadata": {},
   "source": [
    "#### Memory-Efficient Cache Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0825d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowCache(MultiLayerKVCache):\n",
    "    \"\"\"\n",
    "    KV Cache with sliding window - keeps only last N tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers: int, window_size: int, num_sink_tokens: int = 4):\n",
    "        super().__init__(num_layers)\n",
    "        self.window_size = window_size\n",
    "        self.num_sink_tokens = num_sink_tokens  # Keep first N tokens (attention sinks)\n",
    "    \n",
    "    def update(self, layer_idx: int, new_key: torch.Tensor, new_value: torch.Tensor):\n",
    "        \"\"\"Update with sliding window logic\"\"\"\n",
    "        if self.caches[layer_idx] is None:\n",
    "            # First update\n",
    "            self.caches[layer_idx] = KVCache(key_cache=new_key, value_cache=new_value)\n",
    "        else:\n",
    "            # Concatenate new K,V\n",
    "            cache = self.caches[layer_idx]\n",
    "            updated_k = torch.cat([cache.key_cache, new_key], dim=2)\n",
    "            updated_v = torch.cat([cache.value_cache, new_value], dim=2)\n",
    "            \n",
    "            # Apply sliding window if exceeded\n",
    "            current_len = updated_k.shape[2]\n",
    "            if current_len > self.window_size:\n",
    "                # Keep: [first num_sink_tokens] + [last (window_size - num_sink_tokens)]\n",
    "                keep_len = self.window_size - self.num_sink_tokens\n",
    "                \n",
    "                sink_k = updated_k[:, :, :self.num_sink_tokens, :]\n",
    "                sink_v = updated_v[:, :, :self.num_sink_tokens, :]\n",
    "                \n",
    "                window_k = updated_k[:, :, -keep_len:, :]\n",
    "                window_v = updated_v[:, :, -keep_len:, :]\n",
    "                \n",
    "                updated_k = torch.cat([sink_k, window_k], dim=2)\n",
    "                updated_v = torch.cat([sink_v, window_v], dim=2)\n",
    "            \n",
    "            self.caches[layer_idx] = KVCache(key_cache=updated_k, value_cache=updated_v)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        base_repr = super().__repr__()\n",
    "        return f\"SlidingWindow{base_repr}, window={self.window_size}, sinks={self.num_sink_tokens}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2108fe9c",
   "metadata": {},
   "source": [
    "#### KV Cache Quantization (INT8/FP8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67fb4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedKVCache:\n",
    "    \"\"\"\n",
    "    INT8 quantized KV cache for memory efficiency.\n",
    "    \n",
    "    Stores K,V in INT8, maintains FP32 scales for dequantization.\n",
    "    \"\"\"\n",
    "    def __init__(self, key_cache_fp: torch.Tensor, val_cache_fp: torch.Tensor, quantize: bool = True):\n",
    "        \"\"\" Args:\n",
    "            key_cache_fp: [batch, num_heads, seq_len, head_dim] in FP32\n",
    "            value_cache_fp: [batch, num_heads, seq_len, head_dim] in FP32\n",
    "            quantize: whether to quantize (False for testing)\n",
    "        \"\"\"\n",
    "\n",
    "        self.shape = key_cache_fp.shape\n",
    "        self.dtype_original = key_cache_fp.device\n",
    "\n",
    "        if quantize:\n",
    "            self.key_cache, self.key_scale, self.key_zero_point = self._quantize_to_int8(key_cache_fp)\n",
    "            self.val_cache, self.val_scale, self.val_zero_point = self._quantize_to_int8(val_cache_fp)\n",
    "        else:\n",
    "            self.key_cache = key_cache_fp\n",
    "            self.val_cache = val_cache_fp\n",
    "            self.key_scale, self.val_scale = None, None\n",
    "            self.key_zero_point, self.val_zero_point = None, None\n",
    "\n",
    "        self.is_quantized = quantize\n",
    "\n",
    "    def _quantize_to_int8(self, tensor):\n",
    "        q_max = 2 ** (8 - 1) - 1\n",
    "        q_min = - q_max\n",
    "\n",
    "        max_val = tensor.abs().max()\n",
    "        min_val = tensor.abs().min()\n",
    "\n",
    "        scale = (max_val - min_val) / (q_max - q_min)\n",
    "        if scale == 0.0:\n",
    "            scale = 1.0\n",
    "\n",
    "        zero_point = q_min - torch.round(min_val / scale)\n",
    "\n",
    "        quantized = torch.clamp(\n",
    "            torch.round(tensor / scale) + zero_point, \n",
    "            q_min, q_max\n",
    "        ).to(torch.int8)\n",
    "\n",
    "        return quantized, scale, zero_point\n",
    "\n",
    "    def _dequantize_from_int8(self, quantized, scale, zero_point):\n",
    "        dq = (quantized - zero_point) * scale\n",
    "        return dq.to(torch.float32)\n",
    "\n",
    "    def update(self, new_key: torch.Tensor, new_val: torch.Tensor):\n",
    "        if self.is_quantized:\n",
    "            q_key, q_val = self._quantize_to_int8(new_key), self._quantize_to_int8(new_val)\n",
    "        else:\n",
    "            q_key, q_val = new_key, new_val\n",
    "\n",
    "        self.key_cache = torch.cat([self.key_cache, q_key], dim=2)\n",
    "        self.val_cache = torch.cat([self.val_cache, v_key], dim=2)\n",
    "\n",
    "    \n",
    "    def get_kv_fp(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get K,V in original floating point precision.\n",
    "        \n",
    "        Returns:\n",
    "            K: [batch, num_heads, seq_len, head_dim]\n",
    "            V: [batch, num_heads, seq_len, head_dim]\n",
    "        \"\"\"\n",
    "        if self.is_quantized:\n",
    "            K = self._dequantize_from_int8(self.key_cache, self.key_scale, self.key_zero_point)\n",
    "            V = self._dequantize_from_int8(self.val_cache, self.val_scale, self.val_zero_point)\n",
    "        else:\n",
    "            K = self.key_cache\n",
    "            V = self.val_cache\n",
    "        \n",
    "        return K, V\n",
    "\n",
    "    def get_memory_usage(self) -> dict:\n",
    "        \"\"\"Calculate memory usage\"\"\"\n",
    "        if self.is_quantized:\n",
    "            # INT8 cache\n",
    "            k_bytes = self.key_cache.numel() * 1  # 1 byte per INT8\n",
    "            v_bytes = self.val_cache.numel() * 1\n",
    "            scale_bytes = self.key_scale.numel() * 4 + self.val_scale.numel() * 4  # FP32\n",
    "        else:\n",
    "            # Original precision\n",
    "            k_bytes = self.key_cache.numel() * self.key_cache.element_size()\n",
    "            v_bytes = self.val_cache.numel() * self.val_cache.element_size()\n",
    "            scale_bytes = 0\n",
    "        \n",
    "        total_mb = (k_bytes + v_bytes + scale_bytes) / (1024 ** 2)\n",
    "        \n",
    "        return {\n",
    "            'key_mb': k_bytes / (1024 ** 2),\n",
    "            'value_mb': v_bytes / (1024 ** 2),\n",
    "            'scale_mb': scale_bytes / (1024 ** 2),\n",
    "            'total_mb': total_mb\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def seq_len(self) -> int:\n",
    "        return self.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "571fc2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Testing KV Cache Quantization (INT8)\n",
      "======================================================================\n",
      "\n",
      "FP16 Cache: 32.00 MB\n",
      "INT8 Cache: 16.00 MB\n",
      "Memory Savings: 2.00x\n",
      "\n",
      "Quantization Error:\n",
      "  K mean absolute error: 0.400711\n",
      "  V mean absolute error: 0.400150\n",
      "  K relative error: 0.5102 (51.02%)\n",
      "  V relative error: 0.5100 (51.00%)\n",
      "\n",
      "ðŸ’¡ Typical perplexity impact: <1% increase\n",
      "   Memory savings enable 2x larger batch size!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_kv_quantization():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Testing KV Cache Quantization (INT8)\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    batch, num_heads, seq_len, head_dim = 1, 32, 2048, 128\n",
    "    \n",
    "    # Create random K,V in FP16\n",
    "    K_fp16 = torch.randn(batch, num_heads, seq_len, head_dim, dtype=torch.float16)\n",
    "    V_fp16 = torch.randn(batch, num_heads, seq_len, head_dim, dtype=torch.float16)\n",
    "    \n",
    "    # Create caches\n",
    "    cache_fp16 = QuantizedKVCache(K_fp16, V_fp16, quantize=False)\n",
    "    cache_int8 = QuantizedKVCache(K_fp16, V_fp16, quantize=True)\n",
    "    \n",
    "    # Compare memory\n",
    "    mem_fp16 = cache_fp16.get_memory_usage()\n",
    "    mem_int8 = cache_int8.get_memory_usage()\n",
    "    \n",
    "    print(f\"FP16 Cache: {mem_fp16['total_mb']:.2f} MB\")\n",
    "    print(f\"INT8 Cache: {mem_int8['total_mb']:.2f} MB\")\n",
    "    print(f\"Memory Savings: {mem_fp16['total_mb'] / mem_int8['total_mb']:.2f}x\\n\")\n",
    "    \n",
    "    # Check quality: dequantize and compare\n",
    "    K_dequant, V_dequant = cache_int8.get_kv_fp()\n",
    "    \n",
    "    k_error = (K_fp16 - K_dequant).abs().mean()\n",
    "    v_error = (V_fp16 - V_dequant).abs().mean()\n",
    "    \n",
    "    print(f\"Quantization Error:\")\n",
    "    print(f\"  K mean absolute error: {k_error:.6f}\")\n",
    "    print(f\"  V mean absolute error: {v_error:.6f}\")\n",
    "    \n",
    "    # Relative error\n",
    "    k_rel_error = ((K_fp16 - K_dequant).abs() / (K_fp16.abs() + 1e-6)).mean()\n",
    "    v_rel_error = ((V_fp16 - V_dequant).abs() / (V_fp16.abs() + 1e-6)).mean()\n",
    "    \n",
    "    print(f\"  K relative error: {k_rel_error:.4f} ({k_rel_error*100:.2f}%)\")\n",
    "    print(f\"  V relative error: {v_rel_error:.4f} ({v_rel_error*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Typical perplexity impact: <1% increase\")\n",
    "    print(f\"   Memory savings enable 2x larger batch size!\\n\")\n",
    "\n",
    "test_kv_quantization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a640f",
   "metadata": {},
   "source": [
    "### PagedAttention Concepts (vLLM)\n",
    "\n",
    "PagedAttention Key Ideas:\n",
    "\n",
    "1. **Problem with Continuous Cache:**\n",
    "   - Traditional: KV cache is contiguous tensor\n",
    "   - Issue: Memory fragmentation, can't easily share cache between sequences\n",
    "   \n",
    "2. **PagedAttention Solution:**\n",
    "   - Split KV cache into fixed-size \"pages\" (blocks)\n",
    "   - Like virtual memory in OS!\n",
    "   - Pages can be non-contiguous in physical memory\n",
    "   \n",
    "3. **Benefits:**\n",
    "   - Near-zero memory fragmentation\n",
    "   - Easy cache sharing (beam search, parallel sampling)\n",
    "   - Dynamic memory allocation\n",
    "\n",
    "Visual: \n",
    "1. Traditional:\n",
    "- [Seq1: KKKKKK...] [Seq2: KKKKKK...] [Wasted Space] [Seq3: KKK...]\n",
    "\n",
    "2. PagedAttention:\n",
    "- Physical Memory: [Block0][Block1][Block2][Block3][Block4][Block5]\n",
    "- Seq1 mapping: Block0 â†’ Block2 â†’ Block4\n",
    "- Seq2 mapping: Block1 â†’ Block3\n",
    "- Seq3 mapping: Block5\n",
    "(No fragmentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74c8d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagedKVCache:\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_blocks: int,\n",
    "        block_size: int,\n",
    "        num_heads: int,\n",
    "        head_dim: int,\n",
    "        dtype: torch.float32\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_blocks: Total number of blocks in memory pool\n",
    "            block_size: Number of tokens per block\n",
    "            num_heads: Number of KV heads\n",
    "            head_dim: Dimension per head\n",
    "        \"\"\"\n",
    "        self.num_blocks = num_blocks\n",
    "        self.block_size = block_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        \n",
    "        self.blocks = torch.zeros(\n",
    "            num_blocks, 2, block_size, num_heads, head_dim, dtype=dtype\n",
    "        )\n",
    "\n",
    "        self.free_blocks = set(range(num_blocks))\n",
    "\n",
    "        self.seq_to_blocks = {}\n",
    "        self.seq_lengths = {}\n",
    "\n",
    "    def allocate_blocks(self, seq_id: int, num_blocks_needed: int) -> bool:\n",
    "        \"\"\"\n",
    "        Allocate blocks for a sequence.\n",
    "        \n",
    "        Returns:\n",
    "            True if successful, False if not enough free blocks\n",
    "        \"\"\"\n",
    "        if len(self.free_blocks) < num_blocks_needed:\n",
    "            return False\n",
    "\n",
    "        allocated = []\n",
    "        for _ in range(num_blocks_needed):\n",
    "            block_id = self.free_blocks.pop()\n",
    "            allocated.append(block_id)\n",
    "\n",
    "        self.seq_to_blocks[seq_id] = allocated\n",
    "        self.seq_lengths[seq_id] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def append_tokens(self, seq_id: int, new_k: torch.Tensor, new_v: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Append new K,V to sequence's cache.\n",
    "        \n",
    "        Args:\n",
    "            seq_id: Sequence ID\n",
    "            new_k: [num_tokens, num_heads, head_dim]\n",
    "            new_v: [num_tokens, num_heads, head_dim]\n",
    "        \n",
    "        Returns:\n",
    "            True if successful, False if need more blocks\n",
    "        \"\"\"\n",
    "        if seq_id not in self.seq_to_blocks or seq_id not in self.seq_lengths:\n",
    "            return True\n",
    "\n",
    "        num_new_tokens = new_k.shape[0]\n",
    "        current_len = self.seq_lengths[seq_id]\n",
    "        new_len = current_len + num_new_tokens\n",
    "\n",
    "        blocks_needed = (new_len + self.block_size - 1) // self.block_size\n",
    "        blocks_allocated = len(self.seq_to_blocks[seq_id])\n",
    "\n",
    "        if blocks_needed > blocks_allocated:\n",
    "            additional_blocks = blocks_needed - blocks_allocated\n",
    "            if len(self.free_blocks) < additional_blocks:\n",
    "                return True\n",
    "\n",
    "            for _ in range(additional_blocks):\n",
    "                block_id = self.free_blocks.pop()\n",
    "                self.seq_to_blocks[seq_id].append(block_id)\n",
    "\n",
    "        \n",
    "        for i, (k_token, v_token) in enumerate(zip(new_k, new_v)):\n",
    "            token_idx = current_len + i\n",
    "            block_idx = token_idx // self.block_size\n",
    "            offset = token_idx % self.block_size\n",
    "\n",
    "            block_id = self.seq_to_blocks[seq_id][block_idx]\n",
    "\n",
    "            self.blocks[block_id, 0, offset] = k_token\n",
    "            self.blocks[block_id, 1, offset] = v_token\n",
    "\n",
    "        self.seq_lengths[seq_id] = new_len\n",
    "        return True\n",
    "\n",
    "    def get_kv(self, seq_id: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Gather K,V for a sequence from its blocks.\n",
    "        \n",
    "        Returns:\n",
    "            K: [seq_len, num_heads, head_dim]\n",
    "            V: [seq_len, num_heads, head_dim]\n",
    "        \"\"\"\n",
    "        if seq_id not in self.seq_to_blocks:\n",
    "            raise ValueError(f\"Sequence {seq_id} not found\")\n",
    "\n",
    "        seq_len = self.seq_lengths[seq_id]\n",
    "        block_ids = self.seq_to_blocks[seq_id]\n",
    "\n",
    "        K_list = []\n",
    "        V_list = []\n",
    "\n",
    "        for block_idx, block_id in enumerate(blocks_ids):\n",
    "            start_token = block_idx * self.block_size\n",
    "            end_token = min(start_token + self.block_size, seq_len)\n",
    "\n",
    "            num_tokens = end_token - start_token\n",
    "\n",
    "            if num_tokens > 0:\n",
    "                K_list.append(self.blocks[block_id, 0, :num_tokens])\n",
    "                V_list.append(self.blocks[block_id, 1, :num_tokens])\n",
    "\n",
    "        K = torch.cat(K_list, dim=0)\n",
    "        V = torch.cat(V_list, dim=0)\n",
    "\n",
    "        return K, V\n",
    "\n",
    "    def free_sequence(self, seq_id: int):\n",
    "        \"\"\"Free all blocks associated with a sequence\"\"\"\n",
    "        if seq_id in self.seq_to_blocks:\n",
    "            for block_id in self.seq_to_blocks[seq_id]:\n",
    "                self.free_blocks.add(block_id)\n",
    "            \n",
    "            del self.seq_to_blocks[seq_id]\n",
    "            del self.seq_lengths[seq_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd80b5",
   "metadata": {},
   "source": [
    "#### Grouped Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12848ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped-Query Attention (GQA) - used in LLaMA-2, Mistral, etc.\n",
    "    \n",
    "    Reduces KV cache by sharing K,V across groups of query heads.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_query_heads: int,\n",
    "        num_kv_heads: int,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_query_heads == 0, \"d_model must be divisible by num_query_heads\"\n",
    "        assert num_query_heads % num_kv_heads == 0, \"num_query_heads must be divisible by num_kv_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_query_heads = num_query_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = d_model // num_query_heads\n",
    "        \n",
    "        # Number of query heads per KV head\n",
    "        self.num_queries_per_kv = num_query_heads // num_kv_heads\n",
    "        \n",
    "        # Projections\n",
    "        self.W_q = nn.Linear(d_model, num_query_heads * self.head_dim, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, num_kv_heads * self.head_dim, bias=False)  # Smaller!\n",
    "        self.W_v = nn.Linear(d_model, num_kv_heads * self.head_dim, bias=False)  # Smaller!\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, d_model]\n",
    "            attention_mask: [batch, 1, seq_len, total_len]\n",
    "            kv_cache: (K, V) where K,V are [batch, num_kv_heads, past_len, head_dim]\n",
    "            use_cache: bool\n",
    "            \n",
    "        Returns:\n",
    "            output: [batch, seq_len, d_model]\n",
    "            new_cache: (K, V) or None\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x)  # [batch, seq_len, num_query_heads * head_dim]\n",
    "        K = self.W_k(x)  # [batch, seq_len, num_kv_heads * head_dim]\n",
    "        V = self.W_v(x)  # [batch, seq_len, num_kv_heads * head_dim]\n",
    "        \n",
    "        # Reshape Q: [batch, num_query_heads, seq_len, head_dim]\n",
    "        Q = Q.view(batch_size, seq_len, self.num_query_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Reshape K,V: [batch, num_kv_heads, seq_len, head_dim]\n",
    "        K = K.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Handle cache\n",
    "        if kv_cache is not None:\n",
    "            K_cache, V_cache = kv_cache\n",
    "            K = torch.cat([K_cache, K], dim=2)\n",
    "            V = torch.cat([V_cache, V], dim=2)\n",
    "        \n",
    "        # CRITICAL: Repeat K,V to match number of query heads\n",
    "        # Each KV head is shared across num_queries_per_kv query heads\n",
    "        # [batch, num_kv_heads, seq_len, head_dim] â†’ [batch, num_query_heads, seq_len, head_dim]\n",
    "        K_repeated = K.repeat_interleave(self.num_queries_per_kv, dim=1)\n",
    "        V_repeated = V.repeat_interleave(self.num_queries_per_kv, dim=1)\n",
    "        \n",
    "        # Now K_repeated and V_repeated have same num_heads as Q\n",
    "        # Compute attention\n",
    "        scores = Q @ K_repeated.transpose(-2, -1) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn_output = attn_weights @ V_repeated\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        # Return cache (store original K,V, NOT repeated versions!)\n",
    "        new_cache = (K, V) if use_cache else None\n",
    "        \n",
    "        return output, new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7915534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Query Attention (MQA) - extreme version where num_kv_heads = 1.\n",
    "    Used in PaLM, StarCoder, Falcon.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_query_heads: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_query_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_query_heads = num_query_heads\n",
    "        self.head_dim = d_model // num_query_heads\n",
    "        \n",
    "        # Q has full heads, K,V have only 1 head\n",
    "        self.W_q = nn.Linear(d_model, num_query_heads * self.head_dim, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, self.head_dim, bias=False)  # Single head!\n",
    "        self.W_v = nn.Linear(d_model, self.head_dim, bias=False)  # Single head!\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            kv_cache: (K, V) where K,V are [batch, 1, past_len, head_dim]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Q: [batch, num_query_heads, seq_len, head_dim]\n",
    "        Q = Q.view(batch_size, seq_len, self.num_query_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # K,V: [batch, 1, seq_len, head_dim]\n",
    "        K = K.view(batch_size, seq_len, 1, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, 1, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Handle cache\n",
    "        if kv_cache is not None:\n",
    "            K_cache, V_cache = kv_cache\n",
    "            K = torch.cat([K_cache, K], dim=2)\n",
    "            V = torch.cat([V_cache, V], dim=2)\n",
    "        \n",
    "        # Repeat K,V to match Q heads\n",
    "        K_repeated = K.expand(-1, self.num_query_heads, -1, -1)\n",
    "        V_repeated = V.expand(-1, self.num_query_heads, -1, -1)\n",
    "        \n",
    "        # Attention\n",
    "        scores = Q @ K_repeated.transpose(-2, -1) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn_output = attn_weights @ V_repeated\n",
    "        \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        new_cache = (K, V) if use_cache else None\n",
    "        \n",
    "        return output, new_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a49c05",
   "metadata": {},
   "source": [
    "### Flash Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0676d9e0",
   "metadata": {},
   "source": [
    "Flash Attention Key Ideas:\n",
    "\n",
    "Standard Attention Problem:\n",
    "1. Compute scores: Q @ K^T (materialize NxN matrix)\n",
    "2. Softmax: softmax(scores) (materialize NxN matrix)\n",
    "3. Output: attn @ V\n",
    "\n",
    "Memory: O(N^2) - stores full attention matrix\n",
    "\n",
    "Flash Attention Solution:\n",
    "- Tile Q, K, V\n",
    "- Compute attention in blocks\n",
    "- Never materialize full NxN matrix\n",
    "- Memory: O(N) instead of O(N^2)\n",
    "\n",
    "With KV Cache:\n",
    "- Flash Attention + KV cache is THE standard for production\n",
    "- vLLM, TensorRT-LLM all use this combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a767a2",
   "metadata": {},
   "source": [
    "Flash Attention + KV Cache Best Practices:\n",
    "\n",
    "1. **Always use Flash Attention for long sequences (>512 tokens)**\n",
    "   - 2-4x speedup on attention computation\n",
    "   - Enables longer contexts (up to 100k+ tokens)\n",
    "\n",
    "2. **Combine with GQA for maximum efficiency**\n",
    "   - GQA reduces cache size\n",
    "   - Flash Attention reduces compute\n",
    "   - Together: massive throughput gains\n",
    "\n",
    "3. **Production Stack:**\n",
    "   - vLLM: PagedAttention + Flash Attention + GQA + INT8 cache\n",
    "   - TensorRT-LLM: Flash Attention + GQA + FP8 cache\n",
    "   - SGLang: RadixAttention (prefix caching) + Flash Attention\n",
    "\n",
    "4. **When to use what:**\n",
    "   - Batch=1, latency-critical: Flash Attention + FP16 cache\n",
    "   - High throughput serving: Flash + PagedAttention + INT8 cache\n",
    "   - Long context (>8k): Flash + GQA + sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a261399",
   "metadata": {},
   "source": [
    "#### Continuous Batching Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bbac920f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¡ Continuous Batching + KV Cache:\n",
      "\n",
      "Key Insight: PagedAttention makes continuous batching efficient!\n",
      "\n",
      "Without PagedAttention:\n",
      "- Hard to dynamically resize cache tensors\n",
      "- Memory fragmentation when swapping sequences\n",
      "- Copying overhead\n",
      "\n",
      "With PagedAttention:\n",
      "- Just update block mappings (O(1) operation)\n",
      "- No memory copying\n",
      "- Perfect for continuous batching\n",
      "\n",
      "This is why vLLM achieves 10-20x higher throughput than HuggingFace!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Continuous Batching (used in vLLM, TensorRT-LLM):\n",
    "\n",
    "Traditional Batching:\n",
    "- Wait for N requests\n",
    "- Process all together\n",
    "- All finish at different times â†’ wasted compute\n",
    "\n",
    "Continuous Batching:\n",
    "- As soon as one request finishes, add new one\n",
    "- Dynamically change batch composition\n",
    "- Maximum GPU utilization\n",
    "\n",
    "KV Cache Challenges:\n",
    "1. Variable sequence lengths in batch\n",
    "2. Sequences finishing at different times\n",
    "3. Need to efficiently add/remove from batch\n",
    "\n",
    "Solutions:\n",
    "1. PagedAttention: Easy to swap sequences in/out\n",
    "2. Padding + masking: Handle variable lengths\n",
    "3. Separate prefill and decode batches\n",
    "\"\"\"\n",
    "\n",
    "class ContinuousBatchManager:\n",
    "    \"\"\"\n",
    "    Manages continuous batching with KV cache.\n",
    "    Simplified version of what vLLM does.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, max_batch_size: int, paged_cache: PagedKVCache):\n",
    "        self.model = model\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.cache = paged_cache\n",
    "        \n",
    "        # Active sequences\n",
    "        self.active_sequences = {}  # seq_id -> sequence info\n",
    "        \n",
    "    def add_sequence(self, seq_id: int, prompt: torch.Tensor):\n",
    "        \"\"\"Add new sequence to batch\"\"\"\n",
    "        if len(self.active_sequences) >= self.max_batch_size:\n",
    "            return False  # Batch full\n",
    "        \n",
    "        # Allocate cache blocks\n",
    "        prompt_len = prompt.shape[0]\n",
    "        blocks_needed = (prompt_len + self.cache.block_size - 1) // self.cache.block_size\n",
    "        \n",
    "        if not self.cache.allocate_blocks(seq_id, blocks_needed):\n",
    "            return False  # Not enough cache memory\n",
    "        \n",
    "        self.active_sequences[seq_id] = {\n",
    "            'prompt': prompt,\n",
    "            'generated': [],\n",
    "            'finished': False\n",
    "        }\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def remove_sequence(self, seq_id: int):\n",
    "        \"\"\"Remove finished sequence from batch\"\"\"\n",
    "        if seq_id in self.active_sequences:\n",
    "            self.cache.free_sequence(seq_id)\n",
    "            del self.active_sequences[seq_id]\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        One generation step for all active sequences.\n",
    "        \n",
    "        In practice, this would:\n",
    "        1. Separate prefill vs decode\n",
    "        2. Batch compatible sequences together\n",
    "        3. Use PagedAttention to gather K,V\n",
    "        \"\"\"\n",
    "        # Pseudocode for continuous batching step\n",
    "        pass\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ Continuous Batching + KV Cache:\n",
    "\n",
    "Key Insight: PagedAttention makes continuous batching efficient!\n",
    "\n",
    "Without PagedAttention:\n",
    "- Hard to dynamically resize cache tensors\n",
    "- Memory fragmentation when swapping sequences\n",
    "- Copying overhead\n",
    "\n",
    "With PagedAttention:\n",
    "- Just update block mappings (O(1) operation)\n",
    "- No memory copying\n",
    "- Perfect for continuous batching\n",
    "\n",
    "This is why vLLM achieves 10-20x higher throughput than HuggingFace!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
