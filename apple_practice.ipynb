{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5f3f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65f3b0",
   "metadata": {},
   "source": [
    "#### Tensor Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a79c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelism(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.out_features_per_rank = out_features // world_size\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(in_features, self.out_features_per_rank)\n",
    "        )\n",
    "\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.zeros(self.out_features_per_rank)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.matmul(x, self.weight) + self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac98ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowParallelism(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.in_features_per_rank = in_features // world_size\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(self.in_features_per_rank, self.out_features)\n",
    "        )\n",
    "\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.zeros(self.out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.matmul(x, self.weight)\n",
    "        dist.all_reduce(out, op=dist.ReduceOp.SUM)\n",
    "\n",
    "        if self.rank == 0:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2cc1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelismAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, rank, world_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.head_dim = d_model // world_size\n",
    "        self.n_heads_per_rank = n_heads // world_size\n",
    "\n",
    "        self.qkv_proj = ColumnParallelism(\n",
    "            in_features=d_model,\n",
    "            out_features=d_model * 3,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        self.out_proj = RowParallelism(\n",
    "            in_features=d_model,\n",
    "            out_features=d_model,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv_proj(x).reshape(batch_size, seq_len, 3, self.n_heads_per_rank, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scores = q @ k.transpose(-2, -1) / (self.head_dim ** 0.5)\n",
    "        attn = nn.functional.softmax(scores, dim=-1)\n",
    "        attn_output = attn @ v\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().reshapse(batch_size, seq_len, self.n_heads_per_rank * self.head_dim)\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb4298db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelismMLP(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.fc1 = ColumnParallelism(\n",
    "            in_features=d_model,\n",
    "            out_features=d_ff,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        self.fc2 = RowParallelism(\n",
    "            in_features=d_ff,\n",
    "            out_features=d_model,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2230a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelismBlock(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.attention = TensorParallelismAttention(\n",
    "            n_heads=n_heads,\n",
    "            d_model=d_model,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        self.mlp = TensorParallelismMLP(\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = out + self.dropout1(self.attention(self.norm1(out)))\n",
    "        out = out + self.dropout2(self.mlp(self.norm2(out)))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8200e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8934e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelismFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias):\n",
    "        ctx.saved_for_backward = input, weight\n",
    "\n",
    "        out = torch.matmul(input, weight)\n",
    "        if bias is not None:\n",
    "            out += bias\n",
    "\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        input, weight = ctx.saved_for_backward\n",
    "\n",
    "        grad_weight = torch.matmul(\n",
    "            input.reshape(-1, input.shape[-1]).t(),\n",
    "            grad_input.reshape(-1, grad_input.shape[-1])\n",
    "        )\n",
    "\n",
    "        grad_bias = grad_input.sum(dim=list(range(len(input.shape) - 1)))\n",
    "        grad_output = torch.matmul(grad_input, weight.t())\n",
    "\n",
    "        dist.all_reduce(grad_output, op=dist.ReduceOp.SUM)\n",
    "\n",
    "        return grad_output, grad_weight, grad_bias, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5131c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelism(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.out_features_per_rank = out_features // world_size\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(in_features, self.out_features_per_rank)\n",
    "        )\n",
    "\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.zeros(self.out_features_per_rank)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return ColumnParallelismFunction.apply(x, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87d6e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowParallelismFunction(nn.Module):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias):\n",
    "        ctx.saved_for_backward = input, weight\n",
    "        \n",
    "        out = torch.matmul(input, weight)\n",
    "        if bias is not None:\n",
    "            out += bias\n",
    "\n",
    "        dist.all_reduce(out, op=dist.ReduceOp.SUM)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        input, weight = ctx.saved_for_backward\n",
    "\n",
    "        grad_weight = torch.matmul(\n",
    "            input.reshape(-1, input.shape[-1]).t(),\n",
    "            weight.reshape(-1, weight.shape[-1])\n",
    "        )\n",
    "\n",
    "        grad_bias = grad_input.sum(dim=list(range(len(grad_input.shape) - 1)))\n",
    "        grad_output = torch.matmul(grad_input, weight.t())\n",
    "\n",
    "        return grad_output, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b74d21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowParallelism(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.in_features_per_rank = in_features // world_size\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(self.in_features_per_rank, out_features)\n",
    "        )\n",
    "\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.zeros(self.out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return RowParallelismFunction(x, self.weight, self.bias, self.rank, self.world_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c291ae07",
   "metadata": {},
   "source": [
    "#### Mixed Precision Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07e06ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5c12ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_features),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_features, hidden_features),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_features, out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0173bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_without_amp():\n",
    "    in_features = os.environ['in_features']\n",
    "    out_features = os.environ['out_features']\n",
    "    hidden_features = os.environ['hidden_features']\n",
    "\n",
    "    X = torch.randn((1000, in_features))\n",
    "    y = torch.randint(0, out_features, (1000,))\n",
    "    dataloader = torch.utils.data.DataLoader(list(zip(X, y)), batch_size=32, shuffle=True)\n",
    "\n",
    "    model = Model(in_features, hidden_features, out_features)\n",
    "    fp32_master_weights = [p.to(torch.float32).detach() for p in model.parameters()]\n",
    "    model = model.half()\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(fp32_master_weights, lr=1e-3)\n",
    "\n",
    "    n_epochs = os.environ['n_epochs']\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for _, (batch, label) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "\n",
    "            batch = batch.half()\n",
    "            label = label.half()\n",
    "\n",
    "            output = model(batch)\n",
    "            loss = criterion(output)\n",
    "            scaled_loss = loss * 8192. \n",
    "            scaled_loss.backward()\n",
    "\n",
    "            for fp16_params, fp32_params in zip(model.parameters, fp32_master_weights):\n",
    "                if fp32_params.grad is None:\n",
    "                    fp32_params.grad = nn.Parameter(torch.empty_like(fp32_params))\n",
    "                fp32_params.grad.data.copy_(fp16_params.grad.data)\n",
    "\n",
    "            for fp32_params in fp32_master_weights:\n",
    "                fp32_params.grad.data = fp32_params.grad.data / 8192. \n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            for fp16_params, fp32_params in zip(model.parameters, fp32_master_weights):\n",
    "                fp16_params.data.copy_(fp32_params.data.half())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9ba0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9968686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_amp():\n",
    "    in_features = os.environ['in_features']\n",
    "    out_features = os.environ['out_features']\n",
    "    hidden_features = os.environ['hidden_features']\n",
    "\n",
    "    X = torch.randn((1000, in_features))\n",
    "    y = torch.randint(0, out_features, (1000,))\n",
    "    dataloader = torch.utils.data.DataLoader(list(zip(X, y)), batch_size=32, shuffle=True)\n",
    "\n",
    "    model = Model(in_features, hidden_features, out_features)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    n_epochs = os.environ['n_epochs']\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for _, (batch, label) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(device_type=torch.float16):\n",
    "                output = model(batch)\n",
    "                loss = criterion(output, label)\n",
    "\n",
    "            scaler.scale(loss)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931db513",
   "metadata": {},
   "source": [
    "#### NCCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "241bb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b522ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCCL:\n",
    "    def __init__(self, rank, world_size):\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        os.environ['MASTER_ADDR'] = 'localhost'\n",
    "        os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "        dist.init_process_group(\n",
    "            backend='nccl',\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        torch.cuda.set_device()\n",
    "\n",
    "    def all_reduce_ring(self, tensor: torch.tensor):\n",
    "        if self.world_size == 1:\n",
    "            return tensor\n",
    "        \n",
    "        send_rank = (self.rank + 1) % self.world_size\n",
    "        recv_rank = (self.rank - 1 + self.world_size) % self.world_size\n",
    "\n",
    "        chunks = torch.chunk(tensor, chunks=self.world_size)\n",
    "        recv_buffer = torch.zeros_like(chunks[0], dtype=torch.float32)\n",
    "\n",
    "        for step in range(self.world_size - 1):\n",
    "            send_chunk_idx = (self.rank + step) % self.world_size\n",
    "            recv_chunk_idx = (self.rank - step - 1 + self.world_size) % self.world_size\n",
    "\n",
    "            send_chunk = chunks[send_chunk_idx]\n",
    "            recv_chunk = chunks[recv_chunk_idx]\n",
    "\n",
    "            if recv_chunk.numel() != recv_buffer.numel():\n",
    "                recv_buffer = torch.zeros_like(recv_chunk, dtype=torch.float32)\n",
    "\n",
    "            send_handle = dist.isend(send_chunk, dst=send_rank)\n",
    "            recv_handle = dist.irecv(recv_buffer, src=recv_rank)\n",
    "\n",
    "            send_handle.wait()\n",
    "            recv_handle.wait()\n",
    "\n",
    "            recv_chunk.add_(recv_buffer)\n",
    "\n",
    "        for step in range(self.world_size - 1):\n",
    "            send_chunk_idx = (self.rank - step - 1 + self.world_size) % self.world_size\n",
    "            recv_chunk_idx = (self.rank + step) % self.world_size\n",
    "\n",
    "            send_chunk = chunks[send_chunk_idx]\n",
    "            recv_chunk = chunks[recv_chunk_idx]\n",
    "\n",
    "            if recv_chunk.numel() != recv_buffer.numel():\n",
    "                recv_buffer = torch.zeros_like(recv_chunk, dtype=torch.float32)\n",
    "\n",
    "            send_handle = dist.isend(send_chunk, dst=send_rank)\n",
    "            recv_handle = dist.irecv(recv_buffer, src=recv_rank)\n",
    "\n",
    "            send_handle.wait()\n",
    "            recv_handle.wait()\n",
    "\n",
    "            recv_chunk.copy_(recv_buffer)\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    def broadcast_mst(self, tensor: torch.tensor, root: int):\n",
    "        if self.world_size == 1:\n",
    "            return tensor\n",
    "        \n",
    "        left_child = 2 * self.rank + 1\n",
    "        right_child = 2 * self.rank + 2\n",
    "\n",
    "        if self.rank != root:\n",
    "            parent = (self.rank - 1) // 2\n",
    "            dist.recv(tensor, src=parent)\n",
    "\n",
    "        if left_child < self.world_size:\n",
    "            dist.send(tensor, dst=left_child)\n",
    "        if right_child < self.world_size:\n",
    "            dist.send(tensor, dst=right_child)\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    def scatter_mst(self, tensor: torch.tensor, root: int):\n",
    "        if self.world_size == 1:\n",
    "            return tensor\n",
    "        \n",
    "        def get_parent(rank):\n",
    "            return (rank - 1) // 2\n",
    "        \n",
    "        def get_children(rank):\n",
    "            left_child = 2 * rank + 1\n",
    "            right_child = 2 * rank + 2\n",
    "\n",
    "            children = []\n",
    "            if left_child < self.world_size:\n",
    "                children.append(left_child)\n",
    "            if right_child < self.world_size:\n",
    "                children.append(right_child)\n",
    "\n",
    "            return children\n",
    "        \n",
    "        def get_subtree(rank):\n",
    "            ranks = [rank]\n",
    "            children = get_children(rank)\n",
    "            for child in children:\n",
    "                ranks.extend(get_subtree(child))\n",
    "\n",
    "            return ranks\n",
    "        \n",
    "        chunks = torch.chunk(tensor, chunks=self.world_size)\n",
    "\n",
    "        if self.rank != root:\n",
    "            parent = get_parent(self.rank)\n",
    "            subtree_ranks = get_subtree(self.rank)\n",
    "            subtree_size = len(subtree_ranks)\n",
    "\n",
    "            chunks = torch.zeros(chunks[0].numel() * subtree_size, dtype=torch.float32)\n",
    "            dist.recv(chunks, src=parent)\n",
    "            chunks = torch.chunk(chunks, chunks=subtree_size)\n",
    "\n",
    "        my_chunk = chunks[self.rank]\n",
    "        children = get_children(self.rank)\n",
    "\n",
    "        for child in children:\n",
    "            subtree_ranks = get_subtree(child)\n",
    "            send_chunks = torch.cat([chunks[r] for r in subtree_ranks], dim=-1)\n",
    "            dist.send(send_chunks, dst=child)\n",
    "\n",
    "        return my_chunk\n",
    "\n",
    "    def reduce_tree(self, tensor: torch.tensor, root: int, op = dist.ReduceOp.SUM):\n",
    "        if self.world_size == 1:\n",
    "            return tensor\n",
    "        \n",
    "        def get_parent(rank):\n",
    "            return (rank - 1) // 2\n",
    "        \n",
    "        def get_children(rank):\n",
    "            left_child = 2 * rank + 1\n",
    "            right_child = 2 * rank + 2\n",
    "\n",
    "            children = []\n",
    "            if left_child < self.world_size:\n",
    "                children.append(left_child)\n",
    "            if right_child < self.world_size:\n",
    "                children.append(right_child)\n",
    "\n",
    "            return children\n",
    "        \n",
    "        parent = get_parent(self.rank)\n",
    "        children = get_children(self.rank)\n",
    "        recv_buffer = torch.zeros_like(tensor, dtype=torch.float32)\n",
    "\n",
    "        for child in children:\n",
    "            dist.recv(recv_buffer, src=child)\n",
    "            tensor.add_(recv_buffer)\n",
    "\n",
    "        if self.rank != root:\n",
    "            dist.send(tensor, dst=parent)\n",
    "            return None\n",
    "        else:\n",
    "            return tensor\n",
    "        \n",
    "    def gather_mst(self, tensor: torch.tensor, root: int):\n",
    "        if self.world_size == 1:\n",
    "            return tensor\n",
    "        \n",
    "        def get_parent(rank):\n",
    "            return (rank - 1) // 2\n",
    "        \n",
    "        def get_children(rank):\n",
    "            left_child = 2 * rank + 1\n",
    "            right_child = 2 * rank + 2\n",
    "\n",
    "            children = []\n",
    "            if left_child < self.world_size:\n",
    "                children.append(left_child)\n",
    "            if right_child < self.world_size:\n",
    "                children.append(right_child)\n",
    "\n",
    "            return children\n",
    "        \n",
    "        def get_subtree_size(rank):\n",
    "            size = 1\n",
    "            for child in get_children(rank):\n",
    "                size += get_subtree_size(child)\n",
    "\n",
    "            return size\n",
    "        \n",
    "        children = get_children(self.rank)\n",
    "        child_data = []\n",
    "        \n",
    "        for child in children:\n",
    "            subtree_size = get_subtree_size(child)\n",
    "            recv_buffer = torch.zeros_like(tensor.numel() * subtree_size, dtype=torch.float32)\n",
    "            dist.recv(recv_buffer, src=child)\n",
    "            child_data.append(recv_buffer)\n",
    "        \n",
    "        data = None\n",
    "        if len(child_data) == 0:\n",
    "            data = tensor\n",
    "        elif len(child_data) == 1:\n",
    "            data = torch.concat([child_data[0], tensor], dim=-1)\n",
    "        else:\n",
    "            data = torch.concat([child_data[0], tensor, child_data[1]], dim=-1)\n",
    "\n",
    "        if self.rank != root:\n",
    "            parent = get_parent(self.rank)\n",
    "            dist.send(data, dst=parent) \n",
    "            return None\n",
    "        else:\n",
    "            return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a9ec7",
   "metadata": {},
   "source": [
    "#### Data Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfdac930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9ed12fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_features),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_features, hidden_features),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_features, out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08d22d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParallelism:\n",
    "    def __init__(self):\n",
    "        self.rank = os.environ['RANK']\n",
    "        self.world_size = os.environ['WORLD_SIZE']\n",
    "\n",
    "        os.environ['MASTER_ADDR'] = 'localhost'\n",
    "        os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "        self.in_features = os.environ['IN_FEATURES']\n",
    "        self.hidden_features = os.environ['HIDDEN_FEATURES']\n",
    "        self.out_features = os.environ['OUT_FEATURES']\n",
    "\n",
    "        self.model = Model(\n",
    "            in_features=self.in_features,\n",
    "            hidden_features=self.hidden_features,\n",
    "            out_features=self.out_features\n",
    "        )\n",
    "\n",
    "        self.ddp_model = DistributedDataParallel(\n",
    "            module=self.model,\n",
    "            device_ids=[self.rank],\n",
    "            bucket_cap_mb=25,\n",
    "            gradient_as_bucket_view=True\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def setup(self):\n",
    "        dist.init_process_group(\n",
    "            backend='nccl',\n",
    "            rank=self.rank,\n",
    "            world_size=self.world_size\n",
    "        )\n",
    "\n",
    "        torch.cuda.set_device()\n",
    "\n",
    "    def cleanup(self):\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "    def train(self, n_samples):\n",
    "        dataset = torch.randn(n_samples, self.in_features)\n",
    "        labels = torch.randint(0, self.out_features, (n_samples,))\n",
    "\n",
    "        sampler = DistributedSampler(\n",
    "            dataset=dataset,\n",
    "            num_replicas=self.world_size,\n",
    "            rank=self.rank,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            list(zip(dataset, labels)),\n",
    "            batch_size=32,\n",
    "            sampler=sampler\n",
    "        )\n",
    "\n",
    "        n_epochs = os.environ['N_EPOCHS']\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.ddp_model.parameters(), lr=1e-3)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            sampler.set_epoch(epoch)\n",
    "\n",
    "            for _, (X, y) in enumerate(dataloader):\n",
    "                X = X.to(self.rank)\n",
    "                y = y.to(self.rank)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = self.ddp_model(X)\n",
    "                loss = criterion(output, y)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30923d68",
   "metadata": {},
   "source": [
    "#### Sequence Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb0fffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ffcca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceParallelismLayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-15):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.eps = eps\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.randn(d_model))\n",
    "        self.beta = nn.Parameter(torch.randn(d_model))\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "\n",
    "        normalised = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        output = self.alpha * normalised + self.beta\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "333908e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceParallelismDropout(nn.Module):\n",
    "    def __init__(self, p, rank):\n",
    "        super().__init__()\n",
    "\n",
    "        self.p = p\n",
    "        self.rank = rank\n",
    "\n",
    "        self.register_buffer('rng_seed', torch.zeros(1, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "        \n",
    "        if self.rank == 0:\n",
    "            self.rng_seed.random_()\n",
    "\n",
    "        dist.broadcast(self.rng_seed, src=0)\n",
    "        seed_val = int(self.rng_seed.item())\n",
    "\n",
    "        with torch.random.fork_rng(devices=[x.device]):\n",
    "            torch.random.manual_seed(seed_val)\n",
    "\n",
    "            out = nn.functional.dropout(x, p=self.p, training=True)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da92cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelism(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.out_features_per_rank = out_features // world_size\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(in_features, self.out_features_per_rank)\n",
    "        )\n",
    "\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.zeros(self.out_features_per_rank)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.matmul(x, self.weight) + self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fba9fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowParallelism(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.in_features_per_rank = in_features // world_size\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(self.in_features_per_rank, out_features)\n",
    "        )\n",
    "\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.zeros(self.out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return RowParallelismFunction(x, self.weight, self.bias, self.rank, self.world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c4bef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelismAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, rank, world_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.head_dim = d_model // world_size\n",
    "        self.n_heads_per_rank = n_heads // world_size\n",
    "\n",
    "        self.qkv_proj = ColumnParallelism(\n",
    "            in_features=d_model,\n",
    "            out_features=d_model * 3,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        self.out_proj = RowParallelism(\n",
    "            in_features=d_model,\n",
    "            out_features=d_model,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv_proj(x).reshape(batch_size, seq_len, 3, self.n_heads_per_rank, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scores = q @ k.transpose(-2, -1) / (self.head_dim ** 0.5)\n",
    "        attn = nn.functional.softmax(scores, dim=-1)\n",
    "        attn_output = attn @ v\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().reshapse(batch_size, seq_len, self.n_heads_per_rank * self.head_dim)\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37709dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelismMLP(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.fc1 = ColumnParallelism(\n",
    "            in_features=d_model,\n",
    "            out_features=d_ff,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        self.fc2 = RowParallelism(\n",
    "            in_features=d_ff,\n",
    "            out_features=d_model,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b03b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPSPTransformerBlock(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, rank, world_size, p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.p = p\n",
    "\n",
    "        self.attention = TensorParallelismAttention(\n",
    "            n_heads=n_heads,\n",
    "            d_model=d_model,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        self.mlp = TensorParallelismMLP(\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "\n",
    "        self.norm1 = SequenceParallelismLayerNorm(d_model)\n",
    "        self.norm2 = SequenceParallelismLayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = SequenceParallelismDropout(p)\n",
    "        self.dropout2 = SequenceParallelismDropout(p)\n",
    "\n",
    "    def sp_to_tp_transition(self, tensor: torch.tensor):\n",
    "        tensor_list = [torch.zeros_like(tensor) for _ in range(self.world_size)]\n",
    "        dist.all_gather(tensor_list, tensor=tensor)\n",
    "\n",
    "        tensor = torch.cat(tensor_list, dim=1)\n",
    "        return tensor\n",
    "    \n",
    "    def tp_to_sp_transition(self, tensor: torch.tensor):\n",
    "        chunks = torch.chunk(tensor, chunks=self.world_size, dim=1)\n",
    "        chunk = chunks[self.rank].contiguous()\n",
    "\n",
    "        return chunk\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = x\n",
    "\n",
    "        residual = output\n",
    "        output = self.norm1(output)\n",
    "        output = self.sp_to_tp_transition(output)\n",
    "        output = self.attention(output)\n",
    "        output = self.tp_to_sp_transition(output)\n",
    "        output = self.dropout1(output)\n",
    "        output += residual\n",
    "\n",
    "        residual = output\n",
    "        output = self.norm2(output)\n",
    "        output = self.sp_to_tp_transition(output)\n",
    "        output = self.mlp(output)\n",
    "        output = self.tp_to_sp_transition(output)\n",
    "        output = self.dropout2(output)\n",
    "        output += residual\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6d92af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RingAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, rank, world_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(\n",
    "            in_features=d_model,\n",
    "            out_features=d_model * 3,\n",
    "            bias=False\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len_local, d_model = x.shape\n",
    "\n",
    "        qkv = self.qkv_proj(x).reshape(batch_size, seq_len_local, 3, self.n_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        m = torch.full(\n",
    "            size=(batch_size, self.n_heads, seq_len_local, 1),\n",
    "            fill_value=float('-inf'),\n",
    "            device=x.device\n",
    "        )\n",
    "\n",
    "        d = torch.full(\n",
    "            size=(batch_size, self.n_heads, seq_len_local, 1),\n",
    "            fill_value=float('-inf'),\n",
    "            device=x.device\n",
    "        )\n",
    "\n",
    "        o = torch.full(\n",
    "            size=(batch_size, self.n_heads, seq_len_local, self.head_dim),\n",
    "            fill_value=0.0,\n",
    "            device=x.device\n",
    "        )\n",
    "\n",
    "        k_recv = k.clone()\n",
    "        v_recv = v.clone()\n",
    "\n",
    "        for _ in range(self.world_size - 1):\n",
    "            scores = q @ k_recv.transpose(-2, -1) / torch.sqrt(self.head_dim ** 0.5)\n",
    "\n",
    "            m, d, o = self._online_softmax(m, d, o, scores, v_recv)\n",
    "\n",
    "    def _online_softmax(self, m_prev, d_prev, o_prev, scores, v):\n",
    "        m_new = scores.max(dim=-1, keepdim=True).values()\n",
    "        m_new = torch.max(m_prev, m_new)\n",
    "\n",
    "        d = torch.exp(scores - m_new)\n",
    "        d_new = d_prev * torch.exp(m_prev - m_new) + d\n",
    "\n",
    "        o_new = o_prev * d_prev * torch.exp(m_prev - m_new) / d_new + torch.matmul(d / d_new, v)\n",
    "\n",
    "        return m_new, d_new, o_new\n",
    "    \n",
    "    def _ring_exchange(self, k_recv, v_recv):\n",
    "        send_rank = (self.rank + 1) % self.world_size\n",
    "        recv_rank = (self.rank - 1 + self.world_size) % self.world_size\n",
    "\n",
    "        k_recv_buffer = torch.zeros_like(k_recv)\n",
    "        v_recv_buffer = torch.zeros_like(v_recv)\n",
    "\n",
    "        k_send_handle = dist.isend(k_recv, dst=send_rank)\n",
    "        v_send_handle = dist.isend(v_recv, dst=send_rank)\n",
    "\n",
    "        k_recv_handle = dist.irecv(k_recv_buffer, src=recv_rank)\n",
    "        v_recv_handle = dist.irecv(v_recv_buffer, src=recv_rank)\n",
    "\n",
    "        k_send_handle.wait()\n",
    "        v_send_handle.wait()\n",
    "\n",
    "        k_recv_handle.wait()\n",
    "        v_recv_handle.wait()\n",
    "\n",
    "        k_recv, v_recv = k_recv_buffer, v_recv_buffer\n",
    "\n",
    "        return k_recv, v_recv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed6efc",
   "metadata": {},
   "source": [
    "#### Pipeline Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "537f4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    rank = os.environ['RANK']\n",
    "    world_size = os.environ['WORLD_SIZE']\n",
    "\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    dist.init_process_group(\n",
    "        backend='nccl',\n",
    "        rank=rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "    torch.cuda.set_device(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b7a4659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8e12ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineStage(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fa53eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_stages(hidden_dim, n_layers, world_size):\n",
    "    n_layers_per_rank = n_layers // world_size\n",
    "\n",
    "    all_layers = []\n",
    "    for _ in range(n_layers):\n",
    "        layer = torch.nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        all_layers.append(layer)\n",
    "\n",
    "    stages = []\n",
    "    for idx in range(world_size):\n",
    "        start_idx = idx * n_layers_per_rank\n",
    "        end_idx = (idx + 1) * n_layers_per_rank\n",
    "\n",
    "        stages.append(PipelineStage(all_layers[start_idx : end_idx]))\n",
    "\n",
    "    return stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56193d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneFOneBPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        stage: PipelineStage,\n",
    "        rank: int,\n",
    "        world_size: int,\n",
    "        n_micro_batches: int,\n",
    "        micro_batch_size: int,\n",
    "        input_shape: tuple, \n",
    "        dtype=torch.float32\n",
    "    ):\n",
    "        self.stage = stage,\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.n_micro_batches = n_micro_batches\n",
    "        self.micro_batch_size = micro_batch_size\n",
    "        self.input_shape = input_shape\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.next_rank = self.rank + 1 if self.rank < self.world_size - 1 else None\n",
    "        self.prev_rank = self.rank - 1 if self.rank > 0 else None\n",
    "\n",
    "        self.warmup_states = world_size - rank - 1\n",
    "        self.active_states = n_micro_batches - world_size\n",
    "\n",
    "    def send(self, tensor: torch.tensor, dst: int):\n",
    "        if dst is not None:\n",
    "            dist.send(tensor.contiguous(), dst=dst)\n",
    "\n",
    "    def recv(self, tensor_buffer: torch.tensor, src: int):\n",
    "        if src is not None:\n",
    "            dist.recv(tensor_buffer, src=src)\n",
    "            return tensor_buffer\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def forward(self, input: torch.tensor):\n",
    "        input.requires_grad = True\n",
    "        output = self.stage(input)\n",
    "\n",
    "        return output, input\n",
    "    \n",
    "    def backward(self, grad_input: torch.tensor, output: torch.tensor, saved_input: torch.tensor):\n",
    "        output.backward(grad_input)\n",
    "        return saved_input.grad\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        saved_inputs = []\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(self.warmup_states):\n",
    "            micro_batch = None\n",
    "            start_idx = i * self.micro_batch_size\n",
    "            end_idx = (i + 1) * self.micro_batch_size\n",
    "\n",
    "            if self.rank == 0:\n",
    "                micro_batch = batch[start_idx : end_idx]\n",
    "            else:\n",
    "                micro_batch = torch.zeros(self.micro_batch_size, *self.input_shape, dtype=self.dtype)\n",
    "                dist.recv(micro_batch, src=self.prev_rank)\n",
    "\n",
    "            output, saved_input = self.forward(micro_batch)\n",
    "\n",
    "            outputs.append(output)\n",
    "            saved_inputs.append(saved_input)\n",
    "\n",
    "            self.send(output, dst=self.next_rank)\n",
    "\n",
    "        for i in range(self.active_states):\n",
    "            micro_batch = None\n",
    "            start_idx = (i + self.warmup_states) * self.micro_batch_size\n",
    "            end_idx = (i + 1 + self.warmup_states) * self.micro_batch_size\n",
    "\n",
    "            if self.rank == 0:\n",
    "                micro_batch = batch[start_idx : end_idx]\n",
    "            else:\n",
    "                micro_batch = torch.zeros(self.micro_batch_size, *self.input_shape, dtype=self.dtype)\n",
    "                dist.recv(micro_batch, src=self.prev_rank)\n",
    "\n",
    "            output, saved_input = self.forward(micro_batch)\n",
    "\n",
    "            outputs.append(output)\n",
    "            saved_inputs.append(saved_input)\n",
    "\n",
    "            self.send(output, dst=self.next_rank)\n",
    "\n",
    "            output = outputs.pop(0)\n",
    "            saved_input = saved_inputs.pop(0)\n",
    "\n",
    "            if self.rank == self.world_size - 1:\n",
    "                grad_input = torch.ones_like(output, dtype=self.dtype)\n",
    "            else:\n",
    "                grad_input = torch.zeros_like(output, dtype=self.dtype)\n",
    "                dist.recv(grad_input, src=self.next_rank)\n",
    "\n",
    "            grad_output = self.backward(grad_input, output, saved_input)\n",
    "            self.send(grad_output, dst=self.prev_rank)\n",
    "\n",
    "        for idx in range(self.warmup_states):\n",
    "            output = outputs[idx]\n",
    "            saved_input = saved_inputs[idx]\n",
    "\n",
    "            if self.rank == self.world_size - 1:\n",
    "                grad_input = torch.ones_like(output, dtype=self.dtype)\n",
    "            else:\n",
    "                tensor_buffer = torch.zeros_like(output, dtype=self.dtype)\n",
    "                grad_input = self.recv(tensor_buffer, self.next_rank)\n",
    "\n",
    "            grad_output = self.backward(grad_input, output, saved_input)\n",
    "\n",
    "            self.send(grad_output, self.prev_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490d3193",
   "metadata": {},
   "source": [
    "#### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b585c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, alpha):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling_factor = alpha / rank\n",
    "\n",
    "        self.b = nn.Parameter(torch.zeros(in_features, rank))\n",
    "        self.a = nn.Parameter(torch.zeros(rank, out_features))\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.kaiming_normal_(self.a)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.matmul(x, self.b)\n",
    "        output = torch.matmul(output, self.a)\n",
    "\n",
    "        return output * self.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5d44e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, rank, alpha):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = linear\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.linear.weight.requires_grad = False\n",
    "        if self.linear.bias is not None:\n",
    "            self.linear.bias.requires_grad = False\n",
    "\n",
    "        self.lora = LoRA(\n",
    "            linear.in_features,\n",
    "            linear.out_features,\n",
    "            rank,\n",
    "            alpha\n",
    "        )\n",
    "\n",
    "        self.merged = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.merged:\n",
    "            return self.linear(x)\n",
    "        else:\n",
    "            return self.linear(x) + self.lora(x)\n",
    "        \n",
    "    def merge(self):\n",
    "        if not self.merged:\n",
    "            with torch.no_grad():\n",
    "                delta_w = torch.matmul(self.lora.b, self.lora.a) * self.lora.scaling_factor\n",
    "                self.linear.weight += delta_w\n",
    "            self.merged = True\n",
    "\n",
    "    def unmerge(self):\n",
    "        if self.merged:\n",
    "            with torch.no_grad():\n",
    "                delta_w = torch.matmul(self.lora.b, self.lora.a) * self.lora.scaling_factor\n",
    "                self.linear.weight -= delta_w\n",
    "            self.merged = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110996bb",
   "metadata": {},
   "source": [
    "#### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1164658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(model, input_ids, eos_token_id, max_token_length):\n",
    "    generated = input_ids.clone()\n",
    "\n",
    "    for _ in range(max_token_length):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            next_logits = logits[:,-1,:]\n",
    "\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "        next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "\n",
    "        generated = torch.concat([generated, next_token], dim=-1)\n",
    "\n",
    "        if (next_token == eos_token_id).all():\n",
    "            break\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca5020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_sampling(model, input_ids, temperature, max_length, eos_token_id):\n",
    "    generated = input_ids.clone()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            next_logits = logits[:,-1,:] / temperature\n",
    "\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        generated = torch.concat([generated, next_token], dim=-1)\n",
    "\n",
    "        if (next_token == eos_token_id).all():\n",
    "            break\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b1952e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_sampling(model, input_ids, temperature, k, eos_token_id, max_length):\n",
    "    generated = input_ids.clone()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            next_logits = logits[:,-1,:] / temperature\n",
    "\n",
    "        topk_values, topk_indices = torch.topk(next_logits, k=k, dim=-1)\n",
    "\n",
    "        filtered_logits = torch.full_like(next_logits, fill_value=float('-inf'))\n",
    "        filtered_logits.scatter_(dim=-1, index=topk_indices, src=topk_values)\n",
    "\n",
    "        probs = torch.softmax(filtered_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        generated = torch.concat([generated, next_token], dim=-1)\n",
    "\n",
    "        if (next_token == eos_token_id).all():\n",
    "            break\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40862e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(model, input_ids, p, temperature, eos_token_id, max_length):\n",
    "    generated = input_ids.clone()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            next_logits = logits[:,-1,:] / temperature\n",
    "\n",
    "        sorted_logits, sorted_indices = torch.sort(next_logits, dim=-1, descending=True)\n",
    "\n",
    "        sorted_probs = torch.softmax(sorted_logits)\n",
    "        prob_cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        mask = prob_cumsum > p\n",
    "        mask[...,0] = False\n",
    "        sorted_logits[mask] = float('-inf')\n",
    "\n",
    "        filtered_logits = torch.full_like(sorted_logits, fill_value=float('-inf'))\n",
    "        filtered_logits.scatter_(dim=-1, index=sorted_indices, src=sorted_logits)\n",
    "\n",
    "        probs = torch.softmax(filtered_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        generated = torch.concat([generated, next_token], dim=-1)\n",
    "\n",
    "        if (next_token == eos_token_id).all():\n",
    "            break\n",
    "    \n",
    "    return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2237b2",
   "metadata": {},
   "source": [
    "#### Expert Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff5df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c03a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c5af3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKRouter(nn.Module):\n",
    "    def __init__(self, d_model, n_experts, top_k, capacity_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_experts = n_experts\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factory = capacity_factor\n",
    "\n",
    "        self.router = nn.Linear(d_model, n_experts, bias=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, seq_len, d_model = input.size\n",
    "\n",
    "        n_tokens = batch_size * seq_len\n",
    "        input = input.reshape(n_tokens, d_model)\n",
    "\n",
    "        router_logits = self.router(input)\n",
    "        router_probs = torch.softmax(input=router_logits, dim=-1)\n",
    "\n",
    "        expert_weights, expert_indices = torch.topk(router_probs, self.top_k, dim=-1)\n",
    "        expert_weights = expert_weights / expert_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        load_balancing_loss = self._compute_load_balancing_loss(router_probs)\n",
    "\n",
    "        return expert_weights, expert_indices, load_balancing_loss\n",
    "\n",
    "    def _compute_load_balancing_loss(self, router_probs):\n",
    "        num_tokens = router_probs.shape[0]\n",
    "\n",
    "        expert_assignment = torch.argmax(router_probs, dim=-1)\n",
    "        expert_count = torch.bincount(expert_assignment).float()\n",
    "\n",
    "        f_i = expert_count / num_tokens\n",
    "        p_i = router_probs.mean(dim=0)\n",
    "\n",
    "        loss = self.n_experts * (f_i * p_i).sum()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8282877",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedMoELayer(nn.Module):\n",
    "    def __init__(self, rank, world_size, d_model, d_ff, n_experts, top_k, capacity_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.n_experts = n_experts\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factor = capacity_factor\n",
    "\n",
    "        self.n_experts_per_rank = n_experts // world_size\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(d_model, d_ff) for _ in range(self.n_experts_per_rankx)\n",
    "        ])\n",
    "\n",
    "        self.router = TopKRouter(d_model, n_experts, top_k, capacity_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        num_tokens = batch_size * seq_len\n",
    "\n",
    "        expert_weights, expert_indices, load_balancing_loss = self.router(x)\n",
    "\n",
    "        capacity = int((num_tokens / self.n_experts) * self.capacity_factor)\n",
    "\n",
    "        dispatch_data, combine_weights = self._prepare_all_to_all(\n",
    "            x, expert_indices, expert_weights, capacity\n",
    "        )\n",
    "\n",
    "        received_data = self._all_to_all_scatter(dispatch_data)\n",
    "\n",
    "        expert_output = self._process_local_experts(received_data)\n",
    "\n",
    "        gathered_output = self._all_to_all_gather(expert_output)\n",
    "\n",
    "        output = self._combine_outputs(gathered_output, combine_weights)\n",
    "\n",
    "        output = output.reshape(batch_size, seq_len, d_model)\n",
    "\n",
    "        return output, load_balancing_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8082a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoETransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete transformer block with MoE instead of dense FFN.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, num_experts, intermediate_size,\n",
    "                 top_k=2, capacity_factor=1.25):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Layer norms\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Attention (standard multi-head attention)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            hidden_size, \n",
    "            num_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # MoE layer (replaces dense FFN)\n",
    "        self.moe = DistributedMoELayer(\n",
    "            hidden_size, \n",
    "            num_experts, \n",
    "            intermediate_size,\n",
    "            top_k, \n",
    "            capacity_factor\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq, hidden)\n",
    "            attn_mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq, hidden)\n",
    "            aux_loss: Load balancing loss\n",
    "        \"\"\"\n",
    "        # Attention block\n",
    "        normed = self.ln1(x)\n",
    "        attn_output, _ = self.attention(normed, normed, normed, attn_mask=attn_mask)\n",
    "        x = x + attn_output\n",
    "        \n",
    "        # MoE block\n",
    "        normed = self.ln2(x)\n",
    "        moe_output, aux_loss = self.moe(normed)\n",
    "        x = x + moe_output\n",
    "        \n",
    "        return x, aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae1b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        positions = torch.arange(0, max_seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.pow(10000.0, - torch.arange(0, d_model, 2).float() / d_model)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(positions * div_term)\n",
    "        pe[:, 1::2] = torch.cos(positions * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, seq_len, _ = x.shape\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88240457",
   "metadata": {},
   "source": [
    "## Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a070b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.w = torch.randn(in_features, out_features)\n",
    "        self.b = torch.randn(out_features)\n",
    "\n",
    "    def forward(self, x, y_true, lr, iterations):\n",
    "        for _ in range(iterations):\n",
    "            y_pred = x @ self.w + self.b\n",
    "            loss = self.mean_squared_error(y_pred, y_true)\n",
    "            dw, db = self.gradients(y_pred, y_true)\n",
    "            self.backward(dw, db, lr)\n",
    "    \n",
    "    def mean_squared_error(self, y_pred, y_true):\n",
    "        return torch.mean((y_pred - y_true) ** 2) \n",
    "    \n",
    "    def gradients(self, y_pred, y_true, x):\n",
    "        diff = (y_pred - y_true)\n",
    "\n",
    "        dw = - 2 * torch.mean(diff) * x\n",
    "        db = - 2 * torch.mean(diff)\n",
    "\n",
    "        return dw, db\n",
    "    \n",
    "    def backward(self, dw, db, lr):\n",
    "        self.w = self.w - lr * dw\n",
    "        self.b = self.b - lr * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de73b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, in_features, n_labels):\n",
    "        self.w = torch.randn(in_features, n_labels)\n",
    "        self.b = torch.zeros(n_labels)\n",
    "\n",
    "    def binary_cross_entropy(self, y_pred, y_true, x):\n",
    "        return torch.mean(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "    \n",
    "    def compute_gradients(self, y_true, y_pred, x):\n",
    "        diff = (y_true - y_pred)\n",
    "\n",
    "        dw = torch.mean(x * diff)\n",
    "        db = torch.mean(diff)\n",
    "\n",
    "        return dw, db\n",
    "    \n",
    "    def backward(self, dw, db, learning_rate):\n",
    "        self.w = self.w - learning_rate * dw\n",
    "        self.b = self.b - learning_rate * db\n",
    "        \n",
    "    def forward(self, x, y_true, learning_rate, iterations):\n",
    "        for _ in range(iterations):\n",
    "            y_pred = x @ self.w + self.b\n",
    "            loss = self.binary_cross_entropy(y_pred, y_true, x)\n",
    "            dw, db = self.compute_gradients(y_true, y_pred, x)\n",
    "            self.backward(dw, db, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c917dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters, max_iterations):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "        self.centroids = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        idx = torch.randint(0, X.shape[0], (self.n_clusters,))\n",
    "        self.centroids = X[idx]\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            distances = torch.stack([torch.norm(X - c, dim=1) for c in self.centroids], dim=1)\n",
    "            labels = torch.argmin(distances, dim=1)\n",
    "\n",
    "            new_centroids = torch.stack([X[labels == k].mean(dim=0) for k in range(self.n_clusters)], dim=0)\n",
    "\n",
    "            if torch.allclose(new_centroids, self.centroids):\n",
    "                break\n",
    "\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "        self.labels_ = labels\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        distances = torch.stack([torch.norm(x - c, dim=1) for c in self.centroids], dim=1)\n",
    "        return torch.argmin(distances, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b2105d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        for x in X:\n",
    "            distances = torch.norm(self.X - x, dim=1)\n",
    "            _, topk_indices = torch.topk(distances, k=self.k, largest=False)\n",
    "\n",
    "            k_labels = self.y[topk_indices]\n",
    "\n",
    "            prediction = torch.mode(k_labels).values\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        return torch.stack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900315ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data, n_components):\n",
    "    mean = torch.mean(data, dim=0)\n",
    "    std = torch.std(data, dim=0)\n",
    "\n",
    "    normalized = (data - mean) / (std)\n",
    "    n_samples = data.shape[0]\n",
    "\n",
    "    cov_matrix = torch.matmul(normalized.t(), normalized) / (n_samples - 1)\n",
    "\n",
    "    eigen_values, eigen_vectors = torch.linalg.eigh(cov_matrix)\n",
    "    \n",
    "    sorted_index = torch.argsort(eigen_values)[::-1]\n",
    "    sorted_eigen_values = eigen_values[sorted_index]\n",
    "    sorted_eigen_vectors = eigen_vectors[:, sorted_index]\n",
    "\n",
    "    principle_components = sorted_eigen_vectors[:, : n_components]\n",
    "\n",
    "    reduced_data = torch.matmul(normalized, principle_components)\n",
    "\n",
    "    return reduced_data, principle_components, sorted_eigen_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b214e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, lambda_p):\n",
    "        self.lambda_p = lambda_p\n",
    "        self.W, self.b = None, None\n",
    "\n",
    "    def fit(self, X, y, n_epochs, learning_rate):\n",
    "        y = torch.where(y == 0, -1.0, 1.0)\n",
    "\n",
    "        self.W = torch.randn(X.shape[1])\n",
    "        self.b = torch.zeros(X.shape[0])\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        for epoch in range(n_epochs):\n",
    "            for i in range(n_samples):\n",
    "                condition = y[i] * (self.W * X[i] + self.b) >= 1\n",
    "\n",
    "                if condition:\n",
    "                    self.W -= learning_rate * (2 * self.lambda_p * self.W)\n",
    "                else:\n",
    "                    self.W -= learning_rate * (2 * self.lambda_p * self.W - y[i] * X[i])\n",
    "                    self.b -= learning_rate * (-y[i])\n",
    "\n",
    "    def hinge_loss(self, X, y):\n",
    "        loss = torch.max(0, torch.mean(1 - y * (X @ self.W + self.b)))\n",
    "        return loss\n",
    "    \n",
    "    def total_loss(self, X, y):\n",
    "        return torch.square(self.W) + self.hinge_loss(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = X @ self.W + self.b\n",
    "        return torch.where(y >= 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9d47f",
   "metadata": {},
   "source": [
    "#### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1433808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    return torch.mean(torch.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da7c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_loss(y_true, y_pred):\n",
    "    return torch.mean(torch.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df5ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(y_true, y_pred, eps=1e-8):\n",
    "    return - torch.mean(\n",
    "        y_true * torch.log(y_pred + eps) + (1 - y_true) * torch.log(1 - y_pred + eps)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f80ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, logits, eps=1e-8):\n",
    "    y_pred = torch.softmax(logits)\n",
    "    return - torch.mean(y_true * torch.log(y_pred + eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc26db1e",
   "metadata": {},
   "source": [
    "##### Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9031eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc173a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c38e54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return torch.max(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2083b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.1):\n",
    "    return torch.max(x, alpha * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "834e0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PReLU:\n",
    "    def __init__(self):\n",
    "        self.alpha = torch.tensor(0.25, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.max(x, self.alpha * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9070d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(x, alpha):\n",
    "    return torch.where(x > 0, x, alpha * (torch.exp(x) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f517878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = torch.exp(x - torch.max(x).values) # stable form\n",
    "    return exp_x / torch.sum(exp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffbc1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    # log(softmax(x)) = x - log(sum(exp(x)))\n",
    "    return x - torch.logsumexp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd821ac3",
   "metadata": {},
   "source": [
    "#### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a43854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "class SGD:\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            p.data -= self.lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2f45cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent with Momentum\n",
    "class SGDMomentum:\n",
    "    def __init__(self, params, lr=1e-3, momentum=0.9):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.v = [torch.zeros_like(p) for p in params]\n",
    "\n",
    "    def step(self):\n",
    "        for i, p in enumerate(self.params):\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * p.grad\n",
    "            p.data += self.v[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da872c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "    def __init__(self, params, lr=0.01, eps=1e-8):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.cache = [torch.zeros_like(p) for p in params]\n",
    "\n",
    "    def step(self):\n",
    "        for i, p in enumerate(self.params):\n",
    "            self.cache[i] += p.grad ** 2\n",
    "            p.data -= self.lr * p.grad / (torch.sqrt(self.cache[i]) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c19d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp:\n",
    "    def __init__(self, params, decay_rate, lr=0.01, eps=1e-8):\n",
    "        self.params = params\n",
    "        self.decay_rate = decay_rate\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.cache = [torch.zeros_like(p) for p in params]\n",
    "\n",
    "    def step(self):\n",
    "        for i, p in enumerate(self.params):\n",
    "            self.cache[i] = self.cache[i] * self.decay_rate + (1 - self.decay_rate) * p.grad ** 2\n",
    "            p.data -= self.lr * p.grad / (torch.sqrt(self.cache[i]) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd13fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, params, lr=0.0001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps  \n",
    "\n",
    "        self.m = [torch.zeros_like(p) for p in params]\n",
    "        self.v = [torch.zeros_like(p) for p in params]\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "\n",
    "        for i, p in self.params:\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * p.grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * p.grad ** 2\n",
    "\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            p.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c46c0e",
   "metadata": {},
   "source": [
    "## Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf24968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c3f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_height = config['image_height']\n",
    "        self.image_width = config['image_widht']\n",
    "        self.image_channels = config['image_channels']\n",
    "        self.d_model = config['d_model']\n",
    "\n",
    "        self.patch_height = config['patch_height']\n",
    "        self.patch_width = config['patch_width']\n",
    "\n",
    "        n_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "        patch_dim = self.patch_height * self.patch_width * self.image_channels\n",
    "\n",
    "        self.patch_embedding = nn.Sequential(\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, self.d_model),\n",
    "            nn.LayerNorm(patch_dim)\n",
    "        )\n",
    "\n",
    "        self.position_embedding = nn.Embedding(n_patches + 1, self.d_model)\n",
    "        self.cls_token = nn.Parameter(torch.randn(self.d_model))\n",
    "\n",
    "        self.n_patches = n_patches\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = x.reshape(batch_size, self.n_patches, self.patch_height, self.n_patches, self.patch_width, self.image_channels)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5)\n",
    "        x = x.reshape(batch_size, self.n_patches * self.n_patches, self.patch_height * self.patch_width * self.image_channels)\n",
    "\n",
    "        out = self.patch_embedding(x)\n",
    "        cls_token = self.cls_token.unsqueeze(0).expand(batch_size, 1).unsqueeze(1)\n",
    "\n",
    "        out = torch.cat([out, cls_token], dim=1)\n",
    "\n",
    "        out += self.position_embedding(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e6e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = config['n_heads']\n",
    "        self.d_model = config['d_model']\n",
    "\n",
    "        self.head_dim = self.d_model // self.n_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.d_model, 3 * self.d_model)\n",
    "        self.out_proj = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, n_patches, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv_proj(x).reshape(batch_size, n_patches, 3, self.n_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scores = q @ k.transpose(-2, -1) / (self.head_dim ** 0.5)\n",
    "        attn = nn.functional.softmax(scores, dim=-1)\n",
    "\n",
    "        output = attn @ v\n",
    "        output = output.transpose(1, 2).reshape(batch_size, n_patches, self.d_model)\n",
    "\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e25682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = config['n_heads']\n",
    "        self.d_model = config['d_model']\n",
    "        self.d_ff = config['d_ff']\n",
    "\n",
    "        self.attention = Attention(config)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.d_ff, self.d_model)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(self.d_model)\n",
    "        self.norm2 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "        dropout_p = config['dropout_p']\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout_p)\n",
    "        self.dropout2 = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.norm1(out)\n",
    "        out = self.attention(out)\n",
    "        out = self.dropout1(out)    \n",
    "        out += residual\n",
    "\n",
    "        residual = out\n",
    "        out = self.norm2(out)\n",
    "        out = self.mlp(out)\n",
    "        out = self.dropout2(out)\n",
    "        out += residual\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fdce59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self, config): \n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embedding_block = PatchEmbedding(config)\n",
    "        \n",
    "        n_layers = config['n_layers']\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            TransformerLayer(config) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.norm = nn.LayerNorm(config['d_model'])\n",
    "        self.linear = nn.Linear(config['d_model'], config['n_classes'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.patch_embedding_block(out)\n",
    "        for layer in self.n_layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        out = self.norm(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cd55e2",
   "metadata": {},
   "source": [
    "## DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "242c75ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNoiseSchedule:\n",
    "    def __init__(self, n_steps, beta_start, beta_end):\n",
    "        self.n_steps = n_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        \n",
    "        self.betas = torch.linspace(beta_start, beta_end, steps=self.n_steps)\n",
    "\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_cum_prod = torch.cumprod(self.alpha)\n",
    "\n",
    "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
    "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1. - self.alpha_cum_prod)\n",
    "\n",
    "    def noise(self, original, noise, t):\n",
    "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod[t]\n",
    "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod[t]\n",
    "\n",
    "        for _ in range(len(original.shape) - 1):\n",
    "            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(0)\n",
    "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(0)\n",
    "\n",
    "        return sqrt_alpha_cum_prod * original + sqrt_one_minus_alpha_cum_prod * noise\n",
    "    \n",
    "    def denoise(self, xt, noise_pred, t):\n",
    "        x0 = (xt - self.sqrt_one_minus_alpha_cum_prod[t] * noise_pred) / self.sqrt_alpha_cum_prod[t]\n",
    "        x0 = torch.clamp(x0, 0.0, 1.0)\n",
    "\n",
    "        xt_1 = xt - ((1. - self.alpha[t]) / (self.sqrt_one_minus_alpha_cum_prod)) * noise_pred\n",
    "        xt_1 = xt_1 / self.sqrt_alpha_cum_prod[t]\n",
    "\n",
    "        if t == 0:\n",
    "            return xt_1, x0\n",
    "        else:\n",
    "            variance = (1. - self.alpha[t]) * ((1. - self.alpha_cum_prod[t - 1]) / (1. - self.alpha_cum_prod[t]))\n",
    "            std = variance ** 0.5\n",
    "\n",
    "            z = torch.randn(xt.shape)\n",
    "\n",
    "            return xt_1 + std * z, x0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
