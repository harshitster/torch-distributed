{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39a4938f",
   "metadata": {},
   "source": [
    "### Part 1: Core Quantization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806ff241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c3fed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING BASIC QUANTIZATION\n",
      "============================================================\n",
      "\n",
      "Original tensor:\n",
      "tensor([[ 0.1793, -3.9349, -1.2818,  0.9817],\n",
      "        [-2.7567,  2.7398, -1.8403, -0.9391],\n",
      "        [-3.3878, -0.0266, -1.0098,  2.2864],\n",
      "        [ 0.0894, -0.6689, -1.5727,  3.1289]])\n",
      "Range: [-3.935, 3.129]\n",
      "\n",
      "Quantized (INT8):\n",
      "tensor([[   6, -127,  -41,   32],\n",
      "        [ -89,   88,  -59,  -30],\n",
      "        [-109,   -1,  -33,   74],\n",
      "        [   3,  -22,  -51,  101]], dtype=torch.int8)\n",
      "Scale: 0.030984\n",
      "\n",
      "Dequantized:\n",
      "tensor([[ 0.1859, -3.9349, -1.2703,  0.9915],\n",
      "        [-2.7576,  2.7266, -1.8280, -0.9295],\n",
      "        [-3.3772, -0.0310, -1.0225,  2.2928],\n",
      "        [ 0.0930, -0.6816, -1.5802,  3.1294]])\n",
      "\n",
      "Mean absolute error: 0.007639\n",
      "\n",
      "\n",
      "ReLU activations (asymmetric):\n",
      "tensor([[0.0000, 0.0000, 1.1442, 0.9498],\n",
      "        [2.4502, 0.0193, 0.0000, 0.0831],\n",
      "        [0.8293, 0.1243, 0.0000, 0.0000],\n",
      "        [2.5610, 0.0000, 1.0084, 0.0000]])\n",
      "Range: [0.000, 2.561]\n",
      "\n",
      "Quantized (UINT8):\n",
      "tensor([[-127, -127,  -14,  -33],\n",
      "        [ 116, -125, -127, -119],\n",
      "        [ -45, -115, -127, -127],\n",
      "        [ 127, -127,  -27, -127]], dtype=torch.int8)\n",
      "Scale: 0.010083, Zero point: -127\n",
      "\n",
      "Dequantized:\n",
      "tensor([[0.0000, 0.0000, 1.1394, 0.9478],\n",
      "        [2.4501, 0.0202, 0.0000, 0.0807],\n",
      "        [0.8268, 0.1210, 0.0000, 0.0000],\n",
      "        [2.5610, 0.0000, 1.0083, 0.0000]])\n",
      "\n",
      "Mean absolute error: 0.001003\n"
     ]
    }
   ],
   "source": [
    "def quantize_tensor_symmetric(tensor, num_bits=8):\n",
    "    \"\"\"\n",
    "    Symmetric quantization: range centered at 0\n",
    "    \n",
    "    Args:\n",
    "        tensor: FP32 tensor to quantize\n",
    "        num_bits: bit width (8 for INT8)\n",
    "    \n",
    "    Returns:\n",
    "        q_tensor: quantized tensor (INT)\n",
    "        scale: quantization scale\n",
    "    \"\"\"\n",
    "\n",
    "    q_max = 2 ** (num_bits - 1) - 1\n",
    "    q_min = - q_max\n",
    "\n",
    "    max_val = tensor.abs().max()\n",
    "    scale = max_val / q_max\n",
    "\n",
    "    if scale == 0.0:\n",
    "        scale = 1.0\n",
    "\n",
    "    q_tensor = torch.clamp(\n",
    "        torch.round(tensor / scale), \n",
    "        q_min, q_max\n",
    "    ).to(torch.int8)\n",
    "\n",
    "    return q_tensor, scale\n",
    "\n",
    "def dequantize_tensor(q_tensor, scale):\n",
    "    \"\"\"\n",
    "    Dequantize: INT8 -> FP32\n",
    "    \n",
    "    Args:\n",
    "        q_tensor: quantized tensor (INT8)\n",
    "        scale: quantization scale\n",
    "    \n",
    "    Returns:\n",
    "        tensor: dequantized FP32 tensor\n",
    "    \"\"\"\n",
    "    return q_tensor.float() * scale\n",
    "\n",
    "\n",
    "def quantize_tensor_asymmetric(tensor, num_bits=8):\n",
    "    \"\"\"\n",
    "    Asymmetric quantization: uses full range\n",
    "    \n",
    "    Args:\n",
    "        tensor: FP32 tensor to quantize\n",
    "        num_bits: bit width\n",
    "    \n",
    "    Returns:\n",
    "        q_tensor: quantized tensor (INT)\n",
    "        scale: quantization scale\n",
    "        zero_point: zero point\n",
    "    \"\"\"\n",
    "    q_max = 2 ** (num_bits - 1) - 1\n",
    "    q_min = - q_max\n",
    "\n",
    "    max_val = tensor.abs().max()\n",
    "    min_val = tensor.abs().min()\n",
    "\n",
    "    scale = (max_val - min_val) / (q_max - q_min)\n",
    "\n",
    "    if scale == 0.0:\n",
    "        scale = 1.0\n",
    "\n",
    "    zero_point = q_min - torch.round(min_val / scale)\n",
    "    zero_point = torch.clamp(zero_point, q_min, q_max).to(torch.int8)\n",
    "\n",
    "    q_tensor = torch.clamp(\n",
    "        torch.round(tensor / scale) + zero_point,\n",
    "        q_min, q_max \n",
    "    ).to(torch.int8)\n",
    "\n",
    "    return q_tensor, scale, zero_point\n",
    "\n",
    "def dequantize_tensor_asymmetric(q_tensor, scale, zero_point):\n",
    "    \"\"\"\n",
    "    Dequantize asymmetric: UINT8 -> FP32\n",
    "    \"\"\"\n",
    "    return (q_tensor.float() - zero_point) * scale\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING BASIC QUANTIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tensor_fp32 = torch.randn(4, 4) * 2.0\n",
    "print(f\"\\nOriginal tensor:\\n{tensor_fp32}\")\n",
    "print(f\"Range: [{tensor_fp32.min():.3f}, {tensor_fp32.max():.3f}]\")\n",
    "\n",
    "q_tensor, scale = quantize_tensor_symmetric(tensor_fp32)\n",
    "print(f\"\\nQuantized (INT8):\\n{q_tensor}\")\n",
    "print(f\"Scale: {scale:.6f}\")\n",
    "\n",
    "dq_tensor = dequantize_tensor(q_tensor, scale)\n",
    "print(f\"\\nDequantized:\\n{dq_tensor}\")\n",
    "\n",
    "error = (tensor_fp32 - dq_tensor).abs().mean()\n",
    "print(f\"\\nMean absolute error: {error:.6f}\")\n",
    "\n",
    "tensor_relu = torch.relu(torch.randn(4, 4))\n",
    "print(f\"\\n\\nReLU activations (asymmetric):\\n{tensor_relu}\")\n",
    "print(f\"Range: [{tensor_relu.min():.3f}, {tensor_relu.max():.3f}]\")\n",
    "\n",
    "q_tensor_asym, scale_asym, zp = quantize_tensor_asymmetric(tensor_relu)\n",
    "print(f\"\\nQuantized (UINT8):\\n{q_tensor_asym}\")\n",
    "print(f\"Scale: {scale_asym:.6f}, Zero point: {zp}\")\n",
    "\n",
    "dq_tensor_asym = dequantize_tensor_asymmetric(q_tensor_asym, scale_asym, zp)\n",
    "print(f\"\\nDequantized:\\n{dq_tensor_asym}\")\n",
    "\n",
    "error_asym = (tensor_relu - dq_tensor_asym).abs().mean()\n",
    "print(f\"\\nMean absolute error: {error_asym:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621350ba",
   "metadata": {},
   "source": [
    "### Part 2: Per-Channel Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "969884cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING PER-CHANNEL QUANTIZATION\n",
      "============================================================\n",
      "\n",
      "Weight matrix:\n",
      "tensor([[ 0.1753,  0.9806,  0.3283,  0.7704, -0.5617,  0.6033,  0.1387,  0.2544],\n",
      "        [-0.8346,  0.5745,  0.5997,  0.4270,  0.4686, -1.8178,  1.7313,  2.6731],\n",
      "        [ 0.1446, -1.6448,  1.1356, -1.9062, -0.2997,  0.1748, -1.0819, -1.7786],\n",
      "        [ 0.2215, -0.2306, -0.2458, -0.1055, -0.1148,  0.2646, -0.4096, -0.3463]])\n",
      "\n",
      "Per-channel statistics:\n",
      "  Channel 0: range [-0.562, 0.981]\n",
      "  Channel 1: range [-1.818, 2.673]\n",
      "  Channel 2: range [-1.906, 1.136]\n",
      "  Channel 3: range [-0.410, 0.265]\n",
      "\n",
      "Per-tensor error: 0.006477\n",
      "Per-channel error: 0.003069\n",
      "Improvement: 52.6%\n",
      "\n",
      "Per-channel scales: tensor([0.0077, 0.0210, 0.0150, 0.0032])\n"
     ]
    }
   ],
   "source": [
    "def quantize_per_channel_symmetric(tensor, channel_dim=0, num_bits=8):\n",
    "    \"\"\"\n",
    "    Per-channel symmetric quantization for weights\n",
    "    \n",
    "    Args:\n",
    "        tensor: Weight tensor (e.g., [out_features, in_features])\n",
    "        channel_dim: Which dimension is the channel (0 for output channels)\n",
    "        num_bits: bit width\n",
    "    \n",
    "    Returns:\n",
    "        q_tensor: quantized tensor\n",
    "        scales: per-channel scales\n",
    "    \"\"\"\n",
    "    q_max = 2 ** (num_bits - 1) - 1\n",
    "    q_min = - q_max\n",
    "\n",
    "    tensor_transposed = tensor.transpose(0, channel_dim) if channel_dim != 0 else tensor\n",
    "    num_channels = tensor_transposed.shape[0]\n",
    "\n",
    "    scales = []\n",
    "    q_tensor_list = []\n",
    "\n",
    "    for i in range(num_channels):\n",
    "        channel_data = tensor_transposed[i,:]\n",
    "        max_val = channel_data.abs().max()\n",
    "\n",
    "        scale = max_val / q_max\n",
    "        if scale == 0.0:\n",
    "            scale = 1.0\n",
    "        scales.append(scale)\n",
    "\n",
    "        q_channel = torch.clamp(\n",
    "            torch.round(channel_data / scale),\n",
    "            q_min, q_max\n",
    "        ).to(torch.int8)\n",
    "\n",
    "        q_tensor_list.append(q_channel)\n",
    "\n",
    "    q_tensor = torch.stack(q_tensor_list, dim=0)\n",
    "    scales = torch.tensor(scales)\n",
    "\n",
    "    if channel_dim != 0:\n",
    "        q_tensor = q_tensor.transpose(0, channel_dim)\n",
    "    \n",
    "    return q_tensor, scales\n",
    "\n",
    "def dequantize_per_channel(q_tensor, scales, channel_dim=0):\n",
    "    \"\"\"\n",
    "    Dequantize per-channel quantized tensor\n",
    "    \"\"\"\n",
    "    q_tensor_transposed = q_tensor.transpose(0, channel_dim) if channel_dim != 0 else q_tensor\n",
    "    num_channels = q_tensor_transposed.shape[0]\n",
    "    \n",
    "    dq_list = []\n",
    "    for i in range(num_channels):\n",
    "        dq_channel = q_tensor_transposed[i].float() * scales[i]\n",
    "        dq_list.append(dq_channel)\n",
    "    \n",
    "    dq_tensor = torch.stack(dq_list)\n",
    "    \n",
    "    if channel_dim != 0:\n",
    "        dq_tensor = dq_tensor.transpose(0, channel_dim)\n",
    "    \n",
    "    return dq_tensor\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING PER-CHANNEL QUANTIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "weight = torch.randn(4, 8)  \n",
    "weight[0] *= 0.5  \n",
    "weight[1] *= 2.0  \n",
    "weight[2] *= 1.0  \n",
    "weight[3] *= 0.3  \n",
    "\n",
    "print(f\"\\nWeight matrix:\\n{weight}\")\n",
    "print(f\"\\nPer-channel statistics:\")\n",
    "for i in range(4):\n",
    "    print(f\"  Channel {i}: range [{weight[i].min():.3f}, {weight[i].max():.3f}]\")\n",
    "\n",
    "q_tensor_pt, scale_pt = quantize_tensor_symmetric(weight)\n",
    "dq_tensor_pt = dequantize_tensor(q_tensor_pt, scale_pt)\n",
    "error_pt = (weight - dq_tensor_pt).abs().mean()\n",
    "print(f\"\\nPer-tensor error: {error_pt:.6f}\")\n",
    "\n",
    "q_tensor_pc, scales_pc = quantize_per_channel_symmetric(weight, channel_dim=0)\n",
    "dq_tensor_pc = dequantize_per_channel(q_tensor_pc, scales_pc, channel_dim=0)\n",
    "error_pc = (weight - dq_tensor_pc).abs().mean()\n",
    "print(f\"Per-channel error: {error_pc:.6f}\")\n",
    "print(f\"Improvement: {(error_pt - error_pc) / error_pt * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nPer-channel scales: {scales_pc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18e81d",
   "metadata": {},
   "source": [
    "### Part 3: Quantized Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0744765f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING QUANTIZED LINEAR LAYER\n",
      "============================================================\n",
      "\n",
      "FP32 output shape: torch.Size([8, 64])\n",
      "FP32 output range: [-1.851, 1.513]\n",
      "\n",
      "Quantized output shape: torch.Size([8, 64])\n",
      "Quantized output range: [-1.850, 1.510]\n",
      "\n",
      "Absolute error: 0.004590\n",
      "Relative error: 0.009914 (0.99%)\n",
      "\n",
      "Memory usage:\n",
      "  FP32: 32768 bytes\n",
      "  INT8: 8448 bytes\n",
      "  Reduction: 3.88x\n"
     ]
    }
   ],
   "source": [
    "class QuantizedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with INT8 weights and INT8 activations\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store original FP32 weights for initialization\n",
    "        self.linear_fp32 = nn.Linear(in_features, out_features, bias=bias)\n",
    "        \n",
    "        # Quantized weights (will be set by quantize method)\n",
    "        self.register_buffer('weight_quantized', torch.zeros(out_features, in_features, dtype=torch.int8))\n",
    "        self.register_buffer('weight_scales', torch.zeros(out_features))\n",
    "        \n",
    "        # Activation scale (will be set during calibration)\n",
    "        self.register_buffer('activation_scale', torch.tensor(1.0))\n",
    "        \n",
    "        self.quantized = False\n",
    "    \n",
    "    def quantize_weights(self):\n",
    "        \"\"\"Quantize the weights (per-channel)\"\"\"\n",
    "        weight = self.linear_fp32.weight.data\n",
    "        q_weight, scales = quantize_per_channel_symmetric(weight, channel_dim=0)\n",
    "        \n",
    "        self.weight_quantized = q_weight\n",
    "        self.weight_scales = scales\n",
    "        self.quantized = True\n",
    "    \n",
    "    def set_activation_scale(self, scale):\n",
    "        \"\"\"Set activation quantization scale (from calibration)\"\"\"\n",
    "        self.activation_scale = scale\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.quantized:\n",
    "            # FP32 mode\n",
    "            return self.linear_fp32(x)\n",
    "        \n",
    "        # Quantize input activations\n",
    "        x_q, x_scale = quantize_tensor_symmetric(x)\n",
    "        \n",
    "        # Dequantize weights for computation (in practice, use INT8 matmul)\n",
    "        weight_dq = dequantize_per_channel(\n",
    "            self.weight_quantized, \n",
    "            self.weight_scales, \n",
    "            channel_dim=0\n",
    "        )\n",
    "        \n",
    "        # Dequantize activations\n",
    "        x_dq = dequantize_tensor(x_q, x_scale)\n",
    "        \n",
    "        # Compute (in FP32 for simplicity - real impl would use INT8 ops)\n",
    "        output = torch.nn.functional.linear(x_dq, weight_dq, self.linear_fp32.bias)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# ==================== TESTING QUANTIZED LINEAR ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING QUANTIZED LINEAR LAYER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create layers\n",
    "linear_fp32 = nn.Linear(128, 64)\n",
    "linear_quant = QuantizedLinear(128, 64)\n",
    "\n",
    "# Copy weights\n",
    "linear_quant.linear_fp32.load_state_dict(linear_fp32.state_dict())\n",
    "\n",
    "# Test input\n",
    "x = torch.randn(8, 128)  # Batch of 8\n",
    "\n",
    "# FP32 forward\n",
    "output_fp32 = linear_fp32(x)\n",
    "print(f\"\\nFP32 output shape: {output_fp32.shape}\")\n",
    "print(f\"FP32 output range: [{output_fp32.min():.3f}, {output_fp32.max():.3f}]\")\n",
    "\n",
    "# Quantize and forward\n",
    "linear_quant.quantize_weights()\n",
    "output_quant = linear_quant(x)\n",
    "print(f\"\\nQuantized output shape: {output_quant.shape}\")\n",
    "print(f\"Quantized output range: [{output_quant.min():.3f}, {output_quant.max():.3f}]\")\n",
    "\n",
    "# Compare outputs\n",
    "error = (output_fp32 - output_quant).abs().mean()\n",
    "relative_error = error / output_fp32.abs().mean()\n",
    "print(f\"\\nAbsolute error: {error:.6f}\")\n",
    "print(f\"Relative error: {relative_error:.6f} ({relative_error*100:.2f}%)\")\n",
    "\n",
    "# Memory comparison\n",
    "fp32_size = linear_fp32.weight.numel() * 4  # 4 bytes per float\n",
    "int8_size = linear_quant.weight_quantized.numel() * 1  # 1 byte per int8\n",
    "scales_size = linear_quant.weight_scales.numel() * 4  # scales are FP32\n",
    "\n",
    "print(f\"\\nMemory usage:\")\n",
    "print(f\"  FP32: {fp32_size} bytes\")\n",
    "print(f\"  INT8: {int8_size + scales_size} bytes\")\n",
    "print(f\"  Reduction: {fp32_size / (int8_size + scales_size):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "796cda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedModel(nn.Module):\n",
    "    \"\"\"Quantized version\"\"\"\n",
    "    def __init__(self, input_size=784, hidden_size=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = QuantizedLinear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = QuantizedLinear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = QuantizedLinear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def quantize_all_weights(self):\n",
    "        \"\"\"Quantize all linear layers\"\"\"\n",
    "        self.fc1.quantize_weights()\n",
    "        self.fc2.quantize_weights()\n",
    "        self.fc3.quantize_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd357cff",
   "metadata": {},
   "source": [
    "## Step 7: Advanced PyTorch Implementation - Fake Quantization & QAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621bf469",
   "metadata": {},
   "source": [
    "### Part 1: Advanced Fake Quantization with Observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbafc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a3e271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeQuantize(Function):\n",
    "    \"\"\"\n",
    "    Fake quantization with Straight-Through Estimator\n",
    "    Supports both symmetric and asymmetric quantization\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(self, x, scale, zero_point, q_max, q_min):\n",
    "        x_int = torch.clamp(\n",
    "            torch.round(x / scale) + zero_point,\n",
    "            q_min, q_max\n",
    "        )\n",
    "        x_deq = (x_int - zero_point) * scale\n",
    "        return x_deq\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output, None, None, None, None\n",
    "\n",
    "fake_quantize = FakeQuantize.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50e38617",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxObserver(nn.Module):\n",
    "    \"\"\"\n",
    "    Observes min/max values during calibration to compute scales\n",
    "    \"\"\"\n",
    "    def __init__(self, dtype=torch.qint8, qscheme='per_tensor_symmetric'):\n",
    "        super().__init__()\n",
    "        self.dtype = dtype\n",
    "        self.qscheme = qscheme\n",
    "        \n",
    "        # Quantization range\n",
    "        if dtype == torch.qint8:\n",
    "            self.quant_min = -127\n",
    "            self.quant_max = 127\n",
    "        elif dtype == torch.quint8:\n",
    "            self.quant_min = 0\n",
    "            self.quant_max = 255\n",
    "        \n",
    "        # Observed statistics\n",
    "        self.register_buffer('min_val', torch.tensor(float('inf')))\n",
    "        self.register_buffer('max_val', torch.tensor(float('-inf')))\n",
    "        \n",
    "        # Computed scale and zero point\n",
    "        self.register_buffer('scale', torch.tensor(1.0))\n",
    "        self.register_buffer('zero_point', torch.tensor(0, dtype=torch.int32))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Update min/max statistics\"\"\"\n",
    "        if self.training:\n",
    "            # Update running min/max\n",
    "            current_min = x.min()\n",
    "            current_max = x.max()\n",
    "            \n",
    "            self.min_val = torch.min(self.min_val, current_min)\n",
    "            self.max_val = torch.max(self.max_val, current_max)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def calculate_qparams(self):\n",
    "        \"\"\"Calculate quantization parameters from observed min/max\"\"\"\n",
    "        if self.qscheme == 'per_tensor_symmetric':\n",
    "            max_val = torch.max(self.min_val.abs(), self.max_val.abs())\n",
    "            self.scale = max_val / 127\n",
    "            self.zero_point = torch.tensor(0, dtype=torch.int32)\n",
    "        \n",
    "        elif self.qscheme == 'per_tensor_asymmetric':\n",
    "            self.scale = (self.max_val - self.min_val) / 255\n",
    "            self.zero_point = torch.round(-self.min_val / self.scale).to(torch.int32)\n",
    "            self.zero_point = torch.clamp(self.zero_point, 0, 255)\n",
    "        \n",
    "        # Handle zero scale\n",
    "        if self.scale == 0:\n",
    "            self.scale = torch.tensor(1.0)\n",
    "        \n",
    "        return self.scale, self.zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f00f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingAverageMinMaxObserver(MinMaxObserver):\n",
    "    \"\"\"\n",
    "    Observer with exponential moving average (better for QAT)\n",
    "    \"\"\"\n",
    "    def __init__(self, dtype=torch.qint8, qscheme='per_tensor_symmetric', \n",
    "                 averaging_constant=0.01):\n",
    "        super().__init__(dtype, qscheme)\n",
    "        self.averaging_constant = averaging_constant\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Update with exponential moving average\"\"\"\n",
    "        if self.training:\n",
    "            current_min = x.detach().min()\n",
    "            current_max = x.detach().max()\n",
    "            \n",
    "            if self.min_val == float('inf'):\n",
    "                # First observation\n",
    "                self.min_val = current_min\n",
    "                self.max_val = current_max\n",
    "            else:\n",
    "                # EMA update\n",
    "                self.min_val = (\n",
    "                    self.min_val * (1 - self.averaging_constant) +\n",
    "                    current_min * self.averaging_constant\n",
    "                )\n",
    "                self.max_val = (\n",
    "                    self.max_val * (1 - self.averaging_constant) +\n",
    "                    current_max * self.averaging_constant\n",
    "                )\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QATLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with fake quantization for QAT\n",
    "    Uses observers to track statistics and compute scales\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True, \n",
    "                 weight_qscheme='per_channel_symmetric',\n",
    "                 activation_qscheme='per_tensor_symmetric'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        self.weight_qscheme = weight_qscheme\n",
    "        self.activation_qscheme = activation_qscheme\n",
    "        \n",
    "        # Observers\n",
    "        if weight_qscheme == 'per_channel_symmetric':\n",
    "            # Per-channel observer (one per output channel)\n",
    "            self.weight_observer = nn.ModuleList([\n",
    "                MinMaxObserver(torch.qint8, 'per_tensor_symmetric')\n",
    "                for _ in range(out_features)\n",
    "            ])\n",
    "        else:\n",
    "            self.weight_observer = MinMaxObserver(torch.qint8, weight_qscheme)\n",
    "        \n",
    "        self.activation_observer = MovingAverageMinMaxObserver(\n",
    "            torch.qint8, activation_qscheme\n",
    "        )\n",
    "        \n",
    "        # Quantization parameters (set after calibration)\n",
    "        self.register_buffer('weight_scale', None)\n",
    "        self.register_buffer('weight_zero_point', None)\n",
    "        self.register_buffer('activation_scale', torch.tensor(1.0))\n",
    "        self.register_buffer('activation_zero_point', torch.tensor(0, dtype=torch.int32))\n",
    "        \n",
    "        # QAT mode flag\n",
    "        self.qat_mode = False\n",
    "    \n",
    "    def enable_observer(self):\n",
    "        \"\"\"Enable observers for calibration\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (MinMaxObserver, MovingAverageMinMaxObserver)):\n",
    "                module.train()\n",
    "    \n",
    "    def disable_observer(self):\n",
    "        \"\"\"Disable observers after calibration\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (MinMaxObserver, MovingAverageMinMaxObserver)):\n",
    "                module.eval()\n",
    "    \n",
    "    def calculate_qparams(self):\n",
    "        \"\"\"Calculate quantization parameters from observers\"\"\"\n",
    "        # Weight quantization parameters\n",
    "        if self.weight_qscheme == 'per_channel_symmetric':\n",
    "            scales = []\n",
    "            for i, observer in enumerate(self.weight_observer):\n",
    "                # Observe this channel\n",
    "                with torch.no_grad():\n",
    "                    observer(self.linear.weight[i])\n",
    "                    scale, _ = observer.calculate_qparams()\n",
    "                    scales.append(scale)\n",
    "            self.weight_scale = torch.stack(scales)\n",
    "            self.weight_zero_point = torch.zeros(len(scales), dtype=torch.int32)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                self.weight_observer(self.linear.weight)\n",
    "                self.weight_scale, self.weight_zero_point = self.weight_observer.calculate_qparams()\n",
    "        \n",
    "        # Activation quantization parameters\n",
    "        self.activation_scale, self.activation_zero_point = \\\n",
    "            self.activation_observer.calculate_qparams()\n",
    "    \n",
    "    def enable_qat(self):\n",
    "        \"\"\"Enable QAT mode (fake quantization)\"\"\"\n",
    "        self.qat_mode = True\n",
    "        self.calculate_qparams()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Observe activations\n",
    "        x = self.activation_observer(x)\n",
    "        \n",
    "        if not self.qat_mode:\n",
    "            # Normal FP32 forward\n",
    "            return self.linear(x)\n",
    "        \n",
    "        # Fake quantize activations\n",
    "        x_fq = fake_quantize(\n",
    "            x,\n",
    "            self.activation_scale,\n",
    "            self.activation_zero_point,\n",
    "            self.activation_observer.quant_min,\n",
    "            self.activation_observer.quant_max\n",
    "        )\n",
    "        \n",
    "        # Fake quantize weights (per-channel)\n",
    "        if self.weight_qscheme == 'per_channel_symmetric':\n",
    "            weight_fq = []\n",
    "            for i in range(self.linear.weight.shape[0]):\n",
    "                w_channel = self.linear.weight[i]\n",
    "                w_fq_channel = fake_quantize(\n",
    "                    w_channel,\n",
    "                    self.weight_scale[i],\n",
    "                    self.weight_zero_point[i],\n",
    "                    -127, 127\n",
    "                )\n",
    "                weight_fq.append(w_fq_channel)\n",
    "            weight_fq = torch.stack(weight_fq)\n",
    "        else:\n",
    "            weight_fq = fake_quantize(\n",
    "                self.linear.weight,\n",
    "                self.weight_scale,\n",
    "                self.weight_zero_point,\n",
    "                -127, 127\n",
    "            )\n",
    "        \n",
    "        # Linear with fake-quantized inputs\n",
    "        return F.linear(x_fq, weight_fq, self.linear.bias)\n",
    "\n",
    "class QATModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete model with QAT support\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=784, hidden_size=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = QATLinear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = QATLinear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = QATLinear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def enable_observer(self):\n",
    "        \"\"\"Enable all observers for calibration\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, QATLinear):\n",
    "                module.enable_observer()\n",
    "    \n",
    "    def disable_observer(self):\n",
    "        \"\"\"Disable all observers after calibration\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, QATLinear):\n",
    "                module.disable_observer()\n",
    "    \n",
    "    def calculate_qparams(self):\n",
    "        \"\"\"Calculate quantization parameters for all layers\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, QATLinear):\n",
    "                module.calculate_qparams()\n",
    "    \n",
    "    def enable_qat(self):\n",
    "        \"\"\"Enable QAT mode (fake quantization)\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, QATLinear):\n",
    "                module.enable_qat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2900752c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPLETE QAT WORKFLOW\n",
      "============================================================\n",
      "\n",
      "Step 1: Pretraining FP32 model...\n",
      "  Batch 0/16, Loss: 2.3075\n",
      "Pretrain Epoch 1: Loss = 2.3127\n",
      "  Batch 0/16, Loss: 2.0951\n",
      "Pretrain Epoch 2: Loss = 1.9762\n",
      "\n",
      "FP32 Accuracy: 9.00%\n",
      "\n",
      "============================================================\n",
      "Step 2: Calibration\n",
      "============================================================\n",
      "Starting calibration...\n",
      "  Calibration batch 0/8\n",
      "Calibration complete!\n",
      "\n",
      "Quantization scales:\n",
      "  fc1:\n",
      "    Weight scale: 0.000407\n",
      "    Activation scale: 0.032719\n",
      "  fc2:\n",
      "    Weight scale: 0.000584\n",
      "    Activation scale: 0.017808\n",
      "  fc3:\n",
      "    Weight scale: 0.000601\n",
      "    Activation scale: 0.008153\n",
      "\n",
      "============================================================\n",
      "Step 3: QAT Training\n",
      "============================================================\n",
      "\n",
      "QAT Epoch 1/3\n",
      "  Batch 0/16, Loss: 2.7003\n",
      "QAT Epoch 1: Loss = 2.6010, Accuracy = 11.00%\n",
      "\n",
      "QAT Epoch 2/3\n",
      "  Batch 0/16, Loss: 2.6204\n",
      "QAT Epoch 2: Loss = 2.5973, Accuracy = 11.00%\n",
      "\n",
      "QAT Epoch 3/3\n",
      "  Batch 0/16, Loss: 2.6751\n",
      "QAT Epoch 3: Loss = 2.6024, Accuracy = 11.00%\n",
      "\n",
      "============================================================\n",
      "Final Results\n",
      "============================================================\n",
      "FP32 Accuracy: 9.00%\n",
      "QAT Accuracy: 11.00%\n",
      "Accuracy drop: -2.00%\n"
     ]
    }
   ],
   "source": [
    "# ==================== TRAINING PIPELINE ====================\n",
    "\n",
    "def calibrate_model(model, calibration_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Calibration phase: collect statistics\n",
    "    \n",
    "    Args:\n",
    "        model: Model with observers\n",
    "        calibration_loader: DataLoader with calibration data\n",
    "        device: Device to run on\n",
    "    \"\"\"\n",
    "    print(\"Starting calibration...\")\n",
    "    model.eval()\n",
    "    model.enable_observer()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, _) in enumerate(calibration_loader):\n",
    "            data = data.to(device)\n",
    "            model(data)\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Calibration batch {batch_idx}/{len(calibration_loader)}\")\n",
    "    \n",
    "    # Calculate quantization parameters\n",
    "    model.calculate_qparams()\n",
    "    model.disable_observer()\n",
    "    \n",
    "    print(\"Calibration complete!\")\n",
    "    \n",
    "    # Print scales\n",
    "    print(\"\\nQuantization scales:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QATLinear):\n",
    "            print(f\"  {name}:\")\n",
    "            print(f\"    Weight scale: {module.weight_scale.mean().item():.6f}\")\n",
    "            print(f\"    Activation scale: {module.activation_scale.item():.6f}\")\n",
    "\n",
    "\n",
    "def train_qat(model, train_loader, optimizer, criterion, device='cpu'):\n",
    "    \"\"\"\n",
    "    QAT training for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    \n",
    "    accuracy = 100.0 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# ==================== COMPLETE QAT WORKFLOW ====================\n",
    "\n",
    "def complete_qat_workflow():\n",
    "    \"\"\"\n",
    "    Full QAT workflow: Pretrain → Calibrate → QAT → Evaluate\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPLETE QAT WORKFLOW\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    batch_size = 64\n",
    "    num_calibration_batches = 100\n",
    "    num_qat_epochs = 3\n",
    "    \n",
    "    # Create dummy datasets (replace with real MNIST/CIFAR)\n",
    "    def get_dummy_loader(num_samples, batch_size):\n",
    "        data = torch.randn(num_samples, 784)\n",
    "        targets = torch.randint(0, 10, (num_samples,))\n",
    "        dataset = torch.utils.data.TensorDataset(data, targets)\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    train_loader = get_dummy_loader(1000, batch_size)\n",
    "    calibration_loader = get_dummy_loader(500, batch_size)\n",
    "    test_loader = get_dummy_loader(200, batch_size)\n",
    "    \n",
    "    # Step 1: Create and pretrain FP32 model\n",
    "    print(\"\\nStep 1: Pretraining FP32 model...\")\n",
    "    model = QATModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Pretrain for a few epochs\n",
    "    for epoch in range(2):\n",
    "        loss = train_qat(model, train_loader, optimizer, criterion)\n",
    "        print(f\"Pretrain Epoch {epoch+1}: Loss = {loss:.4f}\")\n",
    "    \n",
    "    fp32_acc = evaluate(model, test_loader)\n",
    "    print(f\"\\nFP32 Accuracy: {fp32_acc:.2f}%\")\n",
    "    \n",
    "    # Step 2: Calibration\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 2: Calibration\")\n",
    "    print(\"=\"*60)\n",
    "    calibrate_model(model, calibration_loader)\n",
    "    \n",
    "    # Step 3: Enable QAT mode\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 3: QAT Training\")\n",
    "    print(\"=\"*60)\n",
    "    model.enable_qat()\n",
    "    \n",
    "    # Use smaller learning rate for QAT fine-tuning\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(num_qat_epochs):\n",
    "        print(f\"\\nQAT Epoch {epoch+1}/{num_qat_epochs}\")\n",
    "        loss = train_qat(model, train_loader, optimizer, criterion)\n",
    "        qat_acc = evaluate(model, test_loader)\n",
    "        print(f\"QAT Epoch {epoch+1}: Loss = {loss:.4f}, Accuracy = {qat_acc:.2f}%\")\n",
    "    \n",
    "    # Step 4: Final evaluation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Final Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"FP32 Accuracy: {fp32_acc:.2f}%\")\n",
    "    print(f\"QAT Accuracy: {qat_acc:.2f}%\")\n",
    "    print(f\"Accuracy drop: {fp32_acc - qat_acc:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# ==================== RUN ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = complete_qat_workflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
