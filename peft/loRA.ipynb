{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dba7506",
   "metadata": {},
   "source": [
    "#### LoRA\n",
    "When fine-tuning large models, we need to update weight matrices W ∈ ℝ^(d×k). Full fine-tuning updates all parameters, which is:\n",
    "- Memory intensive: Need to store gradients and optimizer states for all parameters\n",
    "- Storage expensive: Each task needs a full copy of the model\n",
    "\n",
    "Instead of updating W directly: W_new = W_old + ΔW\n",
    "- LoRA decomposes ΔW into two low-rank matrices: W_new = W + ΔW = W + BA\n",
    "    - B ∈ ℝ^(d×r): Low-rank \"down-projection\"\n",
    "    - A ∈ ℝ^(r×k): Low-rank \"up-projection\"\n",
    "\n",
    "Why This Works:\n",
    "The number of trainable parameters goes from d×k to r×(d+k):\n",
    "- Example: For a 4096×4096 weight matrix:\n",
    "\n",
    "    - Full fine-tuning: 16,777,216 parameters\n",
    "    - LoRA with r=8: 65,536 parameters (~0.4% of original!)\n",
    "\n",
    "Key Mathematical Properties\n",
    "- Forward pass: h = W₀x + BAx = W₀x + s·BAx\n",
    "    - s is a scaling factor (often α/r where α is a hyperparameter)\n",
    "- Merging: After training, can merge BA into W for zero inference overhead\n",
    "    - W_merged = W₀ + BA\n",
    "- Task switching: Keep W₀ frozen, swap different (B,A) pairs for different tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e477b",
   "metadata": {},
   "source": [
    "##### Practical Implementation Details\n",
    "Which Layers to Apply LoRA?\n",
    "In transformer models, you have multiple weight matrices. The original LoRA paper experiments show:\n",
    "Typical choices (in order of importance):\n",
    "\n",
    "- Query & Value projections (Wq, Wv): Most common, best bang for buck\n",
    "- Query, Key, Value, Output (Wq, Wk, Wv, Wo): More expressive\n",
    "- All linear layers: Including FFN layers (Wup, Wdown, Wgate in LLaMA)\n",
    "\n",
    "Why Q and V are often preferred:\n",
    "- Q controls what information to attend to\n",
    "- V controls what information to extract\n",
    "- K is often less critical (patterns emerge during pre-training)\n",
    "- Trade-off: More layers = more parameters but better adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13571b3",
   "metadata": {},
   "source": [
    "##### Initialization Strategies\n",
    "Critical for training stability!\n",
    "- Matrix A (up-projection):\n",
    "    - Initialized randomly: Usually Gaussian N(0, σ²) or Kaiming/Xavier\n",
    "    - Provides the \"diversity\" in the low-rank space\n",
    "\n",
    "- Matrix B (down-projection):\n",
    "    - Initialized to zero: BA = 0 at start\n",
    "    - Ensures LoRA starts as identity: W₀ + BA = W₀\n",
    "    - Model begins exactly as pre-trained, then gradually adapts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ef7e88",
   "metadata": {},
   "source": [
    "##### The Scaling Factor (α/r)\n",
    "Forward pass: h = W₀x + (α/r)·BAx\n",
    "- Parameters:\n",
    "    - α (alpha): Constant, often set to r or 2r\n",
    "    - r: The rank we choose\n",
    "\n",
    "Common strategies:\n",
    "- α = r: Scale by 1, straightforward\n",
    "- α = 2r or α = 16: Scale up LoRA contribution\n",
    "- α = constant: Keeps LoRA magnitude constant when changing r\n",
    "\n",
    "Why scale?\n",
    "- Without scaling, LoRA updates might be too small relative to W₀\n",
    "- Scaling ensures LoRA has sufficient learning signal\n",
    "- Allows fair comparison between different ranks\n",
    "\n",
    "##### Key insights:\n",
    "- Higher rank: More expressive, more parameters, slower\n",
    "- Lower rank: Faster, fewer parameters, may underfit\n",
    "- Empirical finding: r=8 often sufficient for many tasks!\n",
    "\n",
    "#### Compute/Memory Trade-offs:\n",
    "For each LoRA-adapted layer:\n",
    "\n",
    "- Extra memory: r×(d+k) parameters + gradients + optimizer states\n",
    "- Extra compute: Two extra matrix multiplies per forward pass\n",
    "    - Bx: (d×r) @ (r×batch) = O(d·r·batch)\n",
    "    - A(Bx): (r×k) @ (k×batch) = O(r·k·batch)\n",
    "- Still way cheaper than full fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7522268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d503c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, alpha):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.b = nn.Parameter(torch.zeros(in_features, rank))\n",
    "        self.a = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_normal_(self.b)\n",
    "        nn.init.zeros_(self.a)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.matmul(x, self.b)\n",
    "        out = torch.matmul(out, self.a)\n",
    "        return out * self.scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, rank, alpha):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = linear\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.linear.weight.requires_grad = False\n",
    "        if self.linear.bias is not None:\n",
    "            self.linear.weight.requires_grad = False\n",
    "\n",
    "        self.lora = LoRA(\n",
    "            linear.in_features,\n",
    "            linear.out_features,\n",
    "            rank,\n",
    "            alpha\n",
    "        )\n",
    "\n",
    "        self.merged = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.merged:\n",
    "            return self.linear(x)\n",
    "        else:\n",
    "            return self.linear(x) + self.lora(x)\n",
    "        \n",
    "    def merge(self, x):\n",
    "        if not self.merged:\n",
    "            with torch.no_grad():\n",
    "                delta_w = torch.matmul(self.lora.b, self.lora.a) * self.lora.scaling\n",
    "                self.linear.weight += delta_w\n",
    "            self.merged = True\n",
    "\n",
    "    def unmerge(self):\n",
    "        if self.merged:\n",
    "            with torch.no_grad():\n",
    "                delta_w = torch.matmul(self.lora.b, self.lora.a) * self.lora.scaling\n",
    "                self.linear.weight.data -= delta_w\n",
    "            self.merged = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57782462",
   "metadata": {},
   "source": [
    "#### QLoRA (Quantized LoRA)\n",
    "QLoRA is LoRA + 4-bit quantization of the base model. It's the technique that made fine-tuning 65B models possible on a single 48GB GPU!\n",
    "\n",
    "The Core Idea\n",
    "- LoRA problem:\n",
    "    - Base model weights W still need to be loaded in memory (even if frozen)\n",
    "    - 7B model in FP16 = 14GB just for weights\n",
    "    - 65B model in FP16 = 130GB (won't fit on consumer GPUs!)\n",
    "- QLoRA solution:\n",
    "    - Store W in 4-bit (16x compression from FP32!)\n",
    "    - Keep LoRA adapters (BA) in higher precision (FP16/BF16)\n",
    "    - Dequantize W on-the-fly during forward/backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05cd6630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRALinear(nn.Module):\n",
    "    def __init__(self, weights, in_features, out_features, rank, alpha, bits):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.bits = bits\n",
    "\n",
    "        self.quantized_weights, self.scale, self.zero_point = self._quantize_weights(weights)\n",
    "\n",
    "        self.lora_b = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_a = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        self.lora_scale = alpha / rank\n",
    "\n",
    "        self._init_lora()\n",
    "\n",
    "    def _init_lora(self):\n",
    "        nn.init.kaiming_uniform_(self.lora_b)\n",
    "        nn.init.zeros_(self.lora_a)\n",
    "\n",
    "    def _quantize_weights(self, weights):\n",
    "        q_max = 2 ** (self.bits - 1) - 1\n",
    "        q_min = - q_max\n",
    "\n",
    "        m_max = weights.max()\n",
    "        m_min = weights.min()\n",
    "\n",
    "        scale = (m_max - m_min) / (q_max - q_min)\n",
    "\n",
    "        zero_point = q_min - torch.round(m_min / scale)\n",
    "\n",
    "        quantized_linear = torch.clamp(\n",
    "            torch.round(weights / scale) + zero_point, \n",
    "            q_min, q_max\n",
    "        ).to(dtype=torch.int8)\n",
    "\n",
    "        return quantized_linear, scale, zero_point\n",
    "    \n",
    "    def _dequantize_weights(self):\n",
    "        return (self.quantized_weights.float() - self.zero_point) * self.scale\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W = self._dequantize_weights()\n",
    "        \n",
    "        base_out = x @ W\n",
    "\n",
    "        lora_out = x @ self.lora_b.T\n",
    "        lora_out = lora_out @ self.lora_a.T\n",
    "        lora_out = lora_out * self.lora_scale\n",
    "\n",
    "        return base_out + lora_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
