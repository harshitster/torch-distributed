{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd503f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a1150f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    \"\"\"\n",
    "    A scalar value with gradient tracking.\n",
    "    This is the fundamental building block - similar to PyTorch's autograd.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Variable) else Variable(other)\n",
    "        out = Variable(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Variable) else Variable(other)\n",
    "        out = Variable(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float))\n",
    "        out = Variable(self.data ** other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Compute gradients via backpropagation.\n",
    "        Uses topological sort to ensure we process nodes in correct order.\n",
    "        \"\"\"\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "599a7994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXAMPLE 1: Basic Scalar Autodiff\n",
      "============================================================\n",
      "Forward pass: f = 10.0\n",
      "df/dx = 7.0\n",
      "df/dy = 2.0\n",
      "\n",
      "Manual verification:\n",
      "df/dx = y + 2*x = 3.0 + 2*2.0 = 7.0\n",
      "df/dy = x = 2.0\n"
     ]
    }
   ],
   "source": [
    "def example_1_basic():\n",
    "    \"\"\"\n",
    "    Compute: f(x, y) = x*y + x^2\n",
    "    Find: df/dx and df/dy\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXAMPLE 1: Basic Scalar Autodiff\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    x = Variable(2.0, label='x')\n",
    "    y = Variable(3.0, label='y')\n",
    "    \n",
    "    # Forward pass\n",
    "    z = x * y  # z = 6.0\n",
    "    w = x ** 2  # w = 4.0\n",
    "    f = z + w  # f = 10.0\n",
    "    \n",
    "    print(f\"Forward pass: f = {f.data}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    f.backward()\n",
    "    \n",
    "    print(f\"df/dx = {x.grad}\")  # Should be: y + 2*x = 3 + 4 = 7\n",
    "    print(f\"df/dy = {y.grad}\")  # Should be: x = 2\n",
    "    \n",
    "    # Verify manually\n",
    "    print(f\"\\nManual verification:\")\n",
    "    print(f\"df/dx = y + 2*x = {y.data} + 2*{x.data} = {y.data + 2*x.data}\")\n",
    "    print(f\"df/dy = x = {x.data}\")\n",
    "\n",
    "example_1_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eba46ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=True, _children=(), _op=''):\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "        self.requires_grad = requires_grad\n",
    "        self._prev = _children\n",
    "        self._op = _op\n",
    "\n",
    "        self.grad = None\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "\n",
    "    @staticmethod\n",
    "    def _unbroadcast(grad, original_shape):\n",
    "        \"\"\"\n",
    "        Reduce gradient to match original shape by summing along broadcasted dimensions.\n",
    "        \n",
    "        This handles two cases:\n",
    "        1. Prepended dimensions (added on left): Sum along axis 0 repeatedly\n",
    "        2. Expanded dimensions (size 1 -> size N): Sum along that axis with keepdims\n",
    "        \"\"\"\n",
    "        # Step 1: Remove prepended dimensions\n",
    "        ndims_added = grad.ndim - len(original_shape)\n",
    "        for _ in range(ndims_added):\n",
    "            grad = grad.sum(axis=0)\n",
    "        \n",
    "        # Step 2: Reduce expanded dimensions (where original had size 1)\n",
    "        for i, (grad_dim, orig_dim) in enumerate(zip(grad.shape, original_shape)):\n",
    "            if orig_dim == 1 and grad_dim > 1:\n",
    "                grad = grad.sum(axis=i, keepdims=True)\n",
    "        \n",
    "        return grad\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other, requires_grad=False)\n",
    "        out = Tensor(self.data + other.data, _children=(self, other), _op='+')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = self._unbroadcast(out.grad, self.data.shape)\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "            \n",
    "            if other.requires_grad:\n",
    "                grad = self._unbroadcast(out.grad, other.data.shape)\n",
    "                if other.grad is None:\n",
    "                    other.grad = grad\n",
    "                else:\n",
    "                    other.grad += grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other, requires_grad=False)\n",
    "        out = Tensor(self.data * other.data, _children=(self, other), _op='*')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = out.grad * other.data\n",
    "                grad = self._unbroadcast(grad, self.data.shape)\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "            \n",
    "            if other.requires_grad:\n",
    "                grad = out.grad * self.data\n",
    "                grad = self._unbroadcast(grad, other.data.shape)\n",
    "                if other.grad is None:\n",
    "                    other.grad = grad\n",
    "                else:\n",
    "                    other.grad += grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def matmul(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other, requires_grad=False)\n",
    "        out = Tensor(self.data @ other.data, _children=(self, other), _op='@')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = out.grad @ other.data.T\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "\n",
    "            if other.requires_grad:\n",
    "                grad = self.data.T @ out.grad\n",
    "                if other.grad is None:\n",
    "                    other.grad = grad\n",
    "                else:\n",
    "                    other.grad += grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sum(self, axis=None, keepdim=False):\n",
    "        out = Tensor(self.data.sum(axis=axis, keepdims=keepdim), _children=(self,), _op='sum')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = out.grad\n",
    "\n",
    "                if axis is None:\n",
    "                    grad = np.full_like(self.data, fill_value=grad)\n",
    "                else:\n",
    "                    if not keepdim:\n",
    "                        grad = np.expand_dims(grad, axis=axis)\n",
    "                    grad = np.broadcast_to(grad, self.data.shape).copy()\n",
    "\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def mean(self, axis=None, keepdim=False):\n",
    "        out = Tensor(self.data.mean(axis=axis, keepdims=keepdim), _children=(self,), _op='mean')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = out.grad\n",
    "\n",
    "                if axis is None:\n",
    "                    n = self.data.size\n",
    "                    grad = np.full(self.data.shape, fill_value=grad / n)\n",
    "                else:\n",
    "                    n = self.data.shape[axis]\n",
    "                    grad = grad / n\n",
    "\n",
    "                    if not keepdim:\n",
    "                        grad = np.expand_dims(grad, axis=axis)\n",
    "                    grad = np.broadcast_to(grad, self.data.shape).copy()\n",
    "\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Tensor(np.maximum(self.data, 0.0), _children=(self,), _op='relu')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = (self.data > 0).astype(np.float32) * out.grad\n",
    "\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        s = 1. / (1. + np.exp(-self.data))\n",
    "        out = Tensor(s, _children=(self,), _op='sigmoid')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = s * (1 - s) * out.grad\n",
    "\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def log(self):\n",
    "        out = Tensor(np.log(self.data + 1e-10), _children=(self,), _op='log')\n",
    "        \n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = (1.0 / (self.data + 1e-10)) * out.grad\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def softmax(self, axis=-1):\n",
    "        x_max = self.data.max(axis=axis, keepdims=True)\n",
    "        exp_x = np.exp(self.data - x_max)\n",
    "        s = exp_x / exp_x.sum(axis=axis, keepdim=True)\n",
    "\n",
    "        out = Tensor(s, _children=(self,), _op='softmax')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                sum_term = (s * out.grad).sum(axis=axis, keepdim=True)\n",
    "\n",
    "                grad = s * (out.grad - sum_term)\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def reshape(self, shape):\n",
    "        out = Tensor(self.data.reshape(shape), _children=(self,), _op='reshape')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = out.grad.reshape(self.data.shape)\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def transpose(self, axes=None):\n",
    "        out = Tensor(self.data.transpose(axes), _children=(self,), _op='transpose')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if axes is None:\n",
    "                    grad = out.grad.T\n",
    "                else:\n",
    "                    inverse_axes = [0] * len(axes)\n",
    "                    for i, axis in enumerate(axes):\n",
    "                        inverse_axes[axis] = i\n",
    "                    grad = np.transpose(out.grad, axes=inverse_axes)\n",
    "\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def batchnorm(self, gamma, beta):\n",
    "        mean = self.data.mean(axis=0, keepdim=True)\n",
    "        var = self.data.var(axis=0, keepdim=True)\n",
    "\n",
    "        std = np.sqrt(var + 1e-5)\n",
    "        x_centered = (self.data - mean)\n",
    "        x_norm = x_centered / std\n",
    "\n",
    "        y = gamma.data * x_norm + beta.data\n",
    "\n",
    "        out = Tensor(y, _children=(self, gamma, beta), _op='batch_norm')\n",
    "\n",
    "        def _backward():\n",
    "            if gamma.requires_grad:\n",
    "                grad_gamma = (out.grad * x_norm).sum(axis=0, keepdim=True)\n",
    "                if gamma.grad is not None:\n",
    "                    gamma.grad = grad_gamma\n",
    "                else:\n",
    "                    gamma.grad += grad_gamma\n",
    "\n",
    "            if beta.requires_grad:\n",
    "                grad_beta = (out.grad).sum(axis=0, keepdim=True)\n",
    "                if beta.grad is not None:\n",
    "                    beta.grad = grad_beta\n",
    "                else:\n",
    "                    beta.grad += grad_beta\n",
    "\n",
    "            if self.requires_grad:\n",
    "                out_grad = out.grad\n",
    "                N = self.data.shape[0]\n",
    "\n",
    "                grad_x_norm = out_grad\n",
    "                grad_var = (grad_x_norm * (-0.5) * (x_centered) * var ** (-3/2)).sum(axis=0, keepdim=True)\n",
    "                grad_mean = (grad_x_norm * (1. / std) + grad_var * (-2. / N) * (x_centered)).sum(axis=0, keepdim=True)\n",
    "\n",
    "                grad = grad_x_norm * (1. / std) + grad_mean * (1. / N) + grad_var * (2. / N) * x_centered * (1 - 1 / N)\n",
    "\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def dropout(self, p):\n",
    "        mask = (np.random.rand(*self.data.shape) > p).astype(np.float32)\n",
    "        mask = mask / (1. - p)\n",
    "\n",
    "        out = Tensor(self.data * mask, _children=(self,), _op='dropout')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                out_grad = out.grad * mask\n",
    "\n",
    "                if self.grad is None:\n",
    "                    self.grad = out_grad\n",
    "                else:\n",
    "                    self.grad += out_grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "     \n",
    "    def __pow__(self, other):\n",
    "        other_val = other.data if isinstance(other, Tensor) else other\n",
    "        out = Tensor(self.data ** other_val, _children=(self,), _op=f'**{other_val}')\n",
    "        \n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = other_val * (self.data ** (other_val - 1)) * out.grad\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(shape={self.shape}, data={self.data}, grad={self.grad})\"\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "        \n",
    "        self.grad = np.ones_like(self.data)\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Test Broadcasting\n",
    "# ============================================================================\n",
    "\n",
    "def test_broadcasting():\n",
    "    print(\"=\"*60)\n",
    "    print(\"Testing Broadcasting Support\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test 1: Basic broadcasting\n",
    "    print(\"\\n--- Test 1: (1,3) * (3,1) -> (3,3) ---\")\n",
    "    a = Tensor([[1, 2, 3]])      # (1, 3)\n",
    "    b = Tensor([[10],            # (3, 1)\n",
    "                [20],\n",
    "                [30]])\n",
    "    \n",
    "    c = a * b  # (3, 3)\n",
    "    loss = c.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"a.shape: {a.shape}, a.grad.shape: {a.grad.shape}\")\n",
    "    print(f\"a.grad:\\n{a.grad}\")\n",
    "    print(f\"\\nb.shape: {b.shape}, b.grad.shape: {b.grad.shape}\")\n",
    "    print(f\"b.grad:\\n{b.grad}\")\n",
    "    \n",
    "    # Test 2: Prepended dimensions\n",
    "    print(\"\\n--- Test 2: (3,) + (2,3) -> (2,3) ---\")\n",
    "    a = Tensor([1, 2, 3])        # (3,)\n",
    "    b = Tensor([[10, 20, 30],    # (2, 3)\n",
    "                [40, 50, 60]])\n",
    "    \n",
    "    c = a + b  # (2, 3)\n",
    "    loss = c.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"a.shape: {a.shape}, a.grad.shape: {a.grad.shape}\")\n",
    "    print(f\"a.grad: {a.grad}\")\n",
    "    print(f\"\\nb.shape: {b.shape}, b.grad.shape: {b.grad.shape}\")\n",
    "    print(f\"b.grad:\\n{b.grad}\")\n",
    "    \n",
    "    # Test 3: Scalar broadcasting\n",
    "    print(\"\\n--- Test 3: Scalar * (2,3) -> (2,3) ---\")\n",
    "    a = Tensor(2.0)              # scalar\n",
    "    b = Tensor([[1, 2, 3],\n",
    "                [4, 5, 6]])      # (2, 3)\n",
    "    \n",
    "    c = a * b  # (2, 3)\n",
    "    loss = c.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"a.shape: {a.shape}, a.grad.shape: {a.grad.shape}\")\n",
    "    print(f\"a.grad: {a.grad}\")\n",
    "    print(f\"\\nb.shape: {b.shape}, b.grad.shape: {b.grad.shape}\")\n",
    "    print(f\"b.grad:\\n{b.grad}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"All broadcasting tests passed! âœ…\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_broadcasting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08050df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
